{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"PSP.cz Analyzer","text":"<p>Czech Parliamentary Voting Analyzer \u2014 an OSINT tool that downloads, parses, and visualizes open voting data from the Czech Chamber of Deputies.</p> <p>Built with FastAPI, Polars, and HTMX.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Party Loyalty \u2014 rebellion rates: how often each MP votes against their party's majority</li> <li>Attendance \u2014 participation rates with breakdowns (active, passive, absent, excused)</li> <li>Most Active MPs \u2014 ranked by raw vote count (YES + NO + ABSTAIN)</li> <li>Voting Similarity \u2014 cross-party alliances via cosine similarity + PCA visualization</li> <li>Votes Browser \u2014 searchable, paginated list of all parliamentary votes with detail views</li> <li>Chart Endpoints \u2014 server-rendered PNG charts (seaborn/matplotlib)</li> <li>Multi-period Support \u2014 covers all 10 electoral periods (1993 to present)</li> <li>Tisk Pipeline \u2014 background processing that downloads parliamentary print PDFs, extracts text, and classifies topics</li> <li>AI Summaries \u2014 optional LLM-based bilingual (Czech + English) summarization and topic classification via Ollama</li> <li>i18n \u2014 full Czech/English UI localization with a header language switcher</li> <li>Docker \u2014 containerized deployment with docker-compose</li> <li>API Documentation \u2014 interactive Scalar UI at <code>/docs</code> with full OpenAPI schema</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Requires Python &gt;= 3.12 and uv.</p> <pre><code># Install dependencies\nuv sync\n\n# (Optional) Copy and edit environment variables\ncp .env.example .env\n\n# Run the dev server (with hot reload)\nuv run python -m pspcz_analyzer.main\n</code></pre> <p>The app starts on <code>http://localhost:8000</code>. On first launch it downloads ~50 MB of open data from psp.cz and caches it locally as Parquet files.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>All configuration is via environment variables loaded from <code>.env</code> by <code>python-dotenv</code>:</p> Variable Default Description <code>PSPCZ_CACHE_DIR</code> <code>~/.cache/pspcz-analyzer/psp</code> Data cache directory <code>PSPCZ_DEV</code> <code>1</code> <code>1</code> for hot reload, <code>0</code> for production <code>OLLAMA_BASE_URL</code> <code>http://localhost:11434</code> Ollama API endpoint <code>OLLAMA_API_KEY</code> (empty) Bearer token for remote HTTPS Ollama <code>OLLAMA_MODEL</code> <code>qwen3:8b</code> Model for classification and summarization <p>See <code>.env.example</code> for a documented template.</p>"},{"location":"#docker","title":"Docker","text":"<pre><code>docker compose up --build\n</code></pre> <p>The app runs at <code>http://localhost:8000</code>. Configure <code>OLLAMA_BASE_URL</code> and <code>OLLAMA_API_KEY</code> in <code>.env</code> to connect to your Ollama instance on the local network.</p>"},{"location":"#tech-stack","title":"Tech Stack","text":"Layer Technology Web framework FastAPI + Uvicorn Templating Jinja2 + i18n extension Frontend interactivity HTMX CSS Pico CSS (dark theme) Localization Dict-based i18n (Czech + English) Data processing Polars Charts Seaborn + Matplotlib PDF extraction PyMuPDF HTML scraping BeautifulSoup4 LLM integration Ollama (optional, bilingual) API documentation Scalar HTTP client httpx Configuration python-dotenv Testing pytest + pytest-cov Linting &amp; formatting Ruff Type checking Pyright Containerization Docker + docker-compose CI/CD GitHub Actions Package manager uv"},{"location":"#data-source","title":"Data Source","text":"<p>All data comes from the psp.cz open data portal. Files are pipe-delimited UNL format, Windows-1250 encoded. The app downloads and caches them automatically on first access.</p> <p>Cached data is stored at <code>~/.cache/pspcz-analyzer/psp/</code> (override with <code>PSPCZ_CACHE_DIR</code>).</p>"},{"location":"#license","title":"License","text":"<p>Educational / OSINT project. Parliamentary data is public domain per Czech law.</p>"},{"location":"data-model/","title":"Data Model","text":""},{"location":"data-model/#electoral-periods","title":"Electoral Periods","text":"<p>The Czech Chamber of Deputies operates in electoral periods. Each period has a number, a year identifier (used in psp.cz ZIP filenames), and an organ ID (used in the database).</p> Period Years Label ZIP Year Organ ID 10 2025\u2013present Current 2025 174 9 2021\u20132025 2021 173 8 2017\u20132021 2017 172 7 2013\u20132017 2013 171 6 2010\u20132013 2010 170 5 2006\u20132010 2006 169 4 2002\u20132006 2002 168 3 1998\u20132002 1998 167 2 1996\u20131998 1996 166 1 1993\u20131996 1993 165 <p>The organ ID mapping is critical \u2014 <code>id_obdobi</code> in the <code>poslanec</code> table uses organ IDs (165\u2013174), not period numbers (1\u201310).</p>"},{"location":"data-model/#unl-file-format","title":"UNL File Format","text":"<p>psp.cz distributes data as UNL files inside ZIP archives:</p> <ul> <li>Encoding: Windows-1250 (Czech)</li> <li>Delimiter: pipe <code>|</code></li> <li>Headers: none \u2014 column order defined in <code>models/schemas.py</code></li> <li>Trailing pipe: every line ends with <code>|</code>, producing an extra empty column (dropped during parsing)</li> <li>Quoting: some files contain unescaped double-quotes \u2014 parsed with <code>quote_char=None</code></li> </ul>"},{"location":"data-model/#data-sources-zip-archives","title":"Data Sources (ZIP Archives)","text":"Archive URL Pattern Contents <code>hl-{year}ps.zip</code> <code>/opendata/hl-{year}ps.zip</code> Voting data for one period <code>poslanci.zip</code> <code>/opendata/poslanci.zip</code> MPs, persons, organs, memberships <code>schuze.zip</code> <code>/opendata/schuze.zip</code> Sessions and agenda items <code>tisky.zip</code> <code>/opendata/tisky.zip</code> Parliamentary prints (bills) <p>Base URL: <code>https://www.psp.cz/eknih/cdrom/opendata</code></p>"},{"location":"data-model/#key-tables","title":"Key Tables","text":""},{"location":"data-model/#voting-data-per-period","title":"Voting Data (per-period)","text":"<p>hl_hlasovani \u2014 vote summaries (<code>hl{year}s.unl</code>):</p> Column Type Description <code>id_hlasovani</code> Int64 Unique vote ID <code>id_organ</code> Int32 Organ (chamber) ID <code>schuze</code> Int32 Session number <code>cislo</code> Int32 Vote number within session <code>bod</code> Int32 Agenda item number <code>datum</code> Utf8 Date (string) <code>cas</code> Utf8 Time (string) <code>pro</code> / <code>proti</code> / <code>zdrzel</code> / <code>nehlasoval</code> Int32 Vote counts <code>prihlaseno</code> Int32 MPs registered <code>kvorum</code> Int32 Quorum required <code>vysledek</code> Utf8 Outcome code (see below) <code>nazev_dlouhy</code> / <code>nazev_kratky</code> Utf8 Vote description (long/short) <p>hl_poslanec \u2014 individual MP votes (<code>hl{year}hN.unl</code>, multiple files per period):</p> Column Type Description <code>id_poslanec</code> Int64 MP identifier <code>id_hlasovani</code> Int64 Vote ID (FK to hl_hlasovani) <code>vysledek</code> Utf8 Vote result code (see below) <p>zmatecne \u2014 void vote IDs (<code>hl{year}z.unl</code>):</p> Column Type Description <code>id_hlasovani</code> Int64 ID of a void vote"},{"location":"data-model/#shared-tables","title":"Shared Tables","text":"<p>osoby \u2014 persons: <code>id_osoba</code>, <code>pred</code> (title before), <code>prijmeni</code> (surname), <code>jmeno</code> (first name), <code>za</code> (title after), <code>narozeni</code>, <code>pohlavi</code>, <code>zmena</code>, <code>umrti</code></p> <p>poslanec \u2014 MP records: <code>id_poslanec</code>, <code>id_osoba</code> (FK), <code>id_kraj</code>, <code>id_kandidatka</code>, <code>id_obdobi</code> (organ ID, not period number), <code>web</code>, contact fields, <code>foto</code>, <code>facebook</code></p> <p>organy \u2014 organs/organizations: <code>id_organ</code>, <code>organ_id_organ</code> (parent), <code>id_typ_organu</code> (1 = parliamentary club), <code>zkratka</code> (abbreviation), <code>nazev_organu_cz/en</code>, date range, <code>priorita</code></p> <p>zarazeni \u2014 memberships: <code>id_osoba</code> (FK), <code>id_of</code> (organ FK), <code>cl_funkce</code>, <code>od_o</code>/<code>do_o</code> (membership dates), <code>od_f</code>/<code>do_f</code> (function dates)</p> <p>schuze \u2014 sessions: <code>id_schuze</code>, <code>id_org</code>, <code>schuze</code> (session number), <code>od_schuze</code>/<code>do_schuze</code>, <code>aktualizace</code></p> <p>bod_schuze \u2014 agenda items: <code>id_bod</code>, <code>id_schuze</code> (FK), <code>id_tisk</code> (FK to tisky), <code>bod</code> (item number), <code>uplny_naz</code> (full name)</p> <p>tisky \u2014 parliamentary prints: <code>id_tisk</code>, <code>ct</code> (print number), <code>nazev_tisku</code>, <code>datum_doruceni</code>, <code>id_obdobi</code>, and more</p>"},{"location":"data-model/#tisk-enrichment-data","title":"Tisk Enrichment Data","text":"<p>Data produced by the background tisk pipeline, stored in the cache directory.</p>"},{"location":"data-model/#tiskinfo-model","title":"TiskInfo Model","text":"<p>The <code>TiskInfo</code> dataclass (<code>models/tisk_models.py</code>) holds enriched data for each parliamentary print:</p> Field Type Description <code>ct</code> int Print number <code>nazev</code> str Print name <code>url</code> str psp.cz URL <code>topics</code> list[str] Topic labels (from LLM or keyword classification) <code>summary</code> str Czech AI summary <code>summary_en</code> str English AI summary <code>has_text</code> bool Whether extracted PDF text exists <code>sub_versions</code> list[dict] Sub-tisk versions with diff summaries <code>law_changes</code> list[str] Laws changed by this print <code>history</code> TiskHistory Legislative process timeline"},{"location":"data-model/#pdf-text-cache","title":"PDF Text Cache","text":"<p>Extracted plain text from parliamentary print PDFs:</p> <pre><code>~/.cache/pspcz-analyzer/psp/tisky_text/{period}/{ct}.txt\n</code></pre>"},{"location":"data-model/#topic-classifications","title":"Topic Classifications","text":"<p>Per-period topic classification stored as Parquet files:</p> <pre><code>~/.cache/pspcz-analyzer/psp/tisky_meta/{period}/topic_classifications.parquet\n</code></pre> <p>Columns: <code>ct</code> (print number), <code>topic</code> (serialized topic labels), <code>summary</code> (Czech), <code>summary_en</code> (English), <code>source</code> (classification method).</p> <p>Topics are assigned either by keyword matching (<code>topic_service.py</code>) or by LLM classification (<code>ollama_service.py</code>). The LLM results take priority when available.</p>"},{"location":"data-model/#ai-summaries","title":"AI Summaries","text":"<p>Per-tisk summaries generated by Ollama in both Czech and English, stored in the topic classifications Parquet cache alongside topic data.</p>"},{"location":"data-model/#version-diff-summaries","title":"Version Diff Summaries","text":"<p>LLM-generated comparison summaries between sub-versions of a parliamentary print:</p> <pre><code>~/.cache/pspcz-analyzer/psp/tisky_version_diffs/{period}/{ct}_{sub_ct}.txt      # Czech\n~/.cache/pspcz-analyzer/psp/tisky_version_diffs/{period}/{ct}_{sub_ct}_en.txt   # English\n</code></pre>"},{"location":"data-model/#legislative-histories","title":"Legislative Histories","text":"<p>Scraped from psp.cz HTML, stored as JSON:</p> <pre><code>~/.cache/pspcz-analyzer/psp/tisky_historie/{period}/{ct}.json\n</code></pre> <p>Contains the full legislative process timeline (readings, committee reports, Senate, President).</p>"},{"location":"data-model/#configuration","title":"Configuration","text":"<p>All configuration is via environment variables, loaded from <code>.env</code> by <code>python-dotenv</code>. Constants are defined in <code>config.py</code>.</p>"},{"location":"data-model/#environment-variables","title":"Environment Variables","text":"Variable Default Description <code>PSPCZ_CACHE_DIR</code> <code>~/.cache/pspcz-analyzer/psp</code> Root cache directory for all data <code>PSPCZ_DEV</code> <code>1</code> <code>1</code> for hot reload (dev), <code>0</code> for production <code>OLLAMA_BASE_URL</code> <code>http://localhost:11434</code> Ollama API endpoint <code>OLLAMA_API_KEY</code> (empty) Bearer token for remote HTTPS Ollama <code>OLLAMA_MODEL</code> <code>qwen3:8b</code> Model for inference <code>DAILY_REFRESH_ENABLED</code> <code>1</code> <code>1</code> to enable daily data refresh, <code>0</code> to disable <code>DAILY_REFRESH_HOUR</code> <code>3</code> Hour (CET, 0-23) at which the daily refresh runs"},{"location":"data-model/#ollama-configuration","title":"Ollama Configuration","text":"<p>Additional constants in <code>config.py</code> (not overridable via env var):</p> Constant Default Description <code>OLLAMA_TIMEOUT</code> <code>300.0</code> Per-request timeout in seconds <code>OLLAMA_HEALTH_TIMEOUT</code> <code>5.0</code> Health check timeout <code>OLLAMA_MAX_TEXT_CHARS</code> <code>50000</code> Max text length sent to LLM <code>OLLAMA_VERBATIM_CHARS</code> <code>40000</code> Chars included verbatim (rest truncated) <p>If Ollama is not running or unreachable, the system silently falls back to keyword-based classification.</p>"},{"location":"data-model/#vote-result-codes","title":"Vote Result Codes","text":""},{"location":"data-model/#individual-mp-votes-hl_poslanecvysledek","title":"Individual MP Votes (<code>hl_poslanec.vysledek</code>)","text":"Code Enum Meaning <code>A</code> YES Voted yes <code>B</code> NO Voted no <code>C</code> ABSTAINED Abstained <code>F</code> DID_NOT_VOTE Registered but didn't press button <code>@</code> ABSENT Not registered in the chamber <code>M</code> EXCUSED Formally excused <code>W</code> BEFORE_OATH Before taking oath <code>K</code> ABSTAIN_ALT Alternative abstain code"},{"location":"data-model/#vote-outcomes-hl_hlasovanivysledek","title":"Vote Outcomes (<code>hl_hlasovani.vysledek</code>)","text":"Code Meaning <code>A</code> Passed <code>R</code> Rejected <code>X</code> Invalid <code>Q</code> Invalid (variant) <code>K</code> Invalid (variant) <p>Votes in the <code>zmatecne</code> table are void and are always filtered out before any analysis.</p>"},{"location":"data-model/#caching-strategy","title":"Caching Strategy","text":"<pre><code>~/.cache/pspcz-analyzer/psp/          (or $PSPCZ_CACHE_DIR)\n    raw/              # Downloaded ZIP files\n    extracted/        # Extracted UNL files\n    parquet/          # Parsed DataFrames cached as Parquet\n    tisky_pdf/        # Downloaded parliamentary print PDFs\n    tisky_text/       # Extracted plain text from PDFs\n    tisky_meta/       # Topic classification + summary Parquet caches\n    tisky_historie/   # Legislative history JSON files\n    tisky_version_diffs/  # LLM diff summaries (Czech + English)\n</code></pre> <p>The Parquet cache uses file modification times: if the Parquet file is newer than the source UNL directory, it's loaded directly. Otherwise the UNL files are re-parsed and the Parquet is regenerated.</p> <p>Column schemas are defined in <code>pspcz_analyzer/models/schemas.py</code> \u2014 each table has a <code>*_COLUMNS</code> list (column order) and a <code>*_DTYPES</code> dict (type casts).</p>"},{"location":"routes/","title":"Routes","text":"<p>All routes accept a <code>period</code> query parameter (default: <code>10</code>, the current electoral period). Changing the period loads data for that period on demand.</p> <p>All page and API routes respect the current UI language (set via the <code>lang</code> cookie). Chart labels and vote outcome labels are localized.</p>"},{"location":"routes/#page-routes","title":"Page Routes","text":"<p>Full HTML pages rendered with Jinja2. Defined in <code>pspcz_analyzer/routes/pages.py</code>.</p> Method Path Description GET <code>/</code> Dashboard \u2014 overview stats for the selected period GET <code>/loyalty</code> Party loyalty analysis page GET <code>/attendance</code> Attendance analysis page (includes vote breakdown + activity ranking) GET <code>/similarity</code> Cross-party voting similarity page GET <code>/votes</code> Votes browser (searchable, paginated) GET <code>/votes/{vote_id}</code> Single vote detail \u2014 per-party and per-MP breakdown GET <code>/set-lang/{lang}</code> Set UI language (<code>cs</code> or <code>en</code>) via cookie and redirect back GET <code>/docs</code> Scalar API documentation UI (not included in OpenAPI schema)"},{"location":"routes/#api-routes-htmx-partials","title":"API Routes (HTMX Partials)","text":"<p>Return HTML fragments for dynamic table updates. Defined in <code>pspcz_analyzer/routes/api.py</code>. Mounted under <code>/api</code>.</p>"},{"location":"routes/#get-apiloyalty","title":"GET /api/loyalty","text":"Param Type Default Description <code>period</code> int 10 Electoral period <code>top</code> int 30 Number of MPs to show <code>party</code> string <code>\"\"</code> Filter by party code (e.g. <code>ODS</code>, <code>ANO</code>)"},{"location":"routes/#get-apiattendance","title":"GET /api/attendance","text":"Param Type Default Description <code>period</code> int 10 Electoral period <code>top</code> int 30 Number of MPs to show <code>sort</code> string <code>worst</code> Sort order: <code>worst</code> (lowest first), <code>best</code>, or <code>most_active</code> (by volume) <code>party</code> string <code>\"\"</code> Filter by party code (e.g. <code>ODS</code>, <code>ANO</code>)"},{"location":"routes/#get-apiattendance-sortmost_active","title":"GET /api/attendance (sort=most_active)","text":"<p>When <code>sort=most_active</code>, renders the \"Most Active\" view \u2014 MPs ranked by raw volume of active votes (YES + NO + ABSTAINED), rewarding consistent long-term participation.</p>"},{"location":"routes/#get-apisimilarity","title":"GET /api/similarity","text":"Param Type Default Description <code>period</code> int 10 Electoral period <code>top</code> int 20 Number of cross-party pairs"},{"location":"routes/#get-apivotes","title":"GET /api/votes","text":"Param Type Default Description <code>period</code> int 10 Electoral period <code>search</code> string <code>\"\"</code> Full-text search on vote descriptions <code>outcome</code> string <code>\"\"</code> Filter: <code>A</code> (passed), <code>R</code> (rejected), or empty for all <code>topic</code> string <code>\"\"</code> Filter by topic label (from keyword or LLM classification) <code>page</code> int 1 Page number"},{"location":"routes/#get-apitisk-text","title":"GET /api/tisk-text","text":"<p>Returns extracted PDF text for a parliamentary print as an HTML fragment (for lazy-loading via HTMX on vote detail pages).</p> Param Type Default Description <code>period</code> int 10 Electoral period <code>ct</code> int 0 Print number (cislo tisku)"},{"location":"routes/#get-apitisk-evolution","title":"GET /api/tisk-evolution","text":"<p>Returns the legislative evolution view for a parliamentary print, including sub-versions and LLM-generated diff summaries (bilingual \u2014 displays English when <code>lang=en</code>).</p> Param Type Default Description <code>period</code> int 10 Electoral period <code>ct</code> int (required) Print number"},{"location":"routes/#get-apihealth","title":"GET /api/health","text":"<p>Health check endpoint returning JSON.</p> <p>Response: <pre><code>{\"status\": \"ok\", \"periods_loaded\": [10, 9]}\n</code></pre></p>"},{"location":"routes/#chart-routes","title":"Chart Routes","text":"<p>Return PNG images via <code>StreamingResponse</code>. Defined in <code>pspcz_analyzer/routes/charts.py</code>. Mounted under <code>/charts</code>.</p> Method Path Params Description GET <code>/charts/loyalty.png</code> <code>period</code>, <code>top=20</code> Horizontal bar chart \u2014 rebellion rates (coolwarm palette) GET <code>/charts/attendance.png</code> <code>period</code>, <code>top=20</code> Horizontal bar chart \u2014 worst attendance (RdYlGn palette) GET <code>/charts/similarity.png</code> <code>period</code> PCA scatter plot \u2014 MPs colored by party (husl palette) <p>All charts render at 150 DPI with a dark background (<code>#1a1a2e</code>). Labels and titles are localized based on the current UI language.</p>"},{"location":"routes/#openapi","title":"OpenAPI","text":"<p>The full OpenAPI schema is available at <code>/openapi.json</code>. The interactive API documentation (Scalar UI) is at <code>/docs</code>. Default Swagger UI and ReDoc are disabled in favor of Scalar.</p>"},{"location":"services/","title":"Services","text":""},{"location":"services/#data-pipeline","title":"Data Pipeline","text":"<p>Data flows through three stages before it reaches the analysis services:</p> <pre><code>psp.cz ZIPs  \u2192  UNL files (extracted)  \u2192  Polars DataFrames  \u2192  Parquet cache\n     \u2193                  \u2193                        \u2193\n downloader.py      parser.py               cache.py\n</code></pre>"},{"location":"services/#1-download-datadownloaderpy","title":"1. Download (<code>data/downloader.py</code>)","text":"<p>Downloads ZIP archives from <code>https://www.psp.cz/eknih/cdrom/opendata</code>:</p> <ul> <li><code>hl-{year}ps.zip</code> \u2014 voting data for a specific period</li> <li><code>poslanci.zip</code> \u2014 MP/person/organ/membership data (shared)</li> <li><code>schuze.zip</code> \u2014 session and agenda data</li> <li><code>tisky.zip</code> \u2014 parliamentary prints (bills, proposals)</li> </ul> <p>Files are cached in <code>~/.cache/pspcz-analyzer/psp/raw/</code> (or <code>$PSPCZ_CACHE_DIR/raw/</code>). Skips download if the file already exists.</p>"},{"location":"services/#2-parse-dataparserpy","title":"2. Parse (<code>data/parser.py</code>)","text":"<p>UNL files are pipe-delimited with no header row, Windows-1250 encoded, and have a trailing pipe on each line (producing an extra empty column that gets dropped).</p> <ul> <li><code>parse_unl()</code> \u2014 parses a single file given column names and optional dtype casts</li> <li><code>parse_unl_multi()</code> \u2014 parses multiple files matching a glob pattern and concatenates them (used for per-session vote files like <code>hl2025h1.unl</code>, <code>hl2025h2.unl</code>, ...)</li> </ul> <p>CSV quoting is always disabled (<code>quote_char=None</code>) because UNL files never use CSV-style quoting \u2014 any double quotes in the data are literal characters.</p>"},{"location":"services/#3-cache-datacachepy","title":"3. Cache (<code>data/cache.py</code>)","text":"<p><code>get_or_parse()</code> checks if a Parquet file exists and is newer than the source. If so, it loads from Parquet; otherwise it calls the parse function and caches the result. Cache lives at <code>~/.cache/pspcz-analyzer/psp/parquet/</code> (or <code>$PSPCZ_CACHE_DIR/parquet/</code>).</p>"},{"location":"services/#dataservice-servicesdata_servicepy","title":"DataService (<code>services/data_service.py</code>)","text":"<p>Central orchestrator, initialized at app startup via FastAPI lifespan and stored on <code>app.state.data</code>.</p>"},{"location":"services/#shared-tables","title":"Shared Tables","text":"<p>Loaded once, used across all periods:</p> <ul> <li>osoby \u2014 persons (id_osoba, name, etc.)</li> <li>poslanec \u2014 MP records linking person to period</li> <li>organy \u2014 organs/organizations (parties, committees, etc.)</li> <li>zarazeni \u2014 memberships (which person belongs to which organ)</li> <li>schuze / bod_schuze / tisky \u2014 sessions, agenda items, parliamentary prints</li> </ul>"},{"location":"services/#per-period-data-perioddata","title":"Per-Period Data (<code>PeriodData</code>)","text":"<p>Loaded on demand when a period is first requested:</p> <ul> <li>votes (<code>hl_hlasovani</code>) \u2014 vote summaries (date, result, counts)</li> <li>mp_votes (<code>hl_poslanec</code>) \u2014 individual MP votes per vote event</li> <li>void_votes (<code>zmatecne</code>) \u2014 IDs of void votes (always filtered out)</li> <li>mp_info \u2014 derived table: id_poslanec \u2192 name + current party</li> <li>tisk_lookup \u2014 <code>dict[(schuze_num, bod_num), TiskInfo]</code> linking votes to parliamentary prints</li> </ul>"},{"location":"services/#tisk-lookup","title":"Tisk Lookup","text":"<p>Two strategies for linking votes to parliamentary prints:</p> <ol> <li>Primary: schuze \u2192 bod_schuze \u2192 tisky (via session/agenda item IDs)</li> <li>Fallback: text-match vote descriptions against tisk names (for new periods where schuze.zip hasn't been updated yet)</li> </ol>"},{"location":"services/#i18n-i18n","title":"i18n (<code>i18n/</code>)","text":"<p>Dict-based Czech/English UI localization. Language is determined per-request from a cookie.</p>"},{"location":"services/#architecture","title":"Architecture","text":"<ul> <li><code>i18n/__init__.py</code> \u2014 Core module: <code>contextvars.ContextVar</code> for locale, <code>gettext(key)</code> / <code>ngettext(singular, plural, n)</code> lookup functions, <code>setup_jinja2_i18n(env)</code> to install Jinja2 i18n extension</li> <li><code>i18n/translations.py</code> \u2014 <code>TRANSLATIONS: dict[str, dict[str, str]]</code> with <code>\"cs\"</code> and <code>\"en\"</code> keys containing all UI strings</li> <li><code>i18n/middleware.py</code> \u2014 <code>LocaleMiddleware(BaseHTTPMiddleware)</code> reads <code>lang</code> cookie, calls <code>set_locale()</code>, sets <code>request.state.lang</code></li> </ul>"},{"location":"services/#how-it-works","title":"How It Works","text":"<ol> <li><code>LocaleMiddleware</code> reads the <code>lang</code> cookie on each request (default: <code>\"cs\"</code>)</li> <li>Sets <code>contextvars.ContextVar</code> so <code>gettext()</code> resolves the correct language</li> <li>Jinja2 templates use <code>{{ _(\"key\") }}</code> which calls <code>gettext()</code></li> <li>Chart labels and vote outcome labels also use <code>gettext()</code> for localization</li> <li>The <code>/set-lang/{lang}</code> endpoint sets the cookie and redirects back</li> </ol>"},{"location":"services/#contextvar-propagation","title":"ContextVar Propagation","text":"<p><code>run_with_timeout</code> in <code>middleware.py</code> uses <code>contextvars.copy_context().run()</code> to propagate the locale ContextVar into thread pool workers, ensuring chart rendering and analysis computations use the correct language.</p>"},{"location":"services/#analysis-services","title":"Analysis Services","text":"<p>All services take a <code>PeriodData</code> instance and return <code>list[dict]</code>. Void votes are always excluded.</p>"},{"location":"services/#loyalty-servicesloyalty_servicepy","title":"Loyalty (<code>services/loyalty_service.py</code>)","text":"<p>Computes rebellion rates \u2014 how often an MP votes against their party's majority.</p> <ol> <li>Filter to active votes only (YES or NO; abstentions excluded)</li> <li>For each (vote, party) pair, determine majority direction (YES or NO; ties excluded)</li> <li>An MP \"rebels\" when their vote differs from the party majority</li> <li><code>rebellion_pct = rebellions / active_votes_with_clear_direction * 100</code></li> </ol> <p>Supports filtering by party code.</p>"},{"location":"services/#attendance-servicesattendance_servicepy","title":"Attendance (<code>services/attendance_service.py</code>)","text":"<p>Computes participation rates with category breakdowns and vote type breakdown (YES/NO/ABSTAINED).</p> <p>Vote categories: - Active: YES (<code>A</code>), NO (<code>B</code>), ABSTAINED (<code>C</code>) - Passive: registered but no button press (<code>F</code>) - Absent: not registered (<code>@</code>) - Excused: formally excused (<code>M</code>)</p> <p>Formula: <code>attendance_pct = active / (total - excused) * 100</code></p> <p>Excused absences are excluded from the denominator (legitimate absences don't penalize).</p> <p>Sort modes: - <code>worst</code> \u2014 lowest attendance first (default) - <code>best</code> \u2014 highest attendance first - <code>most_active</code> \u2014 ranked by raw volume of active votes (YES + NO + ABSTAINED), rewarding consistent long-term participation</p> <p>Supports filtering by party code.</p>"},{"location":"services/#similarity-servicessimilarity_servicepy","title":"Similarity (<code>services/similarity_service.py</code>)","text":"<p>Two outputs from the same vote matrix (MPs x votes, values: +1 YES, -1 NO, 0 other):</p> <p>PCA Projection (<code>compute_pca_coords</code>): - Centers the matrix, runs SVD, projects to 2D - Returns (x, y) coordinates per MP for scatter plot visualization</p> <p>Cross-Party Pairs (<code>compute_cross_party_similarity</code>): - Computes cosine similarity between all MP pairs - Filters to cross-party pairs only - Returns top N most similar pairs</p>"},{"location":"services/#votes-servicesvotes_servicepy","title":"Votes (<code>services/votes_service.py</code>)","text":"<p><code>list_votes()</code> \u2014 paginated vote listing with text search, outcome filtering, and topic filtering. Enriches each row with tisk links (to psp.cz source documents). Outcome labels are localized via <code>gettext()</code>.</p> <p><code>vote_detail()</code> \u2014 full breakdown of a single vote: metadata, per-party aggregates (YES/NO/ABSTAINED/etc. per party), per-MP individual votes, legislative history timeline, bilingual AI summary, and topic labels.</p>"},{"location":"services/#tisk-pipeline-services","title":"Tisk Pipeline Services","text":"<p>The tisk (parliamentary print) pipeline runs as a background process, downloading PDFs, extracting text, classifying topics, generating bilingual summaries, and scraping legislative histories.</p>"},{"location":"services/#tisk-pipeline-service-servicestisk_pipeline_servicepy","title":"Tisk Pipeline Service (<code>services/tisk_pipeline_service.py</code>)","text":"<p>Background processing orchestrator that coordinates the full tisk data enrichment pipeline. Started automatically at app startup for all periods (newest first).</p> <p>Pipeline stages per period: 1. Download \u2014 fetch PDF documents from psp.cz for each print 2. Extract \u2014 convert PDFs to plain text using PyMuPDF 3. Classify \u2014 assign topic labels via Ollama LLM (or keyword fallback) 4. Summarize \u2014 generate bilingual (Czech + English) summaries via Ollama 5. Consolidate \u2014 merge per-tisk topic classifications into a single Parquet cache 6. Scrape histories \u2014 fetch legislative process timelines from psp.cz HTML pages</p> <p>Key class: <code>TiskPipelineService</code> - <code>start_period(period)</code> \u2014 launch background pipeline for a single period - <code>start_all_periods()</code> \u2014 launch pipeline for all configured periods sequentially - <code>is_running(period)</code> \u2014 check if a period's pipeline is still running - <code>cancel_all()</code> \u2014 cancel all running pipeline tasks (used by the daily refresh service)</p>"},{"location":"services/#tisk-text-service-servicestisk_text_servicepy","title":"Tisk Text Service (<code>services/tisk_text_service.py</code>)","text":"<p>Cache and retrieval layer for extracted tisk PDF text. Used by the <code>/api/tisk-text</code> endpoint for lazy-loading on vote detail pages.</p> <p>Key class: <code>TiskTextService</code> - <code>get_text(period, ct)</code> \u2014 retrieve cached plain text for a print, or <code>None</code> if not yet extracted - <code>has_text(period, ct)</code> \u2014 check if text exists in cache - <code>available_tisky(period)</code> \u2014 list all print numbers with cached text</p> <p>Text files are stored at <code>~/.cache/pspcz-analyzer/psp/tisky_text/{period}/{ct}.txt</code>.</p>"},{"location":"services/#topic-service-servicestopic_servicepy","title":"Topic Service (<code>services/topic_service.py</code>)","text":"<p>Keyword-based topic classifier \u2014 the fast, offline fallback when Ollama is unavailable.</p> <p>Uses a <code>TOPIC_TAXONOMY</code> dictionary mapping topic labels to keyword lists. A tisk is assigned a topic if its name or extracted text contains any of the topic's keywords.</p> <p>Functions: - <code>classify_tisk(name, text)</code> \u2014 returns all matching topic labels - <code>classify_tisk_primary_label(name, text)</code> \u2014 returns the single best-matching topic</p>"},{"location":"services/#ollama-service-servicesollama_servicepy","title":"Ollama Service (<code>services/ollama_service.py</code>)","text":"<p>LLM-based topic classification, summarization, and version comparison using Ollama. This is optional \u2014 if Ollama is not running, the system falls back to keyword classification.</p> <p>Supports both local Ollama (no auth) and remote HTTPS Ollama (Bearer token authentication via <code>OLLAMA_API_KEY</code>).</p> <p>Key class: <code>OllamaClient</code> - <code>is_available()</code> \u2014 health check against the Ollama API (with auth headers if configured) - <code>classify_topics(name, text)</code> \u2014 LLM-based multi-label topic classification - <code>summarize(name, text)</code> \u2014 generate a concise Czech summary - <code>summarize_en(name, text)</code> \u2014 generate a concise English summary - <code>summarize_bilingual(name, text)</code> \u2014 generate summaries in both Czech and English - <code>compare_versions(text1, text2)</code> \u2014 generate a Czech diff summary between two tisk versions - <code>compare_versions_bilingual(text1, text2)</code> \u2014 generate diff summaries in both Czech and English - <code>consolidate_topics(topics_by_ct)</code> \u2014 ask the LLM to merge/deduplicate topic labels across a period</p>"},{"location":"services/#diagnostic-endpoints","title":"Diagnostic Endpoints","text":"<p>Two JSON endpoints for verifying Ollama connectivity without running the full tisk pipeline:</p> <ul> <li><code>GET /api/ollama/health</code> \u2014 Connection check. Returns <code>{\"available\": true/false, \"base_url\": \"...\", \"model\": \"...\"}</code>. Rate limit: 10/minute.</li> <li><code>GET /api/ollama/smoke-test</code> \u2014 Concurrent bilingual generation test using a hardcoded Czech legislative sample. Fires two parallel LLM calls (Czech + English summaries) and measures wall-clock time. Returns <code>{\"success\": true, \"model\": \"...\", \"duration_seconds\": 4.2, \"summary_cs\": \"...\", \"summary_en\": \"...\", ...}</code>. Returns 503 if Ollama is down, 502 on generation failure. Rate limit: 2/minute.</li> </ul> <p>Configuration (in <code>config.py</code>, overridable via <code>.env</code>):</p> Variable Default Description <code>OLLAMA_BASE_URL</code> <code>http://localhost:11434</code> Ollama API endpoint <code>OLLAMA_API_KEY</code> (empty) Bearer token for remote HTTPS Ollama <code>OLLAMA_MODEL</code> <code>qwen3:8b</code> Model for inference <code>OLLAMA_TIMEOUT</code> <code>300.0</code> Per-request timeout in seconds <code>OLLAMA_MAX_TEXT_CHARS</code> <code>50000</code> Max text length sent to LLM <code>OLLAMA_VERBATIM_CHARS</code> <code>40000</code> Chars included verbatim (rest truncated) <p>If Ollama is not running or unreachable, the system silently falls back to keyword-based classification.</p>"},{"location":"services/#tisk-version-service-servicestisk_version_servicepy","title":"Tisk Version Service (<code>services/tisk_version_service.py</code>)","text":"<p>Compares different versions (sub-tisky) of the same parliamentary print using LLM-generated diff summaries. Produces bilingual (Czech + English) comparison summaries stored as separate text files.</p>"},{"location":"services/#tisk-cache-manager-servicestisk_cache_managerpy","title":"Tisk Cache Manager (<code>services/tisk_cache_manager.py</code>)","text":"<p>Manages loading and caching of tisk enrichment data (topic classifications, summaries, English summaries, version diffs, legislative histories) from the file-based cache.</p>"},{"location":"services/#daily-refresh-service-servicesdaily_refresh_servicepy","title":"Daily Refresh Service (<code>services/daily_refresh_service.py</code>)","text":"<p>Asyncio-based daily scheduler that re-downloads fresh data from psp.cz and reloads all in-memory state. Ensures the app serves up-to-date voting data without manual restarts.</p>"},{"location":"services/#how-it-works_1","title":"How It Works","text":"<ol> <li>Sleeps until the configured hour (default: 03:00 CET)</li> <li>Pauses the tisk AI pipeline via <code>TiskPipelineService.cancel_all()</code></li> <li>Re-downloads shared tables (MPs, organs, sessions, tisky) with <code>force=True</code></li> <li>Re-downloads and reloads each loaded electoral period's voting data</li> <li>Invalidates all analysis caches</li> <li>Restarts the tisk pipeline (resumes incrementally \u2014 no AI work is lost)</li> </ol> <p>Key class: <code>DailyRefreshService</code> - <code>start()</code> \u2014 start the scheduler loop (idempotent, respects <code>DAILY_REFRESH_ENABLED</code>) - <code>stop()</code> \u2014 cancel the scheduler gracefully - <code>trigger_now()</code> \u2014 manually trigger an immediate refresh (for admin/debug use)</p> <p>Configuration:</p> Variable Default Description <code>DAILY_REFRESH_ENABLED</code> <code>1</code> <code>1</code> to enable, <code>0</code> to disable <code>DAILY_REFRESH_HOUR</code> <code>3</code> Hour (CET, 0-23) at which the daily refresh runs <p>The refresh itself takes ~1-5 minutes (downloads + parsing). The tisk AI pipeline's incremental resume logic (Parquet checkpointing, JSON caching, file caching) ensures no AI work is redone after restart.</p>"},{"location":"services/#data-enrichment-modules","title":"Data Enrichment Modules","text":""},{"location":"services/#tisk-downloader-datatisk_downloaderpy","title":"Tisk Downloader (<code>data/tisk_downloader.py</code>)","text":"<p>Downloads PDF documents for parliamentary prints from psp.cz.</p> <ul> <li><code>download_tisk_pdf(period, ct)</code> \u2014 download a single print's PDF</li> <li><code>download_period_tisky(period, ct_list)</code> \u2014 batch download for a period</li> </ul> <p>PDFs are cached at <code>~/.cache/pspcz-analyzer/psp/tisky_pdf/{period}/{ct}.pdf</code>.</p>"},{"location":"services/#tisk-extractor-datatisk_extractorpy","title":"Tisk Extractor (<code>data/tisk_extractor.py</code>)","text":"<p>Extracts plain text from downloaded PDF files using PyMuPDF (fitz).</p> <ul> <li><code>extract_text_from_pdf(pdf_path)</code> \u2014 extract text from a single PDF</li> <li><code>extract_and_cache(period, ct)</code> \u2014 extract and save to the text cache</li> <li><code>extract_period_texts(period)</code> \u2014 batch extract all PDFs for a period</li> </ul>"},{"location":"services/#tisk-scraper-datatisk_scraperpy","title":"Tisk Scraper (<code>data/tisk_scraper.py</code>)","text":"<p>Scrapes psp.cz HTML pages to discover available PDF documents for a given print.</p> <ul> <li><code>scrape_tisk_documents(period, ct)</code> \u2014 returns list of <code>TiskDocument</code> objects (URLs, types)</li> <li><code>get_best_pdf(documents)</code> \u2014 selects the most relevant PDF from available documents</li> </ul>"},{"location":"services/#history-scraper-datahistory_scraperpy","title":"History Scraper (<code>data/history_scraper.py</code>)","text":"<p>Scrapes legislative process history from psp.cz HTML pages for each parliamentary print.</p> <ul> <li><code>scrape_tisk_history(period, ct)</code> \u2014 returns a <code>TiskHistory</code> object with a list of <code>TiskHistoryStage</code> entries</li> <li><code>save_history_json(period, ct, history)</code> / <code>load_history_json(period, ct)</code> \u2014 JSON cache persistence</li> </ul> <p>Each <code>TiskHistoryStage</code> contains: - <code>stage_type</code> \u2014 e.g. \"1. \u010dten\u00ed\", \"2. \u010dten\u00ed\", \"3. \u010dten\u00ed\", \"Sen\u00e1t\", \"Prezident\" - <code>label</code> \u2014 human-readable label - <code>date</code> \u2014 when the stage occurred - <code>outcome</code> \u2014 result text (approved, rejected, etc.) - <code>vote_number</code> \u2014 link to the specific vote, if applicable</p>"},{"location":"templates/","title":"Templates","text":"<p>Frontend is server-rendered HTML with HTMX for dynamic updates. No JavaScript framework \u2014 just Jinja2 templates + HTMX attributes. All user-visible strings are localized via the Jinja2 i18n extension.</p>"},{"location":"templates/#localization-i18n","title":"Localization (i18n)","text":"<p>All templates use <code>{{ _(\"key\") }}</code> for translatable strings. The Jinja2 i18n extension is installed on all template environments at startup via <code>setup_jinja2_i18n()</code>.</p> <p>The <code>lang</code> variable (from <code>request.state.lang</code>, set by <code>LocaleMiddleware</code>) is available in all template contexts and controls:</p> <ul> <li>Which translation strings are displayed</li> <li>Which AI summary variant to show (Czech or English, with fallback)</li> <li>The <code>&lt;html lang=\"...\"&gt;</code> attribute</li> </ul>"},{"location":"templates/#base-layout-templatesbasehtml","title":"Base Layout (<code>templates/base.html</code>)","text":"<p>All pages extend <code>base.html</code>, which provides:</p> <ul> <li>Head: Pico CSS v2 (CDN), HTMX v2.0.4 (CDN), external stylesheet (<code>/static/style.css</code>)</li> <li>Favicon: SVG favicon (<code>/static/favicon.svg</code>) \u2014 Czech lion motif in institutional blue</li> <li>Meta tags: description, theme-color for mobile browsers</li> <li>Navigation: Logo + \"PSP.cz Analyzer\" branding, page links, API Docs link, period selector</li> <li>Language switcher: CZ/EN toggle links in the header (active language highlighted)</li> <li>Footer: data source attribution + educational project disclaimer (separated by CSS border)</li> <li>Theme: Pico CSS dark mode (<code>data-theme=\"dark\"</code>)</li> </ul> <p>Active navigation item is highlighted via <code>aria-current=\"page\"</code>, controlled by the <code>active_page</code> template variable.</p>"},{"location":"templates/#page-templates","title":"Page Templates","text":"<p>Each page template renders a full page with controls and a <code>#results</code> div that HTMX populates:</p> Template Page Controls <code>index.html</code> Dashboard None (static stats + feature cards) <code>loyalty.html</code> Party Loyalty Top N slider, party filter dropdown <code>attendance.html</code> Attendance Top N slider, sort toggle (worst/best/most active), party filter <code>similarity.html</code> Similarity Top N slider <code>votes.html</code> Votes Browser Search input, outcome filter, topic filter, pagination <code>vote_detail.html</code> Vote Detail None (static detail view)"},{"location":"templates/#vote-detail-template-vote_detailhtml","title":"Vote Detail Template (<code>vote_detail.html</code>)","text":"<p>The most complex template. Displays:</p> <ol> <li>Vote metadata \u2014 ID, date/time, session/vote numbers, outcome badge</li> <li>Topic tags \u2014 colored badges from keyword or LLM classification</li> <li>AI summary \u2014 bilingual summary card (shows English when <code>lang == \"en\"</code> and English summary exists, otherwise Czech)</li> <li>Current stage \u2014 badge showing which legislative stage this vote belongs to</li> <li>Legislative timeline \u2014 visual timeline of the bill's progress through parliament (CSS timeline with dots, active stage highlighted with glow effect)</li> <li>External links \u2014 to psp.cz vote page and source document</li> <li>Tisk transcription \u2014 lazy-loaded via HTMX (<code>hx-trigger=\"intersect once\"</code>) from <code>/api/tisk-text</code></li> <li>Tisk evolution \u2014 lazy-loaded sub-version comparison with bilingual LLM diff summaries</li> <li>Vote counts \u2014 stat cards (For/Against/Abstained/Did not vote)</li> <li>Party breakdown \u2014 table with per-party vote counts</li> <li>Individual MPs \u2014 collapsible table of all MP votes with color-coded results</li> </ol>"},{"location":"templates/#partials-templatespartials","title":"Partials (<code>templates/partials/</code>)","text":"<p>HTML fragments returned by <code>/api/*</code> endpoints for HTMX swaps:</p> Partial Endpoint Content <code>loyalty_table.html</code> <code>/api/loyalty</code> Ranked table of MPs by rebellion rate <code>attendance_table.html</code> <code>/api/attendance</code> Ranked table of MPs by attendance % (with vote breakdown) <code>similarity_table.html</code> <code>/api/similarity</code> Table of cross-party MP pairs by similarity <code>votes_list.html</code> <code>/api/votes</code> Paginated vote rows with tisk links <code>tisk_evolution.html</code> <code>/api/tisk-evolution</code> Sub-version comparison with bilingual diff summaries"},{"location":"templates/#htmx-pattern","title":"HTMX Pattern","text":"<p>Each analysis page follows the same pattern:</p> <pre><code>&lt;!-- Form with HTMX attributes --&gt;\n&lt;form hx-get=\"/api/loyalty\"\n      hx-target=\"#results\"\n      hx-trigger=\"load, submit\"\n      hx-indicator=\"#loading\"&gt;\n    &lt;!-- Controls (sliders, filters) --&gt;\n&lt;/form&gt;\n\n&lt;!-- Loading spinner --&gt;\n&lt;div id=\"loading\" class=\"htmx-indicator\"&gt;Loading...&lt;/div&gt;\n\n&lt;!-- Results container (swapped by HTMX) --&gt;\n&lt;div id=\"results\"&gt;&lt;/div&gt;\n</code></pre> <ul> <li><code>hx-trigger=\"load\"</code> fires on page load for initial data fetch</li> <li><code>hx-trigger=\"submit\"</code> fires when the user changes controls</li> <li><code>hx-indicator</code> shows a spinner during requests</li> <li>The API returns an HTML partial that replaces <code>#results</code></li> </ul>"},{"location":"templates/#skeleton-loading-states","title":"Skeleton Loading States","text":"<p>When switching periods or navigating between pages, JavaScript replaces <code>&lt;main&gt;</code> content with animated skeleton placeholders:</p> <ul> <li><code>skeleton-pulse</code> \u2014 base class with pulsing animation (0.4\u21920.8 opacity)</li> <li><code>skeleton-heading</code> \u2014 placeholder for headings (2rem height, 50% width)</li> <li><code>skeleton-line</code> \u2014 placeholder for text lines (variants: short/medium/long)</li> <li><code>skeleton-table-row</code> \u2014 placeholder for table rows</li> <li><code>skeleton-card</code> \u2014 placeholder for stat cards</li> </ul> <p>The skeleton is shown immediately on period change (<code>switchPeriod()</code>) and on nav link clicks, before the full page reload completes. This prevents stale data from being visible during loading.</p>"},{"location":"templates/#charts","title":"Charts","text":"<p>Chart images are embedded as <code>&lt;img&gt;</code> tags pointing to <code>/charts/*.png</code> endpoints. They render server-side via matplotlib/seaborn with a dark theme matching the UI (<code>#1a1a2e</code> background). Chart labels and titles are localized via <code>gettext()</code>.</p>"},{"location":"templates/#styling","title":"Styling","text":"<p>All custom CSS lives in <code>/static/style.css</code> (external file). Pico CSS v2 dark theme is the base. Key customizations:</p> <ul> <li><code>--pico-border-radius: 0.25rem</code> \u2014 sharper corners for a more formal/institutional feel</li> <li>Legislative timeline styles (dots, lines, active glow)</li> <li>Skeleton loading animation</li> <li>Stat grid layout</li> <li>Navigation period selector styling</li> <li>Language switcher (<code>.lang-switcher</code>) \u2014 flex layout with active state highlight</li> <li>Footer with CSS border separator (no <code>&lt;hr&gt;</code>)</li> </ul>"},{"location":"templates/#methodology-sections","title":"Methodology Sections","text":"<p>Each analysis page includes a collapsible <code>&lt;details&gt;</code> element explaining the calculation methodology \u2014 this serves as inline documentation for users. Methodology text is fully localized.</p>"},{"location":"testing/","title":"Testing &amp; CI/CD","text":""},{"location":"testing/#quick-reference","title":"Quick Reference","text":"<pre><code># Install dev dependencies\nuv sync --extra dev\n\n# Run unit + API tests (no network required)\nuv run pytest -m \"not integration\" --cov\n\n# Run integration tests (hits real psp.cz)\nuv run pytest -m integration -v\n\n# Lint + format\nuv run ruff check .\nuv run ruff format .\n\n# Type check\nuv run pyright\n\n# Pre-commit hooks (all files)\nuv run pre-commit run --all-files\n</code></pre>"},{"location":"testing/#test-suite","title":"Test Suite","text":"<p>Tests live in <code>tests/</code> and are organized into three layers:</p>"},{"location":"testing/#unit-tests-testsunit","title":"Unit Tests (<code>tests/unit/</code>)","text":"<p>Pure logic tests with synthetic data \u2014 no network, no disk I/O beyond <code>tmp_path</code>.</p> <p>Data layer:</p> File Tests What it covers <code>test_parser.py</code> 8 UNL parsing: encoding (Windows-1250 \u2192 UTF-8), trailing pipe handling, dtype casting, <code>quote_char=None</code>, empty files, Czech diacritics <code>test_cache.py</code> 3 Parquet round-trip, staleness detection (mtime comparison), missing source fallback <p>Analysis services (<code>tests/unit/services/</code>):</p> File Tests What it covers <code>test_loyalty.py</code> 10 Rebellion rate computation, party filter (case-insensitive), empty data edge case, result sorting, rebellion vote details <code>test_attendance.py</code> 6 Attendance formula (<code>active / (total - excused) * 100</code>), sort modes (<code>best</code> vs <code>worst</code>), field validation <code>test_similarity.py</code> 9 PCA produces 2D coords per MP, cross-party pairs exclude same-party, cosine similarity in [-1, 1] range <code>test_activity.py</code> 5 Vote breakdown fields, party filter, <code>most_active</code> sort mode, active count verification <code>test_votes.py</code> 11 Vote search by description text, pagination (page size, page navigation), vote detail with party breakdown, nonexistent vote returns None"},{"location":"testing/#api-tests-testsapi","title":"API Tests (<code>tests/api/</code>)","text":"<p>Use FastAPI's <code>TestClient</code> with a mocked <code>DataService</code> \u2014 no real downloads. The test client goes through the full middleware stack (including <code>LocaleMiddleware</code> for i18n).</p> File Tests What it covers <code>test_pages.py</code> 5 All page routes (<code>/</code>, <code>/loyalty</code>, <code>/attendance</code>, <code>/similarity</code>, <code>/votes</code>) return 200 + HTML <code>test_api_endpoints.py</code> 6 HTMX partials (<code>/api/loyalty</code>, <code>/api/attendance</code>, etc.) return 200 + HTML, <code>/api/health</code> returns JSON <code>{\"status\": \"ok\"}</code>, invalid period returns 404 <code>test_charts.py</code> 3 Chart endpoints (<code>/charts/loyalty.png</code>, etc.) return <code>image/png</code> with valid PNG magic bytes"},{"location":"testing/#integration-tests-testsintegration","title":"Integration Tests (<code>tests/integration/</code>)","text":"<p>Hit real psp.cz infrastructure \u2014 marked with <code>@pytest.mark.integration</code> and excluded from default <code>pytest</code> runs.</p> File Tests What it covers <code>test_download.py</code> 6 ZIP downloads succeed, contain expected UNL file patterns <code>test_parsing.py</code> 7 Real UNL files parse with our schema definitions, column counts match, ID columns non-null, schema drift canary (no all-null typed columns) <code>test_pipeline.py</code> 9 Full end-to-end: download \u2192 parse \u2192 DataService \u2192 all analysis services produce non-empty results <p>Design decisions: - Uses period 1 (1993) \u2014 smallest dataset, fastest downloads - Session-scoped fixtures download data once per test run, shared across all integration tests - Uses the real cache directory (<code>~/.cache/pspcz-analyzer/psp/</code>) so downloads persist across runs</p>"},{"location":"testing/#test-fixtures","title":"Test Fixtures","text":""},{"location":"testing/#testsfixturessample_datapy","title":"<code>tests/fixtures/sample_data.py</code>","text":"<p>Factory functions that create synthetic Polars DataFrames matching real schemas:</p> <ul> <li><code>make_votes(n=5)</code> \u2014 voting summary with all <code>HL_HLASOVANI</code> columns</li> <li><code>make_mp_votes()</code> \u2014 6 MPs with known patterns:</li> <li>MPs 1-2 (ANO): always YES (loyal)</li> <li>MP 3 (ODS): NO on 3/5 votes (rebel, 60% rebellion rate)</li> <li>MPs 4, 6 (ODS): always YES (establishes party majority)</li> <li>MP 5 (STAN): mixed results (YES, ABSENT, EXCUSED, DID_NOT_VOTE, ABSTAINED) \u2014 tests attendance formula</li> <li><code>make_mp_info()</code> \u2014 6 MPs across 3 parties (ANO, ODS, STAN)</li> <li><code>make_void_votes()</code> \u2014 empty DataFrame (no void votes in test data)</li> <li><code>make_period_data(period=1)</code> \u2014 assembles all above into a <code>PeriodData</code> dataclass</li> </ul>"},{"location":"testing/#testsconftestpy","title":"<code>tests/conftest.py</code>","text":"<p>Shared fixtures:</p> <ul> <li><code>mock_period_data</code> \u2014 calls <code>make_period_data()</code></li> <li><code>mock_data_service</code> \u2014 <code>MagicMock(spec=DataService)</code> that returns the synthetic data</li> <li><code>client</code> \u2014 <code>TestClient(app)</code> with a custom lifespan that injects the mock service (no real downloads)</li> <li><code>test_cache_dir</code> \u2014 <code>tmp_path</code>-based isolated cache directory</li> </ul>"},{"location":"testing/#linting-formatting","title":"Linting &amp; Formatting","text":""},{"location":"testing/#ruff","title":"Ruff","text":"<p>Configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff]\ntarget-version = \"py312\"\nline-length = 100\n\n[tool.ruff.lint]\nselect = [\"E\", \"W\", \"F\", \"I\", \"UP\", \"B\", \"C4\", \"SIM\"]\n</code></pre> <p>Key rule choices: - E, W, F \u2014 pycodestyle errors/warnings + pyflakes (standard) - I \u2014 isort import ordering - UP \u2014 pyupgrade (modernize syntax for py312+) - B \u2014 flake8-bugbear (common gotchas) - C4 \u2014 flake8-comprehensions - SIM \u2014 simplify (with SIM102/SIM108/SIM117/C408 suppressed \u2014 readability preference)</p> <p>Suppressed rules: - <code>E501</code> \u2014 line length handled by the formatter - <code>B008</code> \u2014 FastAPI <code>Depends()</code> pattern uses function calls as defaults - <code>SIM102/108/117</code>, <code>C408</code> \u2014 style preferences that don't warrant rewriting existing code</p>"},{"location":"testing/#pyright","title":"Pyright","text":"<p>Basic mode \u2014 the codebase isn't fully annotated yet. Reports missing imports but tolerates missing type stubs.</p>"},{"location":"testing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p><code>.pre-commit-config.yaml</code> runs on every commit:</p> <ol> <li>pre-commit-hooks \u2014 trailing whitespace, end-of-file fixer, YAML/TOML syntax, large file check, debug statement detection</li> <li>ruff \u2014 lint with <code>--fix</code> + format check</li> <li>pyright \u2014 type checking</li> </ol> <p>Install hooks: <code>uv run pre-commit install</code></p>"},{"location":"testing/#cicd-github-actions","title":"CI/CD (GitHub Actions)","text":""},{"location":"testing/#ciyml-every-pr-push-to-main","title":"<code>ci.yml</code> \u2014 Every PR + Push to main","text":"<p>Two jobs:</p> <ol> <li>Lint \u2014 <code>ruff check</code> + <code>ruff format --check</code> + <code>pyright</code> (pyright is <code>continue-on-error</code> since the codebase isn't fully typed)</li> <li>Unit Tests \u2014 <code>pytest -m \"not integration\" --cov</code> on Python 3.14</li> </ol> <p>Uses <code>astral-sh/setup-uv@v5</code> with caching for fast dependency installation.</p>"},{"location":"testing/#integrationyml-selective-runs","title":"<code>integration.yml</code> \u2014 Selective Runs","text":"<p>Triggers: - PRs targeting <code>main</code> only (not every feature branch) - Weekly cron (Monday 2:00 AM UTC) \u2014 catches upstream psp.cz format changes - Manual <code>workflow_dispatch</code></p> <p>Single job: <code>pytest -m integration --timeout=300 -v</code> with a 30-minute timeout. Uploads test artifacts on failure for debugging.</p> <p>Why real integration tests? psp.cz is government infrastructure \u2014 stable, but when upstream format changes happen, our parsing breaks silently. The weekly cron job is our early warning system.</p>"},{"location":"testing/#releaseyml-docker-image-publishing","title":"<code>release.yml</code> \u2014 Docker Image Publishing","text":"<p>Trigger: <code>workflow_dispatch</code> (manual)</p> <p>Steps: 1. Checkout code 2. Extract version from <code>pyproject.toml</code> 3. Login to GitHub Container Registry (GHCR) 4. Build Docker image with multi-stage <code>Dockerfile</code> 5. Push with version tag + <code>latest</code> tag</p> <p>Bump version first (<code>uv run bump-my-version bump patch &amp;&amp; git push</code>), then trigger the workflow.</p>"},{"location":"testing/#version-management","title":"Version Management","text":"<p><code>bump-my-version</code> is configured to update the version in <code>pyproject.toml</code>:</p> <pre><code>uv run bump-my-version bump patch   # 0.1.0 \u2192 0.1.1\nuv run bump-my-version bump minor   # 0.1.0 \u2192 0.2.0\nuv run bump-my-version bump major   # 0.1.0 \u2192 1.0.0\n</code></pre>"},{"location":"testing/#adding-new-tests","title":"Adding New Tests","text":"<p>When adding a new analysis service:</p> <ol> <li>Add a factory function in <code>tests/fixtures/sample_data.py</code> if the service needs specific data patterns</li> <li>Create <code>tests/unit/services/test_&lt;service&gt;.py</code> with synthetic data tests</li> <li>Add an integration test in <code>tests/integration/test_pipeline.py</code> to verify it works on real data</li> <li>If the service has an API endpoint, add a test in <code>tests/api/test_api_endpoints.py</code></li> </ol> <p>When adding a new route:</p> <ol> <li>Add a page test in <code>tests/api/test_pages.py</code> (returns 200 + HTML)</li> <li>If it's an HTMX partial, add to <code>tests/api/test_api_endpoints.py</code></li> <li>If it's a chart, add to <code>tests/api/test_charts.py</code> (returns PNG)</li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>pspcz_analyzer<ul> <li>config</li> <li>data<ul> <li>cache</li> <li>downloader</li> <li>history_scraper</li> <li>law_changes_scraper</li> <li>parser</li> <li>tisk_downloader</li> <li>tisk_extractor</li> <li>tisk_scraper</li> </ul> </li> <li>i18n<ul> <li>middleware</li> <li>translations</li> </ul> </li> <li>logging_config</li> <li>main</li> <li>middleware</li> <li>models<ul> <li>enums</li> <li>schemas</li> <li>tisk_models</li> </ul> </li> <li>rate_limit</li> <li>routes<ul> <li>api</li> <li>charts</li> <li>pages</li> </ul> </li> <li>services<ul> <li>analysis_cache</li> <li>attendance_service</li> <li>daily_refresh_service</li> <li>data_service</li> <li>feedback_service</li> <li>loyalty_service</li> <li>mp_builder</li> <li>ollama_service</li> <li>similarity_service</li> <li>tisk_cache_manager</li> <li>tisk_classifier</li> <li>tisk_downloader_pipeline</li> <li>tisk_lookup_builder</li> <li>tisk_metadata_scraper</li> <li>tisk_pipeline_service</li> <li>tisk_text_service</li> <li>tisk_version_service</li> <li>topic_service</li> <li>votes_service</li> </ul> </li> <li>utils<ul> <li>text</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/pspcz_analyzer/","title":"pspcz_analyzer","text":""},{"location":"reference/pspcz_analyzer/#pspcz_analyzer","title":"<code>pspcz_analyzer</code>","text":""},{"location":"reference/pspcz_analyzer/config/","title":"config","text":""},{"location":"reference/pspcz_analyzer/config/#pspcz_analyzer.config","title":"<code>config</code>","text":"<p>Central configuration: URLs, paths, constants.</p>"},{"location":"reference/pspcz_analyzer/logging_config/","title":"logging_config","text":""},{"location":"reference/pspcz_analyzer/logging_config/#pspcz_analyzer.logging_config","title":"<code>logging_config</code>","text":"<p>Application-wide logging configuration using loguru.</p> <p>Intercepts all standard-library logging (from uvicorn, httpx, etc.) and routes it through loguru for consistent colorful output.</p>"},{"location":"reference/pspcz_analyzer/logging_config/#pspcz_analyzer.logging_config.setup_logging","title":"<code>setup_logging()</code>","text":"<p>Configure loguru as the sole logging handler for the entire application.</p> Source code in <code>pspcz_analyzer/logging_config.py</code> <pre><code>def setup_logging() -&gt; None:\n    \"\"\"Configure loguru as the sole logging handler for the entire application.\"\"\"\n    logger.remove()\n    logger.add(\n        sys.stderr,\n        format=(\n            \"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | \"\n            \"&lt;level&gt;{level: &lt;8}&lt;/level&gt; | \"\n            \"&lt;cyan&gt;{name}&lt;/cyan&gt;:&lt;cyan&gt;{function}&lt;/cyan&gt;:&lt;cyan&gt;{line}&lt;/cyan&gt; - \"\n            \"&lt;level&gt;{message}&lt;/level&gt;\"\n        ),\n        colorize=True,\n        level=\"INFO\",\n    )\n\n    # Intercept all standard logging (uvicorn, httpx, etc.)\n    logging.basicConfig(handlers=[_InterceptHandler()], level=0, force=True)\n</code></pre>"},{"location":"reference/pspcz_analyzer/main/","title":"main","text":""},{"location":"reference/pspcz_analyzer/main/#pspcz_analyzer.main","title":"<code>main</code>","text":"<p>Czech Parliamentary Voting Analyzer \u2014 FastAPI application.</p>"},{"location":"reference/pspcz_analyzer/middleware/","title":"middleware","text":""},{"location":"reference/pspcz_analyzer/middleware/#pspcz_analyzer.middleware","title":"<code>middleware</code>","text":"<p>Security headers middleware and computation timeout helper.</p>"},{"location":"reference/pspcz_analyzer/middleware/#pspcz_analyzer.middleware.run_with_timeout","title":"<code>run_with_timeout(fn, *args, timeout=15.0, label='computation')</code>  <code>async</code>","text":"<p>Run a sync function in a bounded thread pool with timeout.</p> <p>Propagates ContextVars (incl. locale) into the worker thread. Returns the result or raises HTTP 503 on timeout.</p> Source code in <code>pspcz_analyzer/middleware.py</code> <pre><code>async def run_with_timeout(\n    fn: Callable[..., Any],\n    *args: Any,\n    timeout: float = 15.0,\n    label: str = \"computation\",\n) -&gt; Any:\n    \"\"\"Run a sync function in a bounded thread pool with timeout.\n\n    Propagates ContextVars (incl. locale) into the worker thread.\n    Returns the result or raises HTTP 503 on timeout.\n    \"\"\"\n    loop = asyncio.get_running_loop()\n    ctx = contextvars.copy_context()\n    try:\n        return await asyncio.wait_for(\n            loop.run_in_executor(_compute_pool, partial(ctx.run, fn, *args)),\n            timeout=timeout,\n        )\n    except TimeoutError as err:\n        logger.warning(\"Timeout after {}s for {}\", timeout, label)\n        raise HTTPException(503, detail=f\"{label} timed out after {timeout}s\") from err\n</code></pre>"},{"location":"reference/pspcz_analyzer/rate_limit/","title":"rate_limit","text":""},{"location":"reference/pspcz_analyzer/rate_limit/#pspcz_analyzer.rate_limit","title":"<code>rate_limit</code>","text":"<p>Rate limiting configuration via slowapi.</p>"},{"location":"reference/pspcz_analyzer/data/","title":"data","text":""},{"location":"reference/pspcz_analyzer/data/#pspcz_analyzer.data","title":"<code>data</code>","text":""},{"location":"reference/pspcz_analyzer/data/cache/","title":"cache","text":""},{"location":"reference/pspcz_analyzer/data/cache/#pspcz_analyzer.data.cache","title":"<code>cache</code>","text":"<p>Parquet caching layer for parsed DataFrames.</p>"},{"location":"reference/pspcz_analyzer/data/cache/#pspcz_analyzer.data.cache.get_or_parse","title":"<code>get_or_parse(table_name, source_path, parse_fn, cache_dir=DEFAULT_CACHE_DIR)</code>","text":"<p>Load from parquet cache if fresh, otherwise parse and cache.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>Used as the parquet filename (e.g. \"hl_hlasovani_9\").</p> required <code>source_path</code> <code>Path</code> <p>The source file/dir whose mtime determines cache staleness.</p> required <code>parse_fn</code> <code>Callable[[], DataFrame]</code> <p>Called to produce the DataFrame if cache is stale.</p> required <code>cache_dir</code> <code>Path</code> <p>Root cache directory.</p> <code>DEFAULT_CACHE_DIR</code> Source code in <code>pspcz_analyzer/data/cache.py</code> <pre><code>def get_or_parse(\n    table_name: str,\n    source_path: Path,\n    parse_fn: Callable[[], pl.DataFrame],\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n) -&gt; pl.DataFrame:\n    \"\"\"Load from parquet cache if fresh, otherwise parse and cache.\n\n    Args:\n        table_name: Used as the parquet filename (e.g. \"hl_hlasovani_9\").\n        source_path: The source file/dir whose mtime determines cache staleness.\n        parse_fn: Called to produce the DataFrame if cache is stale.\n        cache_dir: Root cache directory.\n    \"\"\"\n    parquet_path = _parquet_dir(cache_dir) / f\"{table_name}.parquet\"\n\n    if parquet_path.exists() and source_path.exists():\n        if parquet_path.stat().st_mtime &gt; source_path.stat().st_mtime:\n            logger.info(\"Loading {} from parquet cache\", table_name)\n            return pl.read_parquet(parquet_path)\n\n    logger.info(\"Parsing {} (cache miss or stale)\", table_name)\n    df = parse_fn()\n    df.write_parquet(parquet_path)\n    logger.info(\"Cached {} ({} rows)\", table_name, df.height)\n    return df\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/downloader/","title":"downloader","text":""},{"location":"reference/pspcz_analyzer/data/downloader/#pspcz_analyzer.data.downloader","title":"<code>downloader</code>","text":"<p>Download and extract ZIP files from psp.cz open data.</p>"},{"location":"reference/pspcz_analyzer/data/downloader/#pspcz_analyzer.data.downloader.download_voting_data","title":"<code>download_voting_data(period, cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download and extract voting data for a given electoral period.</p> <p>Returns the path to the extracted directory.</p> Source code in <code>pspcz_analyzer/data/downloader.py</code> <pre><code>def download_voting_data(\n    period: int,\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path:\n    \"\"\"Download and extract voting data for a given electoral period.\n\n    Returns the path to the extracted directory.\n    \"\"\"\n    year = PERIOD_YEARS[period]\n    raw, extracted = _ensure_dirs(cache_dir)\n\n    zip_name = f\"hl-{year}ps.zip\"\n    url = VOTING_URL_TEMPLATE.format(year=year)\n    zip_path = _download_file(url, raw / zip_name, force=force)\n    return _extract_zip(zip_path, extracted)\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/downloader/#pspcz_analyzer.data.downloader.download_poslanci_data","title":"<code>download_poslanci_data(cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download and extract MP/party data.</p> <p>Returns the path to the extracted directory.</p> Source code in <code>pspcz_analyzer/data/downloader.py</code> <pre><code>def download_poslanci_data(\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path:\n    \"\"\"Download and extract MP/party data.\n\n    Returns the path to the extracted directory.\n    \"\"\"\n    raw, extracted = _ensure_dirs(cache_dir)\n\n    zip_path = _download_file(POSLANCI_URL, raw / \"poslanci.zip\", force=force)\n    return _extract_zip(zip_path, extracted)\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/downloader/#pspcz_analyzer.data.downloader.download_schuze_data","title":"<code>download_schuze_data(cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download and extract session/agenda data.</p> Source code in <code>pspcz_analyzer/data/downloader.py</code> <pre><code>def download_schuze_data(\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path:\n    \"\"\"Download and extract session/agenda data.\"\"\"\n    raw, extracted = _ensure_dirs(cache_dir)\n    zip_path = _download_file(SCHUZE_URL, raw / \"schuze.zip\", force=force)\n    return _extract_zip(zip_path, extracted)\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/downloader/#pspcz_analyzer.data.downloader.download_tisky_data","title":"<code>download_tisky_data(cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download and extract parliamentary prints data.</p> Source code in <code>pspcz_analyzer/data/downloader.py</code> <pre><code>def download_tisky_data(\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path:\n    \"\"\"Download and extract parliamentary prints data.\"\"\"\n    raw, extracted = _ensure_dirs(cache_dir)\n    zip_path = _download_file(TISKY_URL, raw / \"tisky.zip\", force=force)\n    return _extract_zip(zip_path, extracted)\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/history_scraper/","title":"history_scraper","text":""},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper","title":"<code>history_scraper</code>","text":"<p>Scrape psp.cz legislative history pages for parliamentary prints (tisky).</p> <p>Each tisk has a history page at historie.sqw?o={period}&amp;t={ct} showing stages of the legislative process: submission, government opinion, readings, committee work, senate, president, publication.</p> <p>The page uses div.section elements with headings (\"P\u0159edkladatel\", \"Poslaneck\u00e1 sn\u011bmovna\", \"Sen\u00e1t\", etc.) and inside, ul.document-log with li.document-log-item items. Each item has span.mark (PS, O, 1, V, 2, G, 3, S, P, VL) and <p> content.</p>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.TiskHistoryStage","title":"<code>TiskHistoryStage(stage_type, label, date=None, session_number=None, vote_number=None, outcome=None, details='')</code>  <code>dataclass</code>","text":"<p>A single stage in the legislative process for a tisk.</p>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.TiskHistory","title":"<code>TiskHistory(ct, period, submitter='', submitter_date=None, government_opinion=None, stages=list(), current_status='projedn\u00e1v\u00e1no', law_number=None, scraped_at='', law_changes=list())</code>  <code>dataclass</code>","text":"<p>Full legislative history for a parliamentary print (tisk).</p>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.scrape_tisk_history","title":"<code>scrape_tisk_history(period, ct)</code>","text":"<p>Scrape the legislative history page for a tisk.</p> <p>Returns TiskHistory with stages, or None if the page couldn't be fetched.</p> Source code in <code>pspcz_analyzer/data/history_scraper.py</code> <pre><code>def scrape_tisk_history(period: int, ct: int) -&gt; TiskHistory | None:\n    \"\"\"Scrape the legislative history page for a tisk.\n\n    Returns TiskHistory with stages, or None if the page couldn't be fetched.\n    \"\"\"\n    url = PSP_HISTORIE_URL_TEMPLATE.format(period=period, ct=ct)\n    logger.debug(\"Scraping tisk history: {}\", url)\n\n    try:\n        with httpx.Client(timeout=30, follow_redirects=True) as client:\n            resp = client.get(url)\n            resp.raise_for_status()\n    except Exception:\n        logger.opt(exception=True).warning(\n            \"Failed to fetch history for tisk {}/{}\",\n            period,\n            ct,\n        )\n        return None\n\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    full_text = soup.get_text(\" \", strip=True)\n\n    stages = _parse_stages(soup)\n    submitter, submitter_date = _extract_submitter(soup)\n    gov_opinion = _extract_government_opinion(soup)\n    law_number = _extract_law_number(full_text)\n    status = _determine_status(stages, full_text)\n\n    return TiskHistory(\n        ct=ct,\n        period=period,\n        submitter=submitter,\n        submitter_date=submitter_date,\n        government_opinion=gov_opinion,\n        stages=stages,\n        current_status=status,\n        law_number=law_number,\n        scraped_at=datetime.now(UTC).isoformat(),\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.history_to_dict","title":"<code>history_to_dict(h)</code>","text":"<p>Serialize TiskHistory to a JSON-compatible dict.</p> Source code in <code>pspcz_analyzer/data/history_scraper.py</code> <pre><code>def history_to_dict(h: TiskHistory) -&gt; dict:\n    \"\"\"Serialize TiskHistory to a JSON-compatible dict.\"\"\"\n    return asdict(h)\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.history_from_dict","title":"<code>history_from_dict(d)</code>","text":"<p>Deserialize a dict to TiskHistory.</p> Source code in <code>pspcz_analyzer/data/history_scraper.py</code> <pre><code>def history_from_dict(d: dict) -&gt; TiskHistory:\n    \"\"\"Deserialize a dict to TiskHistory.\"\"\"\n    stages = [TiskHistoryStage(**s) for s in d.get(\"stages\", [])]\n    return TiskHistory(\n        ct=d[\"ct\"],\n        period=d[\"period\"],\n        submitter=d.get(\"submitter\", \"\"),\n        submitter_date=d.get(\"submitter_date\"),\n        government_opinion=d.get(\"government_opinion\"),\n        stages=stages,\n        current_status=d.get(\"current_status\", \"projedn\u00e1v\u00e1no\"),\n        law_number=d.get(\"law_number\"),\n        scraped_at=d.get(\"scraped_at\", \"\"),\n        law_changes=d.get(\"law_changes\", []),\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.save_history_json","title":"<code>save_history_json(h, path)</code>","text":"<p>Save a TiskHistory as JSON.</p> Source code in <code>pspcz_analyzer/data/history_scraper.py</code> <pre><code>def save_history_json(h: TiskHistory, path: Path) -&gt; None:\n    \"\"\"Save a TiskHistory as JSON.\"\"\"\n    path = Path(path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(json.dumps(history_to_dict(h), ensure_ascii=False, indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/history_scraper/#pspcz_analyzer.data.history_scraper.load_history_json","title":"<code>load_history_json(path)</code>","text":"<p>Load a TiskHistory from JSON file.</p> Source code in <code>pspcz_analyzer/data/history_scraper.py</code> <pre><code>def load_history_json(path: Path) -&gt; TiskHistory | None:\n    \"\"\"Load a TiskHistory from JSON file.\"\"\"\n    path = Path(path)\n    if not path.exists():\n        return None\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        return history_from_dict(data)\n    except Exception:\n        logger.opt(exception=True).warning(\"Failed to load history from {}\", path)\n        return None\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/","title":"law_changes_scraper","text":""},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper","title":"<code>law_changes_scraper</code>","text":"<p>Scraper for proposed law changes and related bills from psp.cz.</p> <p>Parses: - <code>historie.sqw?snzp=1</code> \u2014 which existing laws a bill proposes to modify - <code>tisky.sqw?idsb=...</code> \u2014 all other bills modifying the same laws</p>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.ProposedLawChange","title":"<code>ProposedLawChange(citace='', zmena='', predpis='', od_ct=None, idsb=None)</code>  <code>dataclass</code>","text":"<p>A single proposed change to an existing law.</p>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.RelatedBill","title":"<code>RelatedBill(cislo='', kratky_nazev='', typ_tisku='', stav='', period=None, ct=None, url='')</code>  <code>dataclass</code>","text":"<p>A bill found via the related-bills page (tisky.sqw?idsb=...).</p>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.scrape_proposed_law_changes","title":"<code>scrape_proposed_law_changes(period, ct)</code>","text":"<p>Parse the law changes table at <code>historie.sqw?snzp=1</code>.</p> <p>Returns list of ProposedLawChange or empty list on failure.</p> Source code in <code>pspcz_analyzer/data/law_changes_scraper.py</code> <pre><code>def scrape_proposed_law_changes(\n    period: int,\n    ct: int,\n) -&gt; list[ProposedLawChange]:\n    \"\"\"Parse the law changes table at ``historie.sqw?snzp=1``.\n\n    Returns list of ProposedLawChange or empty list on failure.\n    \"\"\"\n    url = PSP_LAW_CHANGES_URL_TEMPLATE.format(period=period, ct=ct)\n    logger.debug(\"Scraping law changes: {}\", url)\n\n    try:\n        with httpx.Client(timeout=30, follow_redirects=True) as client:\n            resp = client.get(url)\n            resp.raise_for_status()\n    except Exception:\n        logger.opt(exception=True).warning(\n            \"Failed to fetch law changes for tisk {}/{}\",\n            period,\n            ct,\n        )\n        return []\n\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    changes: list[ProposedLawChange] = []\n\n    for table in soup.find_all(\"table\"):\n        changes.extend(_parse_law_changes_table(table))\n\n    if not changes:\n        changes = _fallback_extract_law_changes(soup)\n\n    logger.debug(\"Tisk {}/{}: found {} law changes\", period, ct, len(changes))\n    return changes\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.scrape_related_bills","title":"<code>scrape_related_bills(idsb)</code>","text":"<p>Parse the related bills table at <code>tisky.sqw?idsb={id}</code>.</p> <p>Returns list of RelatedBill or empty list on failure.</p> Source code in <code>pspcz_analyzer/data/law_changes_scraper.py</code> <pre><code>def scrape_related_bills(idsb: int) -&gt; list[RelatedBill]:\n    \"\"\"Parse the related bills table at ``tisky.sqw?idsb={id}``.\n\n    Returns list of RelatedBill or empty list on failure.\n    \"\"\"\n    url = PSP_RELATED_BILLS_URL_TEMPLATE.format(idsb=idsb)\n    logger.debug(\"Scraping related bills: {}\", url)\n\n    try:\n        with httpx.Client(timeout=30, follow_redirects=True) as client:\n            resp = client.get(url)\n            resp.raise_for_status()\n    except Exception:\n        logger.opt(exception=True).warning(\n            \"Failed to fetch related bills for idsb={}\",\n            idsb,\n        )\n        return []\n\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    bills: list[RelatedBill] = []\n\n    for table in soup.find_all(\"table\"):\n        bills.extend(_parse_related_bills_table(table))\n\n    logger.debug(\"idsb={}: found {} related bills\", idsb, len(bills))\n    return bills\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.save_law_changes_json","title":"<code>save_law_changes_json(changes, period, ct, cache_dir)</code>","text":"<p>Save law changes to <code>tisky_meta/{period}/tisky_law_changes/{ct}.json</code>.</p> Source code in <code>pspcz_analyzer/data/law_changes_scraper.py</code> <pre><code>def save_law_changes_json(\n    changes: list[ProposedLawChange],\n    period: int,\n    ct: int,\n    cache_dir: Path,\n) -&gt; Path:\n    \"\"\"Save law changes to ``tisky_meta/{period}/tisky_law_changes/{ct}.json``.\"\"\"\n    dest_dir = cache_dir / TISKY_META_DIR / str(period) / TISKY_LAW_CHANGES_DIR\n    dest_dir.mkdir(parents=True, exist_ok=True)\n    dest = dest_dir / f\"{ct}.json\"\n    data = [asdict(c) for c in changes]\n    dest.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n    return dest\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.load_law_changes_json","title":"<code>load_law_changes_json(period, ct, cache_dir)</code>","text":"<p>Load cached law changes. Returns None if not cached.</p> Source code in <code>pspcz_analyzer/data/law_changes_scraper.py</code> <pre><code>def load_law_changes_json(\n    period: int,\n    ct: int,\n    cache_dir: Path,\n) -&gt; list[ProposedLawChange] | None:\n    \"\"\"Load cached law changes. Returns None if not cached.\"\"\"\n    path = cache_dir / TISKY_META_DIR / str(period) / TISKY_LAW_CHANGES_DIR / f\"{ct}.json\"\n    if not path.exists():\n        return None\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        return [ProposedLawChange(**d) for d in data]\n    except Exception:\n        logger.opt(exception=True).warning(\"Failed to load law changes from {}\", path)\n        return None\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.save_related_bills_json","title":"<code>save_related_bills_json(bills, idsb, cache_dir)</code>","text":"<p>Save related bills to <code>tisky_meta/related_bills/{idsb}.json</code>.</p> Source code in <code>pspcz_analyzer/data/law_changes_scraper.py</code> <pre><code>def save_related_bills_json(\n    bills: list[RelatedBill],\n    idsb: int,\n    cache_dir: Path,\n) -&gt; Path:\n    \"\"\"Save related bills to ``tisky_meta/related_bills/{idsb}.json``.\"\"\"\n    dest_dir = cache_dir / TISKY_META_DIR / TISKY_RELATED_BILLS_DIR\n    dest_dir.mkdir(parents=True, exist_ok=True)\n    dest = dest_dir / f\"{idsb}.json\"\n    data = [asdict(b) for b in bills]\n    dest.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n    return dest\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/law_changes_scraper/#pspcz_analyzer.data.law_changes_scraper.load_related_bills_json","title":"<code>load_related_bills_json(idsb, cache_dir)</code>","text":"<p>Load cached related bills. Returns None if not cached.</p> Source code in <code>pspcz_analyzer/data/law_changes_scraper.py</code> <pre><code>def load_related_bills_json(\n    idsb: int,\n    cache_dir: Path,\n) -&gt; list[RelatedBill] | None:\n    \"\"\"Load cached related bills. Returns None if not cached.\"\"\"\n    path = cache_dir / TISKY_META_DIR / TISKY_RELATED_BILLS_DIR / f\"{idsb}.json\"\n    if not path.exists():\n        return None\n    try:\n        data = json.loads(path.read_text(encoding=\"utf-8\"))\n        return [RelatedBill(**d) for d in data]\n    except Exception:\n        logger.opt(exception=True).warning(\"Failed to load related bills from {}\", path)\n        return None\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/parser/","title":"parser","text":""},{"location":"reference/pspcz_analyzer/data/parser/#pspcz_analyzer.data.parser","title":"<code>parser</code>","text":"<p>Parse UNL (pipe-delimited) files into Polars DataFrames.</p>"},{"location":"reference/pspcz_analyzer/data/parser/#pspcz_analyzer.data.parser.parse_unl","title":"<code>parse_unl(file_path, columns, dtypes=None)</code>","text":"<p>Parse a single UNL file into a Polars DataFrame.</p> <p>UNL files are pipe-delimited, Windows-1250 encoded, with no header row and a trailing pipe on each line (producing an extra empty column).</p> Source code in <code>pspcz_analyzer/data/parser.py</code> <pre><code>def parse_unl(\n    file_path: Path,\n    columns: list[str],\n    dtypes: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Parse a single UNL file into a Polars DataFrame.\n\n    UNL files are pipe-delimited, Windows-1250 encoded, with no header row\n    and a trailing pipe on each line (producing an extra empty column).\n    \"\"\"\n    raw_bytes = file_path.read_bytes()\n    if not raw_bytes.strip():\n        logger.info(\"Skipping empty file {}\", file_path.name)\n        return pl.DataFrame({c: pl.Series([], dtype=pl.Utf8) for c in columns})\n\n    text = raw_bytes.decode(UNL_ENCODING)\n    utf8_bytes = text.encode(\"utf-8\")\n\n    # UNL has trailing pipe -&gt; extra column\n    all_columns = columns + [\"_trailing\"]\n\n    csv_kwargs: dict = dict(\n        separator=UNL_SEPARATOR,\n        has_header=False,\n        new_columns=all_columns,\n        infer_schema_length=0,\n        truncate_ragged_lines=True,\n        encoding=\"utf8\",\n        # UNL files never use CSV-style quoting \u2014 any double quotes in the\n        # data are literal characters.  Always disable to avoid misparses\n        # (e.g. period 2 / 1996 voting data contains stray quotes).\n        quote_char=None,\n    )\n\n    df = pl.read_csv(utf8_bytes, **csv_kwargs)\n\n    df = df.drop(\"_trailing\")\n\n    # Cast typed columns\n    if dtypes:\n        cast_exprs = []\n        for col_name, dtype in dtypes.items():\n            if col_name in df.columns:\n                cast_exprs.append(pl.col(col_name).str.strip_chars().cast(dtype, strict=False))\n        if cast_exprs:\n            df = df.with_columns(cast_exprs)\n\n    logger.info(\"Parsed {}: {} rows x {} cols\", file_path.name, df.height, df.width)\n    return df\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/parser/#pspcz_analyzer.data.parser.parse_unl_multi","title":"<code>parse_unl_multi(directory, glob_pattern, columns, dtypes=None)</code>","text":"<p>Parse multiple UNL files matching a glob pattern and concatenate them.</p> Source code in <code>pspcz_analyzer/data/parser.py</code> <pre><code>def parse_unl_multi(\n    directory: Path,\n    glob_pattern: str,\n    columns: list[str],\n    dtypes: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Parse multiple UNL files matching a glob pattern and concatenate them.\"\"\"\n    files = sorted(directory.rglob(glob_pattern))\n    if not files:\n        msg = f\"No files matching {glob_pattern} in {directory}\"\n        raise FileNotFoundError(msg)\n\n    dfs = [parse_unl(f, columns, dtypes) for f in files]\n    dfs = [df for df in dfs if df.height &gt; 0]\n    if not dfs:\n        return pl.DataFrame({c: pl.Series([], dtype=pl.Utf8) for c in columns})\n    result = pl.concat(dfs)\n    logger.info(\n        \"Parsed {} files ({}): {} total rows\",\n        len(files),\n        glob_pattern,\n        result.height,\n    )\n    return result\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_downloader/","title":"tisk_downloader","text":""},{"location":"reference/pspcz_analyzer/data/tisk_downloader/#pspcz_analyzer.data.tisk_downloader","title":"<code>tisk_downloader</code>","text":"<p>Download tisk PDFs from psp.cz.</p>"},{"location":"reference/pspcz_analyzer/data/tisk_downloader/#pspcz_analyzer.data.tisk_downloader.download_tisk_pdf","title":"<code>download_tisk_pdf(period, ct, cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download a single tisk PDF. Returns the cached path or None if unavailable.</p> Source code in <code>pspcz_analyzer/data/tisk_downloader.py</code> <pre><code>def download_tisk_pdf(\n    period: int,\n    ct: int,\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path | None:\n    \"\"\"Download a single tisk PDF. Returns the cached path or None if unavailable.\"\"\"\n    pdf_dir = cache_dir / TISKY_PDF_DIR / str(period)\n    pdf_dir.mkdir(parents=True, exist_ok=True)\n    dest = pdf_dir / f\"{ct}.pdf\"\n\n    if dest.exists() and not force:\n        logger.debug(\"Cached PDF: {}\", dest)\n        return dest\n\n    doc = get_best_pdf(period, ct)\n    if doc is None:\n        logger.warning(\"No PDF found for tisk {}/{}\", period, ct)\n        return None\n\n    url = f\"{PSP_ORIG2_BASE_URL}?idd={doc.idd}\"\n    logger.info(\"Downloading PDF tisk {}/{} (idd={}) ...\", period, ct, doc.idd)\n\n    try:\n        with httpx.Client(timeout=60, follow_redirects=True) as client:\n            with client.stream(\"GET\", url) as response:\n                response.raise_for_status()\n                with open(dest, \"wb\") as f:\n                    for chunk in response.iter_bytes(chunk_size=65536):\n                        f.write(chunk)\n    except httpx.HTTPError:\n        logger.exception(\"Failed to download tisk {}/{}\", period, ct)\n        dest.unlink(missing_ok=True)\n        return None\n\n    logger.info(\"Downloaded {} ({:.1f} KB)\", dest.name, dest.stat().st_size / 1e3)\n    return dest\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_downloader/#pspcz_analyzer.data.tisk_downloader.download_subtisk_pdf","title":"<code>download_subtisk_pdf(period, ct, ct1, idd, cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download a sub-tisk PDF by idd. File naming: <code>{ct}_{ct1}.pdf</code>.</p> Source code in <code>pspcz_analyzer/data/tisk_downloader.py</code> <pre><code>def download_subtisk_pdf(\n    period: int,\n    ct: int,\n    ct1: int,\n    idd: int,\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path | None:\n    \"\"\"Download a sub-tisk PDF by idd. File naming: ``{ct}_{ct1}.pdf``.\"\"\"\n    pdf_dir = cache_dir / TISKY_PDF_DIR / str(period)\n    pdf_dir.mkdir(parents=True, exist_ok=True)\n    dest = pdf_dir / f\"{ct}_{ct1}.pdf\"\n\n    if dest.exists() and not force:\n        logger.debug(\"Cached sub-tisk PDF: {}\", dest)\n        return dest\n\n    url = f\"{PSP_ORIG2_BASE_URL}?idd={idd}\"\n    logger.info(\"Downloading sub-tisk PDF {}/{}/{} (idd={}) ...\", period, ct, ct1, idd)\n\n    try:\n        with httpx.Client(timeout=60, follow_redirects=True) as client:\n            with client.stream(\"GET\", url) as response:\n                response.raise_for_status()\n                with open(dest, \"wb\") as f:\n                    for chunk in response.iter_bytes(chunk_size=65536):\n                        f.write(chunk)\n    except httpx.HTTPError:\n        logger.exception(\"Failed to download sub-tisk {}/{}/{}\", period, ct, ct1)\n        dest.unlink(missing_ok=True)\n        return None\n\n    logger.info(\"Downloaded {} ({:.1f} KB)\", dest.name, dest.stat().st_size / 1e3)\n    return dest\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_downloader/#pspcz_analyzer.data.tisk_downloader.download_period_tisky","title":"<code>download_period_tisky(period, tisk_numbers, cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Download PDFs for multiple tisky with rate limiting.</p> <p>Returns a mapping of ct -&gt; downloaded PDF path (skips failures).</p> Source code in <code>pspcz_analyzer/data/tisk_downloader.py</code> <pre><code>def download_period_tisky(\n    period: int,\n    tisk_numbers: list[int],\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; dict[int, Path]:\n    \"\"\"Download PDFs for multiple tisky with rate limiting.\n\n    Returns a mapping of ct -&gt; downloaded PDF path (skips failures).\n    \"\"\"\n    results: dict[int, Path] = {}\n    total = len(tisk_numbers)\n\n    for i, ct in enumerate(tisk_numbers, 1):\n        logger.info(\"[{}/{}] Tisk {}\", i, total, ct)\n        path = download_tisk_pdf(period, ct, cache_dir, force)\n        if path is not None:\n            results[ct] = path\n\n        # Rate limit \u2014 be polite to psp.cz\n        if i &lt; total:\n            time.sleep(PSP_REQUEST_DELAY)\n\n    logger.info(\"Downloaded {}/{} tisk PDFs for period {}\", len(results), total, period)\n    return results\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_extractor/","title":"tisk_extractor","text":""},{"location":"reference/pspcz_analyzer/data/tisk_extractor/#pspcz_analyzer.data.tisk_extractor","title":"<code>tisk_extractor</code>","text":"<p>Extract text from tisk PDFs using PyMuPDF.</p>"},{"location":"reference/pspcz_analyzer/data/tisk_extractor/#pspcz_analyzer.data.tisk_extractor.extract_text_from_pdf","title":"<code>extract_text_from_pdf(pdf_path)</code>","text":"<p>Extract all text from a PDF file using PyMuPDF.</p> Source code in <code>pspcz_analyzer/data/tisk_extractor.py</code> <pre><code>def extract_text_from_pdf(pdf_path: Path) -&gt; str:\n    \"\"\"Extract all text from a PDF file using PyMuPDF.\"\"\"\n    try:\n        doc = pymupdf.open(pdf_path)\n        pages = [str(page.get_text()) for page in doc]\n        doc.close()\n        return \"\\n\\n\".join(pages)\n    except Exception:\n        logger.exception(\"Failed to extract text from {}\", pdf_path)\n        return \"\"\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_extractor/#pspcz_analyzer.data.tisk_extractor.extract_and_cache","title":"<code>extract_and_cache(pdf_path, period, ct, cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Extract text from a PDF and cache it as a .txt file.</p> Source code in <code>pspcz_analyzer/data/tisk_extractor.py</code> <pre><code>def extract_and_cache(\n    pdf_path: Path,\n    period: int,\n    ct: int,\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; Path | None:\n    \"\"\"Extract text from a PDF and cache it as a .txt file.\"\"\"\n    text_dir = cache_dir / TISKY_TEXT_DIR / str(period)\n    text_dir.mkdir(parents=True, exist_ok=True)\n    dest = text_dir / f\"{ct}.txt\"\n\n    if dest.exists() and not force:\n        logger.debug(\"Cached text: {}\", dest)\n        return dest\n\n    text = extract_text_from_pdf(pdf_path)\n    if not text.strip():\n        logger.warning(\"Empty text extracted from {} (scanned PDF?)\", pdf_path.name)\n        return None\n\n    dest.write_text(text, encoding=\"utf-8\")\n    logger.info(\"Extracted text for tisk {} ({:.1f} KB)\", ct, dest.stat().st_size / 1e3)\n    return dest\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_extractor/#pspcz_analyzer.data.tisk_extractor.extract_period_texts","title":"<code>extract_period_texts(period, pdf_paths, cache_dir=DEFAULT_CACHE_DIR, force=False)</code>","text":"<p>Extract text from all PDFs for a period.</p> <p>Returns mapping of ct -&gt; text file path (skips failures).</p> Source code in <code>pspcz_analyzer/data/tisk_extractor.py</code> <pre><code>def extract_period_texts(\n    period: int,\n    pdf_paths: dict[int, Path],\n    cache_dir: Path = DEFAULT_CACHE_DIR,\n    force: bool = False,\n) -&gt; dict[int, Path]:\n    \"\"\"Extract text from all PDFs for a period.\n\n    Returns mapping of ct -&gt; text file path (skips failures).\n    \"\"\"\n    results: dict[int, Path] = {}\n    total = len(pdf_paths)\n\n    for i, (ct, pdf_path) in enumerate(sorted(pdf_paths.items()), 1):\n        logger.info(\"[{}/{}] Extracting text from tisk {}\", i, total, ct)\n        path = extract_and_cache(pdf_path, period, ct, cache_dir, force)\n        if path is not None:\n            results[ct] = path\n\n    logger.info(\"Extracted text for {}/{} tisky in period {}\", len(results), total, period)\n    return results\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_scraper/","title":"tisk_scraper","text":""},{"location":"reference/pspcz_analyzer/data/tisk_scraper/#pspcz_analyzer.data.tisk_scraper","title":"<code>tisk_scraper</code>","text":"<p>Scrape psp.cz for PDF document links associated with parliamentary prints (tisky).</p>"},{"location":"reference/pspcz_analyzer/data/tisk_scraper/#pspcz_analyzer.data.tisk_scraper.TiskDocument","title":"<code>TiskDocument(idd, description, format, is_complete)</code>  <code>dataclass</code>","text":"<p>A single document (PDF) associated with a parliamentary print.</p>"},{"location":"reference/pspcz_analyzer/data/tisk_scraper/#pspcz_analyzer.data.tisk_scraper.SubTiskVersion","title":"<code>SubTiskVersion(period, ct, ct1, idd=None, description='', has_pdf=False, has_text=False, llm_diff_summary='')</code>  <code>dataclass</code>","text":"<p>A sub-tisk version (CT1=0 original, CT1=1 gov opinion, CT1=2+ amendments).</p>"},{"location":"reference/pspcz_analyzer/data/tisk_scraper/#pspcz_analyzer.data.tisk_scraper.scrape_tisk_documents","title":"<code>scrape_tisk_documents(period, ct)</code>","text":"<p>Scrape the document listing page for a given tisk and return all PDF links.</p> <p>Fetches <code>tiskt.sqw?o={period}&amp;ct={ct}&amp;ct1=0</code> and extracts <code>orig2.sqw?idd=</code> links along with their descriptions.</p> Source code in <code>pspcz_analyzer/data/tisk_scraper.py</code> <pre><code>def scrape_tisk_documents(period: int, ct: int) -&gt; list[TiskDocument]:\n    \"\"\"Scrape the document listing page for a given tisk and return all PDF links.\n\n    Fetches ``tiskt.sqw?o={period}&amp;ct={ct}&amp;ct1=0`` and extracts ``orig2.sqw?idd=``\n    links along with their descriptions.\n    \"\"\"\n    url = PSP_TISKT_URL_TEMPLATE.format(period=period, ct=ct)\n    logger.debug(\"Scraping tisk documents: {}\", url)\n\n    with httpx.Client(timeout=30, follow_redirects=True) as client:\n        resp = client.get(url)\n        resp.raise_for_status()\n\n    soup = BeautifulSoup(resp.text, \"html.parser\")\n    documents: list[TiskDocument] = []\n\n    for link in soup.find_all(\"a\", href=_IDD_RE):\n        href = str(link[\"href\"])\n        match = _IDD_RE.search(href)\n        if not match:\n            continue\n\n        idd = int(match.group(1))\n        desc = link.get_text(strip=True)\n        parent_text = link.parent.get_text(strip=True) if link.parent else desc\n\n        # Detect format from context \u2014 psp.cz labels PDFs\n        fmt = \"PDF\" if \"PDF\" in parent_text.upper() or href.endswith(\".pdf\") else \"unknown\"\n        is_complete = \"cel\" in desc.lower() or \"\u00fapln\u00e9 zn\u011bn\u00ed\" in desc.lower()\n\n        documents.append(\n            TiskDocument(\n                idd=idd,\n                description=desc,\n                format=fmt,\n                is_complete=is_complete,\n            )\n        )\n\n    logger.debug(\"Tisk {}/{}: found {} documents\", period, ct, len(documents))\n    return documents\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_scraper/#pspcz_analyzer.data.tisk_scraper.get_best_pdf","title":"<code>get_best_pdf(period, ct)</code>","text":"<p>Return the best PDF document for a tisk \u2014 prefer complete prints.</p> Source code in <code>pspcz_analyzer/data/tisk_scraper.py</code> <pre><code>def get_best_pdf(period: int, ct: int) -&gt; TiskDocument | None:\n    \"\"\"Return the best PDF document for a tisk \u2014 prefer complete prints.\"\"\"\n    docs = scrape_tisk_documents(period, ct)\n    if not docs:\n        return None\n\n    # Prefer complete prints, then first available\n    complete = [d for d in docs if d.is_complete]\n    return complete[0] if complete else docs[0]\n</code></pre>"},{"location":"reference/pspcz_analyzer/data/tisk_scraper/#pspcz_analyzer.data.tisk_scraper.scrape_all_subtisk_documents","title":"<code>scrape_all_subtisk_documents(period, ct, max_ct1=20)</code>","text":"<p>Iterate CT1=0..N for a tisk, collecting sub-tisk versions.</p> <p>Stops when a page returns 404/empty. Returns list of SubTiskVersion.</p> Source code in <code>pspcz_analyzer/data/tisk_scraper.py</code> <pre><code>def scrape_all_subtisk_documents(\n    period: int,\n    ct: int,\n    max_ct1: int = 20,\n) -&gt; list[SubTiskVersion]:\n    \"\"\"Iterate CT1=0..N for a tisk, collecting sub-tisk versions.\n\n    Stops when a page returns 404/empty. Returns list of SubTiskVersion.\n    \"\"\"\n    versions: list[SubTiskVersion] = []\n\n    for ct1 in range(max_ct1 + 1):\n        docs = _scrape_subtisk_page(period, ct, ct1)\n        if docs is None and ct1 &gt; 0:\n            # CT1=0 might legitimately have no PDFs, but once we hit empty\n            # for ct1 &gt; 0, we're past the last version\n            break\n\n        # Build a description from the page's title/context\n        best_doc = None\n        desc = \"\"\n        if docs:\n            # Prefer complete docs\n            complete = [d for d in docs if d.is_complete]\n            best_doc = complete[0] if complete else docs[0]\n            desc = best_doc.description\n\n        match ct1:\n            case 0:\n                desc = desc or \"P\u016fvodn\u00ed zn\u011bn\u00ed (original)\"\n            case 1:\n                desc = desc or \"Stanovisko vl\u00e1dy (government opinion)\"\n\n        version = SubTiskVersion(\n            period=period,\n            ct=ct,\n            ct1=ct1,\n            idd=best_doc.idd if best_doc else None,\n            description=desc,\n            has_pdf=best_doc is not None,\n        )\n        versions.append(version)\n\n        # If we got no docs for CT1=0, don't bother continuing\n        if docs is None and ct1 == 0:\n            break\n\n    logger.debug(\"Tisk {}/{}: found {} sub-tisk versions\", period, ct, len(versions))\n    return versions\n</code></pre>"},{"location":"reference/pspcz_analyzer/i18n/","title":"i18n","text":""},{"location":"reference/pspcz_analyzer/i18n/#pspcz_analyzer.i18n","title":"<code>i18n</code>","text":"<p>Internationalization support for Czech/English UI localization.</p>"},{"location":"reference/pspcz_analyzer/i18n/#pspcz_analyzer.i18n.get_locale","title":"<code>get_locale()</code>","text":"<p>Return the current request's locale from the ContextVar.</p> Source code in <code>pspcz_analyzer/i18n/__init__.py</code> <pre><code>def get_locale() -&gt; str:\n    \"\"\"Return the current request's locale from the ContextVar.\"\"\"\n    return _locale_var.get()\n</code></pre>"},{"location":"reference/pspcz_analyzer/i18n/#pspcz_analyzer.i18n.set_locale","title":"<code>set_locale(lang)</code>","text":"<p>Set the current request's locale in the ContextVar.</p> Source code in <code>pspcz_analyzer/i18n/__init__.py</code> <pre><code>def set_locale(lang: str) -&gt; None:\n    \"\"\"Set the current request's locale in the ContextVar.\"\"\"\n    _locale_var.set(lang)\n</code></pre>"},{"location":"reference/pspcz_analyzer/i18n/#pspcz_analyzer.i18n.gettext","title":"<code>gettext(key)</code>","text":"<p>Look up a translated string for the current locale.</p> <p>Falls back to the key itself if no translation is found.</p> Source code in <code>pspcz_analyzer/i18n/__init__.py</code> <pre><code>def gettext(key: str) -&gt; str:\n    \"\"\"Look up a translated string for the current locale.\n\n    Falls back to the key itself if no translation is found.\n    \"\"\"\n    locale = get_locale()\n    return TRANSLATIONS.get(locale, {}).get(key, key)\n</code></pre>"},{"location":"reference/pspcz_analyzer/i18n/#pspcz_analyzer.i18n.ngettext","title":"<code>ngettext(singular, plural, n)</code>","text":"<p>Simple plural-aware translation lookup.</p> Source code in <code>pspcz_analyzer/i18n/__init__.py</code> <pre><code>def ngettext(singular: str, plural: str, n: int) -&gt; str:\n    \"\"\"Simple plural-aware translation lookup.\"\"\"\n    key = singular if n == 1 else plural\n    return gettext(key)\n</code></pre>"},{"location":"reference/pspcz_analyzer/i18n/#pspcz_analyzer.i18n.setup_jinja2_i18n","title":"<code>setup_jinja2_i18n(env)</code>","text":"<p>Install gettext callables on a Jinja2 environment.</p> Source code in <code>pspcz_analyzer/i18n/__init__.py</code> <pre><code>def setup_jinja2_i18n(env: Environment) -&gt; None:\n    \"\"\"Install gettext callables on a Jinja2 environment.\"\"\"\n    env.add_extension(\"jinja2.ext.i18n\")\n    env.install_gettext_callables(gettext, ngettext, newstyle=False)  # type: ignore[attr-defined]\n</code></pre>"},{"location":"reference/pspcz_analyzer/i18n/middleware/","title":"middleware","text":""},{"location":"reference/pspcz_analyzer/i18n/middleware/#pspcz_analyzer.i18n.middleware","title":"<code>middleware</code>","text":"<p>Locale middleware \u2014 reads language from cookie and sets ContextVar per request.</p>"},{"location":"reference/pspcz_analyzer/i18n/middleware/#pspcz_analyzer.i18n.middleware.LocaleMiddleware","title":"<code>LocaleMiddleware</code>","text":"<p>               Bases: <code>BaseHTTPMiddleware</code></p> <p>Set locale ContextVar from the <code>lang</code> cookie on every request.</p>"},{"location":"reference/pspcz_analyzer/i18n/translations/","title":"translations","text":""},{"location":"reference/pspcz_analyzer/i18n/translations/#pspcz_analyzer.i18n.translations","title":"<code>translations</code>","text":"<p>Czech and English translation dictionaries for all UI strings.</p>"},{"location":"reference/pspcz_analyzer/models/","title":"models","text":""},{"location":"reference/pspcz_analyzer/models/#pspcz_analyzer.models","title":"<code>models</code>","text":""},{"location":"reference/pspcz_analyzer/models/enums/","title":"enums","text":""},{"location":"reference/pspcz_analyzer/models/enums/#pspcz_analyzer.models.enums","title":"<code>enums</code>","text":"<p>Domain enumerations for Czech Parliament voting data.</p>"},{"location":"reference/pspcz_analyzer/models/enums/#pspcz_analyzer.models.enums.VoteResult","title":"<code>VoteResult</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Individual MP vote codes (hl_poslanec.vysledek).</p>"},{"location":"reference/pspcz_analyzer/models/enums/#pspcz_analyzer.models.enums.VoteOutcome","title":"<code>VoteOutcome</code>","text":"<p>               Bases: <code>StrEnum</code></p> <p>Aggregate vote outcome (hl_hlasovani.vysledek).</p>"},{"location":"reference/pspcz_analyzer/models/schemas/","title":"schemas","text":""},{"location":"reference/pspcz_analyzer/models/schemas/#pspcz_analyzer.models.schemas","title":"<code>schemas</code>","text":"<p>UNL table column definitions and Polars dtype schemas.</p> <p>Column names match psp.cz documentation exactly (Czech names) for traceability. UNL files have no headers \u2014 these lists define the column order.</p>"},{"location":"reference/pspcz_analyzer/models/tisk_models/","title":"tisk_models","text":""},{"location":"reference/pspcz_analyzer/models/tisk_models/#pspcz_analyzer.models.tisk_models","title":"<code>tisk_models</code>","text":"<p>Data models for parliamentary prints (tisky) and period data.</p>"},{"location":"reference/pspcz_analyzer/models/tisk_models/#pspcz_analyzer.models.tisk_models.TiskInfo","title":"<code>TiskInfo(id_tisk, ct, nazev, period, topics=list(), has_text=False, summary='', summary_en='', history=None, law_changes=list(), sub_versions=list())</code>  <code>dataclass</code>","text":"<p>Metadata for a parliamentary print (tisk) linked to a vote.</p> <p>Attributes:</p> Name Type Description <code>id_tisk</code> <code>int</code> <p>Database ID of the tisk.</p> <code>ct</code> <code>int</code> <p>Tisk number (cislo tisku).</p> <code>nazev</code> <code>str</code> <p>Title of the tisk.</p> <code>period</code> <code>int</code> <p>Electoral period number.</p> <code>topics</code> <code>list[str]</code> <p>LLM-classified topic labels (1-3 Czech labels).</p> <code>has_text</code> <code>bool</code> <p>Whether extracted full text is available.</p> <code>summary</code> <code>str</code> <p>LLM-generated Czech summary of the tisk.</p> <code>history</code> <code>TiskHistory | None</code> <p>Scraped legislative history stages.</p> <code>law_changes</code> <code>list[dict]</code> <p>List of affected existing laws.</p> <code>sub_versions</code> <code>list[dict]</code> <p>Sub-tisk version dicts (amendments, gov opinions).</p>"},{"location":"reference/pspcz_analyzer/models/tisk_models/#pspcz_analyzer.models.tisk_models.PeriodData","title":"<code>PeriodData(period, votes, mp_votes, void_votes, mp_info, tisk_lookup=dict())</code>  <code>dataclass</code>","text":"<p>All DataFrames and lookups for a single electoral period.</p> <p>Attributes:</p> Name Type Description <code>period</code> <code>int</code> <p>Electoral period number.</p> <code>votes</code> <code>DataFrame</code> <p>Polars DataFrame of vote records (hl_hlasovani).</p> <code>mp_votes</code> <code>DataFrame</code> <p>Polars DataFrame of per-MP vote results (hl_poslanec).</p> <code>void_votes</code> <code>DataFrame</code> <p>Polars DataFrame of void vote IDs (zmatecne).</p> <code>mp_info</code> <code>DataFrame</code> <p>Polars DataFrame of MP info (name, party, organ).</p> <code>tisk_lookup</code> <code>dict[tuple[int, int], TiskInfo]</code> <p>Mapping of (session, agenda_item) to TiskInfo.</p>"},{"location":"reference/pspcz_analyzer/models/tisk_models/#pspcz_analyzer.models.tisk_models.PeriodData.get_tisk","title":"<code>get_tisk(schuze, bod)</code>","text":"<p>Get tisk info for a vote given its session and agenda item numbers.</p> Source code in <code>pspcz_analyzer/models/tisk_models.py</code> <pre><code>def get_tisk(self, schuze: int, bod: int) -&gt; TiskInfo | None:\n    \"\"\"Get tisk info for a vote given its session and agenda item numbers.\"\"\"\n    return self.tisk_lookup.get((schuze, bod))\n</code></pre>"},{"location":"reference/pspcz_analyzer/models/tisk_models/#pspcz_analyzer.models.tisk_models.PeriodData.get_all_topic_labels","title":"<code>get_all_topic_labels()</code>","text":"<p>Collect all unique topic labels across all tisky, sorted.</p> Source code in <code>pspcz_analyzer/models/tisk_models.py</code> <pre><code>def get_all_topic_labels(self) -&gt; list[str]:\n    \"\"\"Collect all unique topic labels across all tisky, sorted.\"\"\"\n    labels: set[str] = set()\n    for tisk in self.tisk_lookup.values():\n        labels.update(tisk.topics)\n    return sorted(labels)\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/","title":"routes","text":""},{"location":"reference/pspcz_analyzer/routes/#pspcz_analyzer.routes","title":"<code>routes</code>","text":""},{"location":"reference/pspcz_analyzer/routes/api/","title":"api","text":""},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api","title":"<code>api</code>","text":"<p>HTMX partial endpoints \u2014 return HTML fragments.</p>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.tisk_text_api","title":"<code>tisk_text_api(request, period=DEFAULT_PERIOD, ct=Query(default=0, ge=0, le=999999), ct1=Query(default=(-1), ge=(-1), le=999999))</code>  <code>async</code>","text":"<p>Return extracted tisk text as an HTML fragment for HTMX loading.</p> <p>When ct1 &gt;= 0, loads sub-tisk text ({ct}_{ct1}.txt) instead of main text.</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.get(\"/tisk-text\", response_class=HTMLResponse)\n@limiter.limit(\"15/minute\")\nasync def tisk_text_api(\n    request: Request,\n    period: int = DEFAULT_PERIOD,\n    ct: int = Query(default=0, ge=0, le=999999),\n    ct1: int = Query(default=-1, ge=-1, le=999999),\n):\n    \"\"\"Return extracted tisk text as an HTML fragment for HTMX loading.\n\n    When ct1 &gt;= 0, loads sub-tisk text ({ct}_{ct1}.txt) instead of main text.\n    \"\"\"\n    validate_period(period)\n    data_svc = request.app.state.data\n    if ct1 &gt;= 0:\n        text_path = data_svc.cache_dir / TISKY_TEXT_DIR / str(period) / f\"{ct}_{ct1}.txt\"\n        text = text_path.read_text(encoding=\"utf-8\") if text_path.exists() else None\n    else:\n        text = data_svc.tisk_text.get_text(period, ct)\n    if text is None:\n        return HTMLResponse(\n            '&lt;article style=\"background: #fff3cd; padding: 1rem;\"&gt;'\n            f\"&lt;p&gt;{html_mod.escape(_t('tisk.no_text'))}&lt;/p&gt;\"\n            \"&lt;/article&gt;\"\n        )\n    escaped = html_mod.escape(text)\n    return HTMLResponse(\n        '&lt;article style=\"max-height: 60vh; overflow-y: auto; background: #f8f9fa; '\n        'padding: 1rem; border: 1px solid #dee2e6; border-radius: 0.5rem;\"&gt;'\n        f'&lt;pre style=\"white-space: pre-wrap; word-wrap: break-word; font-size: 0.85rem;\"&gt;{escaped}&lt;/pre&gt;'\n        \"&lt;/article&gt;\"\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.tisk_evolution_api","title":"<code>tisk_evolution_api(request, period=DEFAULT_PERIOD, ct=Query(default=0, ge=0, le=999999))</code>  <code>async</code>","text":"<p>Return the legislative evolution partial (law changes + sub-tisk versions).</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.get(\"/tisk-evolution\", response_class=HTMLResponse)\n@limiter.limit(\"15/minute\")\nasync def tisk_evolution_api(\n    request: Request,\n    period: int = DEFAULT_PERIOD,\n    ct: int = Query(default=0, ge=0, le=999999),\n):\n    \"\"\"Return the legislative evolution partial (law changes + sub-tisk versions).\"\"\"\n    validate_period(period)\n    data_svc = request.app.state.data\n    pd = data_svc.get_period(period)\n    tisk = None\n    if pd:\n        for t in pd.tisk_lookup.values():\n            if t.ct == ct:\n                tisk = t\n                break\n\n    law_changes = tisk.law_changes if tisk else []\n    sub_versions = tisk.sub_versions if tisk else []\n\n    return templates.TemplateResponse(\n        \"partials/tisk_evolution.html\",\n        {\n            \"request\": request,\n            \"period\": period,\n            \"ct\": ct,\n            \"law_changes\": law_changes,\n            \"sub_versions\": sub_versions,\n            \"lang\": getattr(request.state, \"lang\", \"cs\"),\n            \"feedback_enabled\": GITHUB_FEEDBACK_ENABLED,\n        },\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.related_bills_api","title":"<code>related_bills_api(request, idsb=Query(default=0, ge=0, le=999999))</code>  <code>async</code>","text":"<p>Lazy-load related bills for a specific law (scrapes on demand, caches).</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.get(\"/related-bills\", response_class=HTMLResponse)\n@limiter.limit(\"5/minute\")\nasync def related_bills_api(\n    request: Request,\n    idsb: int = Query(default=0, ge=0, le=999999),\n):\n    \"\"\"Lazy-load related bills for a specific law (scrapes on demand, caches).\"\"\"\n    if idsb &lt;= 0:\n        return HTMLResponse(f\"&lt;p&gt;{html_mod.escape(_t('related.invalid'))}&lt;/p&gt;\")\n\n    cache_dir = DEFAULT_CACHE_DIR\n    cached = load_related_bills_json(idsb, cache_dir)\n    if cached is not None:\n        bills = [asdict(b) for b in cached]\n    else:\n        raw_bills = await run_with_timeout(\n            scrape_related_bills,\n            idsb,\n            timeout=15.0,\n            label=\"related bills scrape\",\n        )\n        save_related_bills_json(raw_bills, idsb, cache_dir)\n        bills = [asdict(b) for b in raw_bills]\n\n    if not bills:\n        return HTMLResponse(\n            '&lt;p style=\"color: #6c757d; font-size: 0.85rem;\"&gt;'\n            f\"{html_mod.escape(_t('related.no_bills'))}&lt;/p&gt;\"\n        )\n\n    rows_html = \"\"\n    for b in bills:\n        url = b.get(\"url\", \"\")\n        cislo = b.get(\"cislo\", \"?\")\n        link = f'&lt;a href=\"{url}\" target=\"_blank\"&gt;{cislo}&lt;/a&gt;' if url else cislo\n        rows_html += (\n            f\"&lt;tr&gt;\"\n            f\"&lt;td&gt;{link}&lt;/td&gt;\"\n            f\"&lt;td&gt;{b.get('kratky_nazev', '')}&lt;/td&gt;\"\n            f\"&lt;td&gt;{b.get('typ_tisku', '')}&lt;/td&gt;\"\n            f\"&lt;td&gt;{b.get('stav', '')}&lt;/td&gt;\"\n            f\"&lt;/tr&gt;\"\n        )\n\n    return HTMLResponse(\n        '&lt;table style=\"font-size: 0.85rem; margin: 0.5rem 0;\"&gt;'\n        \"&lt;thead&gt;&lt;tr&gt;\"\n        f\"&lt;th&gt;{html_mod.escape(_t('related.th.tisk'))}&lt;/th&gt;\"\n        f\"&lt;th&gt;{html_mod.escape(_t('related.th.title'))}&lt;/th&gt;\"\n        f\"&lt;th&gt;{html_mod.escape(_t('related.th.type'))}&lt;/th&gt;\"\n        f\"&lt;th&gt;{html_mod.escape(_t('related.th.status'))}&lt;/th&gt;\"\n        \"&lt;/tr&gt;&lt;/thead&gt;\"\n        f\"&lt;tbody&gt;{rows_html}&lt;/tbody&gt;\"\n        \"&lt;/table&gt;\"\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.feedback_api","title":"<code>feedback_api(request, vote_id=Form(default=0), period=Form(default=0), title=Form(default=''), body=Form(default=''))</code>  <code>async</code>","text":"<p>Submit user feedback as a GitHub issue.</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.post(\"/feedback\", response_class=HTMLResponse)\n@limiter.limit(\"3/hour\")\nasync def feedback_api(\n    request: Request,\n    vote_id: int = Form(default=0),\n    period: int = Form(default=0),\n    title: str = Form(default=\"\"),\n    body: str = Form(default=\"\"),\n):\n    \"\"\"Submit user feedback as a GitHub issue.\"\"\"\n    lang = getattr(request.state, \"lang\", \"cs\")\n    suffix = \"\"\n\n    if not GITHUB_FEEDBACK_ENABLED:\n        return templates.TemplateResponse(\n            \"partials/feedback_result.html\",\n            {\n                \"request\": request,\n                \"success\": False,\n                \"error_message\": _t(\"feedback.disabled\"),\n                \"feedback_id_suffix\": suffix,\n                \"lang\": lang,\n            },\n        )\n\n    validation_error = _validate_feedback_fields(title, body)\n    if validation_error:\n        return templates.TemplateResponse(\n            \"partials/feedback_result.html\",\n            {\n                \"request\": request,\n                \"success\": False,\n                \"error_message\": validation_error,\n                \"feedback_id_suffix\": suffix,\n                \"lang\": lang,\n            },\n        )\n\n    page_url = str(request.headers.get(\"referer\", f\"/votes/{vote_id}?period={period}\"))\n    client = GitHubFeedbackClient()\n    result = await asyncio.to_thread(\n        client.create_issue, title, body, vote_id, period, page_url, lang\n    )\n\n    if result:\n        return templates.TemplateResponse(\n            \"partials/feedback_result.html\",\n            {\n                \"request\": request,\n                \"success\": True,\n                \"issue_url\": result[\"html_url\"],\n                \"feedback_id_suffix\": suffix,\n                \"lang\": lang,\n            },\n        )\n    return templates.TemplateResponse(\n        \"partials/feedback_result.html\",\n        {\n            \"request\": request,\n            \"success\": False,\n            \"error_message\": _t(\"feedback.error_generic\"),\n            \"feedback_id_suffix\": suffix,\n            \"lang\": lang,\n        },\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.health","title":"<code>health(request)</code>  <code>async</code>","text":"<p>Health check endpoint.</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.get(\"/health\", response_class=JSONResponse, tags=[\"Health\"])\n@limiter.limit(\"120/minute\")\nasync def health(request: Request):\n    \"\"\"Health check endpoint.\"\"\"\n    data_svc = request.app.state.data\n    return {\"status\": \"ok\", \"periods_loaded\": list(data_svc.loaded_periods)}\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.ollama_health","title":"<code>ollama_health(request)</code>  <code>async</code>","text":"<p>Check Ollama connectivity and model availability.</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.get(\"/api/ollama/health\", response_class=JSONResponse, tags=[\"Health\"])\n@limiter.limit(\"10/minute\")\nasync def ollama_health(request: Request) -&gt; dict:\n    \"\"\"Check Ollama connectivity and model availability.\"\"\"\n    client = OllamaClient()\n    available = await asyncio.to_thread(client.is_available)\n    return {\n        \"available\": available,\n        \"base_url\": client.base_url,\n        \"model\": client.model,\n    }\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/api/#pspcz_analyzer.routes.api.ollama_smoke_test","title":"<code>ollama_smoke_test(request)</code>  <code>async</code>","text":"<p>Run concurrent bilingual generation to verify Ollama end-to-end.</p> Source code in <code>pspcz_analyzer/routes/api.py</code> <pre><code>@router.get(\"/api/ollama/smoke-test\", response_class=JSONResponse, tags=[\"Health\"])\n@limiter.limit(\"2/minute\")\nasync def ollama_smoke_test(request: Request) -&gt; dict:\n    \"\"\"Run concurrent bilingual generation to verify Ollama end-to-end.\"\"\"\n    client = OllamaClient()\n    start = time.monotonic()\n\n    available = await asyncio.to_thread(client.is_available)\n    if not available:\n        duration = time.monotonic() - start\n        raise HTTPException(\n            status_code=503,\n            detail=_build_smoke_error(\"Ollama is not available\", duration, client.model),\n        )\n\n    try:\n        cs_result, en_result = await asyncio.gather(\n            asyncio.to_thread(client.summarize, _SMOKE_TEST_TEXT, _SMOKE_TEST_TITLE),\n            asyncio.to_thread(client.summarize_en, _SMOKE_TEST_TEXT, _SMOKE_TEST_TITLE),\n        )\n    except Exception as exc:\n        duration = time.monotonic() - start\n        raise HTTPException(\n            status_code=502,\n            detail=_build_smoke_error(str(exc), duration, client.model),\n        ) from exc\n\n    duration = time.monotonic() - start\n    return {\n        \"success\": True,\n        \"model\": client.model,\n        \"duration_seconds\": round(duration, 2),\n        \"summary_cs\": cs_result,\n        \"summary_en\": en_result,\n        \"summary_cs_length\": len(cs_result),\n        \"summary_en_length\": len(en_result),\n    }\n</code></pre>"},{"location":"reference/pspcz_analyzer/routes/charts/","title":"charts","text":""},{"location":"reference/pspcz_analyzer/routes/charts/#pspcz_analyzer.routes.charts","title":"<code>charts</code>","text":"<p>Chart image endpoints \u2014 seaborn renders to PNG, served as StreamingResponse.</p>"},{"location":"reference/pspcz_analyzer/routes/pages/","title":"pages","text":""},{"location":"reference/pspcz_analyzer/routes/pages/#pspcz_analyzer.routes.pages","title":"<code>pages</code>","text":"<p>HTML page routes (full-page renders).</p>"},{"location":"reference/pspcz_analyzer/routes/pages/#pspcz_analyzer.routes.pages.set_lang","title":"<code>set_lang(request, lang)</code>  <code>async</code>","text":"<p>Set the UI language via cookie and redirect back.</p> Source code in <code>pspcz_analyzer/routes/pages.py</code> <pre><code>@router.get(\"/set-lang/{lang}\")\nasync def set_lang(request: Request, lang: str) -&gt; Response:\n    \"\"\"Set the UI language via cookie and redirect back.\"\"\"\n    if lang not in SUPPORTED_LANGUAGES:\n        lang = \"cs\"\n    referer = request.headers.get(\"referer\", \"/\")\n    response = RedirectResponse(url=referer, status_code=303)\n    response.set_cookie(\"lang\", lang, max_age=365 * 24 * 3600, samesite=\"lax\", httponly=False)\n    return response\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/","title":"services","text":""},{"location":"reference/pspcz_analyzer/services/#pspcz_analyzer.services","title":"<code>services</code>","text":""},{"location":"reference/pspcz_analyzer/services/analysis_cache/","title":"analysis_cache","text":""},{"location":"reference/pspcz_analyzer/services/analysis_cache/#pspcz_analyzer.services.analysis_cache","title":"<code>analysis_cache</code>","text":"<p>In-memory TTL cache for expensive analysis computations.</p>"},{"location":"reference/pspcz_analyzer/services/analysis_cache/#pspcz_analyzer.services.analysis_cache.AnalysisCache","title":"<code>AnalysisCache(ttl=3600)</code>","text":"<p>Thread-safe dict cache with TTL expiry.</p> Source code in <code>pspcz_analyzer/services/analysis_cache.py</code> <pre><code>def __init__(self, ttl: int = 3600):\n    self._ttl = ttl\n    self._store: dict[str, tuple[float, Any]] = {}\n    self._lock = threading.Lock()\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/attendance_service/","title":"attendance_service","text":""},{"location":"reference/pspcz_analyzer/services/attendance_service/#pspcz_analyzer.services.attendance_service","title":"<code>attendance_service</code>","text":"<p>Attendance / participation rate analysis.</p>"},{"location":"reference/pspcz_analyzer/services/attendance_service/#pspcz_analyzer.services.attendance_service.compute_attendance","title":"<code>compute_attendance(data, top=30, sort='worst', party_filter=None)</code>","text":"<p>Compute attendance rates for MPs.</p> <p>Categories: - Active: voted YES (A), NO (B), or ABSTAINED (C) - Passive: registered but didn't press a button (F) - Absent: not registered (@) - Excused: excused absence (M)</p> <p>Attendance % = active / (total - excused) x 100</p> Source code in <code>pspcz_analyzer/services/attendance_service.py</code> <pre><code>def compute_attendance(\n    data: PeriodData,\n    top: int = 30,\n    sort: str = \"worst\",\n    party_filter: str | None = None,\n) -&gt; list[dict]:\n    \"\"\"Compute attendance rates for MPs.\n\n    Categories:\n    - Active: voted YES (A), NO (B), or ABSTAINED (C)\n    - Passive: registered but didn't press a button (F)\n    - Absent: not registered (@)\n    - Excused: excused absence (M)\n\n    Attendance % = active / (total - excused) x 100\n    \"\"\"\n    # Exclude void votes\n    void_ids = data.void_votes.get_column(\"id_hlasovani\")\n    mp_votes = data.mp_votes.filter(~pl.col(\"id_hlasovani\").is_in(void_ids))\n\n    active_set = {VoteResult.YES, VoteResult.NO, VoteResult.ABSTAINED}\n\n    per_mp = mp_votes.group_by(\"id_poslanec\").agg(\n        pl.col(\"vysledek\").is_in(active_set).sum().alias(\"active\"),\n        (pl.col(\"vysledek\") == VoteResult.YES).sum().alias(\"yes_votes\"),\n        (pl.col(\"vysledek\") == VoteResult.NO).sum().alias(\"no_votes\"),\n        (pl.col(\"vysledek\") == VoteResult.ABSTAINED).sum().alias(\"abstained\"),\n        (pl.col(\"vysledek\") == VoteResult.DID_NOT_VOTE).sum().alias(\"passive\"),\n        (pl.col(\"vysledek\") == VoteResult.ABSENT).sum().alias(\"absent\"),\n        (pl.col(\"vysledek\") == VoteResult.EXCUSED).sum().alias(\"excused\"),\n        pl.len().alias(\"total\"),\n    )\n\n    per_mp = per_mp.with_columns(\n        (pl.col(\"active\") / (pl.col(\"total\") - pl.col(\"excused\")).cast(pl.Float64) * 100).alias(\n            \"attendance_pct\"\n        )\n    )\n\n    # Join with MP info\n    result = per_mp.join(data.mp_info, on=\"id_poslanec\", how=\"left\")\n\n    if party_filter:\n        result = result.filter(pl.col(\"party\").str.to_uppercase() == party_filter.upper())\n\n    # Sort by the requested metric\n    sort_config: dict[str, tuple[str, bool]] = {\n        \"worst\": (\"attendance_pct\", False),\n        \"best\": (\"attendance_pct\", True),\n        \"most_active\": (\"active\", True),\n        \"least_active\": (\"active\", False),\n        \"most_abstained\": (\"abstained\", True),\n        \"most_excused\": (\"excused\", True),\n        \"most_passive\": (\"passive\", True),\n        \"most_absent\": (\"absent\", True),\n        \"most_yes\": (\"yes_votes\", True),\n        \"most_no\": (\"no_votes\", True),\n    }\n    col, desc = sort_config.get(sort, (\"attendance_pct\", False))\n    result = result.sort(col, descending=desc).head(top)\n\n    return result.select(\n        \"jmeno\",\n        \"prijmeni\",\n        \"party\",\n        \"active\",\n        \"yes_votes\",\n        \"no_votes\",\n        \"abstained\",\n        \"passive\",\n        \"absent\",\n        \"excused\",\n        \"attendance_pct\",\n    ).to_dicts()\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/daily_refresh_service/","title":"daily_refresh_service","text":""},{"location":"reference/pspcz_analyzer/services/daily_refresh_service/#pspcz_analyzer.services.daily_refresh_service","title":"<code>daily_refresh_service</code>","text":"<p>Daily data refresh scheduler.</p> <p>Pauses the tisk AI pipeline, re-downloads fresh data from psp.cz, reloads all in-memory state, then restarts the pipeline. Uses pure asyncio \u2014 no external scheduler dependencies.</p>"},{"location":"reference/pspcz_analyzer/services/daily_refresh_service/#pspcz_analyzer.services.daily_refresh_service.DailyRefreshService","title":"<code>DailyRefreshService(data_service, hour=DAILY_REFRESH_HOUR)</code>","text":"<p>Asyncio-based daily scheduler that refreshes all psp.cz data.</p> Source code in <code>pspcz_analyzer/services/daily_refresh_service.py</code> <pre><code>def __init__(self, data_service: DataService, hour: int = DAILY_REFRESH_HOUR) -&gt; None:\n    self._data_service = data_service\n    self._hour = hour\n    self._task: asyncio.Task | None = None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/daily_refresh_service/#pspcz_analyzer.services.daily_refresh_service.DailyRefreshService.start","title":"<code>start()</code>","text":"<p>Start the scheduler loop. Idempotent \u2014 does nothing if already running.</p> Source code in <code>pspcz_analyzer/services/daily_refresh_service.py</code> <pre><code>def start(self) -&gt; None:\n    \"\"\"Start the scheduler loop. Idempotent \u2014 does nothing if already running.\"\"\"\n    if not DAILY_REFRESH_ENABLED:\n        logger.info(\"[daily-refresh] Scheduler disabled via DAILY_REFRESH_ENABLED=0\")\n        return\n\n    if self._task is not None and not self._task.done():\n        return\n\n    self._task = asyncio.create_task(\n        self._scheduler_loop(),\n        name=\"daily-refresh-scheduler\",\n    )\n    logger.info(\n        \"[daily-refresh] Scheduler started (refresh at {:02d}:00 CET daily)\",\n        self._hour,\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/daily_refresh_service/#pspcz_analyzer.services.daily_refresh_service.DailyRefreshService.stop","title":"<code>stop()</code>  <code>async</code>","text":"<p>Cancel the scheduler gracefully.</p> Source code in <code>pspcz_analyzer/services/daily_refresh_service.py</code> <pre><code>async def stop(self) -&gt; None:\n    \"\"\"Cancel the scheduler gracefully.\"\"\"\n    if self._task is not None and not self._task.done():\n        self._task.cancel()\n        with contextlib.suppress(asyncio.CancelledError):\n            await self._task\n        logger.info(\"[daily-refresh] Scheduler stopped\")\n    self._task = None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/daily_refresh_service/#pspcz_analyzer.services.daily_refresh_service.DailyRefreshService.trigger_now","title":"<code>trigger_now()</code>  <code>async</code>","text":"<p>Manually trigger an immediate refresh (for admin/debug use).</p> Source code in <code>pspcz_analyzer/services/daily_refresh_service.py</code> <pre><code>async def trigger_now(self) -&gt; None:\n    \"\"\"Manually trigger an immediate refresh (for admin/debug use).\"\"\"\n    logger.info(\"[daily-refresh] Manual refresh triggered\")\n    await self._data_service.refresh_all_data()\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/data_service/","title":"data_service","text":""},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service","title":"<code>data_service</code>","text":"<p>Data service: orchestrates download, parsing, caching, and holds DataFrames.</p>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService","title":"<code>DataService(cache_dir=DEFAULT_CACHE_DIR)</code>","text":"<p>Manages data for multiple electoral periods, loading on demand.</p> Source code in <code>pspcz_analyzer/services/data_service.py</code> <pre><code>def __init__(self, cache_dir: Path = DEFAULT_CACHE_DIR) -&gt; None:\n    self.cache_dir = cache_dir\n    self._periods: dict[int, PeriodData] = {}\n    self.tisk_text = TiskTextService(cache_dir)\n    self.tisk_pipeline = TiskPipelineService(cache_dir)\n    self._cache_mgr = TiskCacheManager(cache_dir)\n    self._refresh_lock = asyncio.Lock()\n\n    # Shared tables (not period-specific), populated by _load_shared_tables\n    self._persons: pl.DataFrame | None = None\n    self._mps: pl.DataFrame | None = None\n    self._organs: pl.DataFrame | None = None\n    self._memberships: pl.DataFrame | None = None\n    self._poslanci_dir: Path | None = None\n\n    # Session/tisk tables (shared across all periods)\n    self._schuze: pl.DataFrame | None = None\n    self._bod_schuze: pl.DataFrame | None = None\n    self._tisky: pl.DataFrame | None = None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService.available_periods","title":"<code>available_periods</code>  <code>property</code>","text":"<p>All periods available for selection, sorted descending.</p>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService.get_period","title":"<code>get_period(period)</code>","text":"<p>Get data for a period, loading it on demand if needed.</p> <p>Also refreshes topic/summary data from the parquet cache if the pipeline has written new data since last access.</p> Source code in <code>pspcz_analyzer/services/data_service.py</code> <pre><code>def get_period(self, period: int) -&gt; PeriodData:\n    \"\"\"Get data for a period, loading it on demand if needed.\n\n    Also refreshes topic/summary data from the parquet cache if the\n    pipeline has written new data since last access.\n    \"\"\"\n    if period not in self._periods:\n        self._load_period(period)\n    self._refresh_tisk_data(period)\n    return self._periods[period]\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService.initialize","title":"<code>initialize(period=DEFAULT_PERIOD)</code>","text":"<p>Pre-load shared data and the default period.</p> Source code in <code>pspcz_analyzer/services/data_service.py</code> <pre><code>def initialize(self, period: int = DEFAULT_PERIOD) -&gt; None:\n    \"\"\"Pre-load shared data and the default period.\"\"\"\n    self._load_shared_tables()\n    self._load_period(period)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService.start_tisk_pipeline","title":"<code>start_tisk_pipeline(period)</code>","text":"<p>Kick off background tisk processing for a period.</p> <p>Extracts the list of ct numbers from the already-loaded tisky table and starts the pipeline. On completion, updates in-memory tisk_lookup entries with fresh topics, summaries, and has_text flags.</p> Source code in <code>pspcz_analyzer/services/data_service.py</code> <pre><code>def start_tisk_pipeline(self, period: int) -&gt; None:\n    \"\"\"Kick off background tisk processing for a period.\n\n    Extracts the list of ct numbers from the already-loaded tisky table\n    and starts the pipeline. On completion, updates in-memory tisk_lookup\n    entries with fresh topics, summaries, and has_text flags.\n    \"\"\"\n    if self._tisky is None:\n        return\n\n    organ_id = PERIOD_ORGAN_IDS[period]\n    period_tisky = self._tisky.filter(\n        (pl.col(\"id_obdobi\") == organ_id) &amp; pl.col(\"ct\").is_not_null()\n    )\n    ct_numbers = sorted(period_tisky.get_column(\"ct\").unique().to_list())\n    if not ct_numbers:\n        return\n\n    def _on_complete(\n        p: int,\n        text_paths: dict,\n        topic_map: dict,\n        summary_map: dict,\n        *_args: object,\n        **_kwargs: object,\n    ) -&gt; None:\n        \"\"\"Callback: refresh in-memory tisk data after pipeline finishes.\"\"\"\n        self._cache_mgr.invalidate(p)\n        pd = self._periods.get(p)\n        if pd is None:\n            return\n        self._refresh_tisk_data(p)\n        logger.info(\n            \"[tisk pipeline] Updated in-memory tisk data for period {}\",\n            p,\n        )\n\n    self.tisk_pipeline.start_period(period, ct_numbers, on_complete=_on_complete)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService.start_all_tisk_pipelines","title":"<code>start_all_tisk_pipelines()</code>","text":"<p>Kick off sequential background tisk processing for ALL periods (newest first).</p> <p>Does not require periods to be loaded \u2014 uses the shared tisky table to get ct numbers. When a period completes, updates in-memory data if that period happens to be loaded.</p> Source code in <code>pspcz_analyzer/services/data_service.py</code> <pre><code>def start_all_tisk_pipelines(self) -&gt; None:\n    \"\"\"Kick off sequential background tisk processing for ALL periods (newest first).\n\n    Does not require periods to be loaded \u2014 uses the shared tisky table\n    to get ct numbers. When a period completes, updates in-memory data\n    if that period happens to be loaded.\n    \"\"\"\n    if self._tisky is None:\n        return\n\n    period_ct: list[tuple[int, list[int]]] = []\n    for period in sorted(PERIOD_ORGAN_IDS.keys(), reverse=True):\n        organ_id = PERIOD_ORGAN_IDS[period]\n        period_tisky = self._tisky.filter(\n            (pl.col(\"id_obdobi\") == organ_id) &amp; pl.col(\"ct\").is_not_null()\n        )\n        ct_numbers = sorted(period_tisky.get_column(\"ct\").unique().to_list())\n        if ct_numbers:\n            period_ct.append((period, ct_numbers))\n\n    if not period_ct:\n        return\n\n    def _on_complete(\n        p: int,\n        text_paths: dict,\n        topic_map: dict,\n        summary_map: dict,\n        *_args: object,\n        **_kwargs: object,\n    ) -&gt; None:\n        self._cache_mgr.invalidate(p)\n        pd = self._periods.get(p)\n        if pd is None:\n            return\n        self._refresh_tisk_data(p)\n        logger.info(\"[tisk pipeline] Updated in-memory tisk data for period {}\", p)\n\n    self.tisk_pipeline.start_all_periods(period_ct, on_complete=_on_complete)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/data_service/#pspcz_analyzer.services.data_service.DataService.refresh_all_data","title":"<code>refresh_all_data()</code>  <code>async</code>","text":"<p>Re-download all data from psp.cz and reload in-memory state.</p> <p>Pauses the tisk AI pipeline, refreshes data, then restarts the pipeline. Safe for concurrent HTTP requests \u2014 old data stays valid until swapped.</p> Source code in <code>pspcz_analyzer/services/data_service.py</code> <pre><code>async def refresh_all_data(self) -&gt; None:\n    \"\"\"Re-download all data from psp.cz and reload in-memory state.\n\n    Pauses the tisk AI pipeline, refreshes data, then restarts the pipeline.\n    Safe for concurrent HTTP requests \u2014 old data stays valid until swapped.\n    \"\"\"\n    if self._refresh_lock.locked():\n        logger.warning(\"[daily-refresh] Refresh already in progress, skipping\")\n        return\n\n    async with self._refresh_lock:\n        logger.info(\"[daily-refresh] Starting full data refresh ...\")\n\n        # 1. Cancel tisk pipeline\n        await self.tisk_pipeline.cancel_all()\n\n        # 2. Re-download and reload shared tables\n        try:\n            await asyncio.to_thread(self._force_reload_shared_tables)\n            logger.info(\"[daily-refresh] Shared tables reloaded\")\n        except Exception:\n            logger.opt(exception=True).error(\"[daily-refresh] Failed to reload shared tables\")\n\n        # 3. Re-download and reload each loaded period\n        for period in list(self._periods.keys()):\n            try:\n                await asyncio.to_thread(self._force_reload_period, period)\n                logger.info(\"[daily-refresh] Period {} reloaded\", period)\n            except Exception:\n                logger.opt(exception=True).error(\n                    \"[daily-refresh] Failed to reload period {}\", period\n                )\n\n        # 4. Invalidate analysis caches\n        analysis_cache.invalidate()\n        for period in self._periods:\n            self._cache_mgr.invalidate(period)\n\n        # 5. Restart tisk pipeline with fresh data\n        self.start_all_tisk_pipelines()\n\n        logger.info(\"[daily-refresh] Full data refresh complete\")\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/feedback_service/","title":"feedback_service","text":""},{"location":"reference/pspcz_analyzer/services/feedback_service/#pspcz_analyzer.services.feedback_service","title":"<code>feedback_service</code>","text":"<p>GitHub Issues integration for user feedback on vote data and AI summaries.</p>"},{"location":"reference/pspcz_analyzer/services/feedback_service/#pspcz_analyzer.services.feedback_service.GitHubFeedbackClient","title":"<code>GitHubFeedbackClient()</code>","text":"<p>Creates GitHub issues from user feedback, modeled after OllamaClient.</p> Source code in <code>pspcz_analyzer/services/feedback_service.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.enabled = GITHUB_FEEDBACK_ENABLED\n    self.token = GITHUB_FEEDBACK_TOKEN\n    self.repo = GITHUB_FEEDBACK_REPO\n    self.labels = [label.strip() for label in GITHUB_FEEDBACK_LABELS if label.strip()]\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/feedback_service/#pspcz_analyzer.services.feedback_service.GitHubFeedbackClient.is_configured","title":"<code>is_configured()</code>","text":"<p>Check if the feedback feature is enabled and has a valid token.</p> Source code in <code>pspcz_analyzer/services/feedback_service.py</code> <pre><code>def is_configured(self) -&gt; bool:\n    \"\"\"Check if the feedback feature is enabled and has a valid token.\"\"\"\n    return self.enabled and bool(self.token)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/feedback_service/#pspcz_analyzer.services.feedback_service.GitHubFeedbackClient.create_issue","title":"<code>create_issue(title, body, vote_id, period, page_url, lang)</code>","text":"<p>Create a GitHub issue with vote metadata.</p> <p>Returns:</p> Type Description <code>dict | None</code> <p>{\"number\": N, \"html_url\": \"...\"} on success, None on failure.</p> Source code in <code>pspcz_analyzer/services/feedback_service.py</code> <pre><code>def create_issue(\n    self,\n    title: str,\n    body: str,\n    vote_id: int,\n    period: int,\n    page_url: str,\n    lang: str,\n) -&gt; dict | None:\n    \"\"\"Create a GitHub issue with vote metadata.\n\n    Returns:\n        {\"number\": N, \"html_url\": \"...\"} on success, None on failure.\n    \"\"\"\n    if not self.is_configured():\n        logger.warning(\"GitHub feedback not configured, skipping issue creation\")\n        return None\n\n    issue_title = f\"[Feedback] Vote #{vote_id}: {title}\"\n    issue_body = _build_issue_body(body, vote_id, period, page_url, lang)\n\n    url = f\"{_GITHUB_API_BASE}/repos/{self.repo}/issues\"\n    headers = {\n        \"Authorization\": f\"Bearer {self.token}\",\n        \"Accept\": \"application/vnd.github+json\",\n        \"X-GitHub-Api-Version\": _GITHUB_API_VERSION,\n    }\n    payload: dict = {\n        \"title\": issue_title,\n        \"body\": issue_body,\n    }\n    if self.labels:\n        payload[\"labels\"] = self.labels\n\n    try:\n        resp = httpx.post(url, json=payload, headers=headers, timeout=_REQUEST_TIMEOUT)\n        if resp.status_code == 201:\n            data = resp.json()\n            logger.info(\"Created GitHub issue #{} for vote {}\", data[\"number\"], vote_id)\n            return {\"number\": data[\"number\"], \"html_url\": data[\"html_url\"]}\n        logger.error(\"GitHub API returned {}: {}\", resp.status_code, resp.text[:200])\n        return None\n    except httpx.HTTPError as exc:\n        logger.error(\"GitHub API request failed: {}\", exc)\n        return None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/loyalty_service/","title":"loyalty_service","text":""},{"location":"reference/pspcz_analyzer/services/loyalty_service/#pspcz_analyzer.services.loyalty_service","title":"<code>loyalty_service</code>","text":"<p>Party loyalty / rebellion analysis.</p>"},{"location":"reference/pspcz_analyzer/services/loyalty_service/#pspcz_analyzer.services.loyalty_service.compute_loyalty","title":"<code>compute_loyalty(data, top=30, party_filter=None)</code>","text":"<p>Compute rebellion rates for MPs.</p> <p>For each vote, determine the party's majority direction (YES vs NO). An MP \"rebels\" when they actively vote against that majority.</p> <p>Returns a list of dicts sorted by rebellion rate descending.</p> Source code in <code>pspcz_analyzer/services/loyalty_service.py</code> <pre><code>def compute_loyalty(\n    data: PeriodData,\n    top: int = 30,\n    party_filter: str | None = None,\n) -&gt; list[dict]:\n    \"\"\"Compute rebellion rates for MPs.\n\n    For each vote, determine the party's majority direction (YES vs NO).\n    An MP \"rebels\" when they actively vote against that majority.\n\n    Returns a list of dicts sorted by rebellion rate descending.\n    \"\"\"\n    # Exclude void votes\n    void_ids = data.void_votes.get_column(\"id_hlasovani\")\n    mp_votes = data.mp_votes.filter(~pl.col(\"id_hlasovani\").is_in(void_ids))\n\n    # Only active votes (YES or NO)\n    active_results = {VoteResult.YES, VoteResult.NO}\n    active_votes = mp_votes.filter(pl.col(\"vysledek\").is_in(active_results))\n\n    # Map id_poslanec -&gt; id_osoba via mp_info\n    active_votes = active_votes.join(\n        data.mp_info.select(\"id_poslanec\", \"id_osoba\", \"party\"),\n        on=\"id_poslanec\",\n        how=\"inner\",\n    )\n\n    # For each vote + party, compute majority direction\n    party_majority = (\n        active_votes.group_by([\"id_hlasovani\", \"party\"])\n        .agg(\n            (pl.col(\"vysledek\") == VoteResult.YES).sum().alias(\"yes_count\"),\n            (pl.col(\"vysledek\") == VoteResult.NO).sum().alias(\"no_count\"),\n        )\n        .with_columns(\n            pl.when(pl.col(\"yes_count\") &gt; pl.col(\"no_count\"))\n            .then(pl.lit(VoteResult.YES))\n            .when(pl.col(\"no_count\") &gt; pl.col(\"yes_count\"))\n            .then(pl.lit(VoteResult.NO))\n            .otherwise(pl.lit(None))\n            .alias(\"party_direction\")\n        )\n        .filter(pl.col(\"party_direction\").is_not_null())\n    )\n\n    # Join back to individual votes\n    with_direction = active_votes.join(\n        party_majority.select(\"id_hlasovani\", \"party\", \"party_direction\"),\n        on=[\"id_hlasovani\", \"party\"],\n        how=\"inner\",\n    )\n\n    # Flag rebellions\n    with_direction = with_direction.with_columns(\n        (pl.col(\"vysledek\") != pl.col(\"party_direction\")).alias(\"is_rebellion\")\n    )\n\n    # Collect rebellion vote details before aggregating\n    rebellions_df = (\n        with_direction.filter(pl.col(\"is_rebellion\"))\n        .join(\n            data.votes.select(\"id_hlasovani\", \"datum\", \"nazev_dlouhy\", \"schuze\", \"bod\"),\n            on=\"id_hlasovani\",\n            how=\"left\",\n        )\n        .select(\n            \"id_poslanec\",\n            \"id_hlasovani\",\n            \"datum\",\n            \"nazev_dlouhy\",\n            \"schuze\",\n            \"bod\",\n            pl.col(\"vysledek\").alias(\"mp_vote\"),\n            \"party_direction\",\n        )\n    )\n\n    # Build per-MP rebellion vote lists\n    rebellion_map: dict[int, list[dict]] = {}\n    for row in rebellions_df.iter_rows(named=True):\n        mp_id = row[\"id_poslanec\"]\n        schuze = row[\"schuze\"]\n        bod = row[\"bod\"]\n        tisk = data.get_tisk(schuze, bod) if schuze and bod else None\n        rebellion_map.setdefault(mp_id, []).append(\n            {\n                \"id_hlasovani\": row[\"id_hlasovani\"],\n                \"datum\": row[\"datum\"] or \"\",\n                \"nazev_dlouhy\": row[\"nazev_dlouhy\"] or \"\",\n                \"mp_vote\": row[\"mp_vote\"],\n                \"party_direction\": row[\"party_direction\"],\n                \"schuze\": schuze,\n                \"bod\": bod,\n                \"tisk_url\": tisk.url if tisk else None,\n            }\n        )\n\n    # Aggregate per MP\n    per_mp = with_direction.group_by(\"id_poslanec\").agg(\n        pl.col(\"is_rebellion\").sum().alias(\"rebellions\"),\n        pl.len().alias(\"active_votes\"),\n    )\n\n    per_mp = per_mp.with_columns(\n        (pl.col(\"rebellions\") / pl.col(\"active_votes\") * 100).alias(\"rebellion_pct\")\n    )\n\n    # Join with MP info\n    result = per_mp.join(data.mp_info, on=\"id_poslanec\", how=\"left\")\n\n    if party_filter:\n        result = result.filter(pl.col(\"party\").str.to_uppercase() == party_filter.upper())\n\n    result = result.sort(\"rebellion_pct\", descending=True).head(top)\n\n    rows = result.select(\n        \"id_poslanec\",\n        \"jmeno\",\n        \"prijmeni\",\n        \"party\",\n        \"active_votes\",\n        \"rebellions\",\n        \"rebellion_pct\",\n    ).to_dicts()\n\n    # Attach rebellion vote details to each row\n    for row in rows:\n        votes = rebellion_map.get(row[\"id_poslanec\"], [])\n        votes.sort(key=lambda v: v[\"datum\"], reverse=True)\n        row[\"rebellion_votes\"] = votes\n        del row[\"id_poslanec\"]\n\n    return rows\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/mp_builder/","title":"mp_builder","text":""},{"location":"reference/pspcz_analyzer/services/mp_builder/#pspcz_analyzer.services.mp_builder","title":"<code>mp_builder</code>","text":"<p>Build MP info table: id_poslanec -&gt; name, party for a given period.</p>"},{"location":"reference/pspcz_analyzer/services/mp_builder/#pspcz_analyzer.services.mp_builder.build_mp_info","title":"<code>build_mp_info(period, mps, persons, organs, memberships)</code>","text":"<p>Build MP lookup table: id_poslanec -&gt; name, party for a given period.</p> Source code in <code>pspcz_analyzer/services/mp_builder.py</code> <pre><code>def build_mp_info(\n    period: int,\n    mps: pl.DataFrame,\n    persons: pl.DataFrame,\n    organs: pl.DataFrame,\n    memberships: pl.DataFrame,\n) -&gt; pl.DataFrame:\n    \"\"\"Build MP lookup table: id_poslanec -&gt; name, party for a given period.\"\"\"\n    organ_id = PERIOD_ORGAN_IDS[period]\n    period_mps = mps.filter(pl.col(\"id_obdobi\") == organ_id)\n\n    mp_persons = period_mps.join(\n        persons.select(\"id_osoba\", \"jmeno\", \"prijmeni\"),\n        on=\"id_osoba\",\n        how=\"left\",\n    )\n\n    clubs = organs.filter(pl.col(\"id_typ_organu\") == 1).select(\"id_organ\", \"zkratka\")\n\n    club_memberships = memberships.join(\n        clubs, left_on=\"id_of\", right_on=\"id_organ\", how=\"inner\"\n    ).select(\"id_osoba\", \"zkratka\", \"od_o\", \"do_o\")\n\n    club_memberships = club_memberships.sort(\"od_o\", descending=True).unique(\n        subset=[\"id_osoba\"], keep=\"first\"\n    )\n\n    mp_info = mp_persons.join(\n        club_memberships.select(\"id_osoba\", pl.col(\"zkratka\").alias(\"party\")),\n        on=\"id_osoba\",\n        how=\"left\",\n    ).select(\"id_poslanec\", \"id_osoba\", \"jmeno\", \"prijmeni\", \"party\")\n\n    # Normalize party abbreviations from psp.cz to commonly used names.\n    # \"ANO2011\" is the official registration name but everyone calls it \"ANO\".\n    # \"Neza\u0159az\" is the truncated abbreviation for independent MPs (\"Neza\u0159azen\u00ed\").\n    party_aliases = {\n        \"ANO2011\": \"ANO\",\n        \"Neza\u0159az\": \"Neza\u0159azen\u00ed\",\n    }\n    return mp_info.with_columns(pl.col(\"party\").replace(party_aliases).alias(\"party\"))\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/","title":"ollama_service","text":""},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service","title":"<code>ollama_service</code>","text":"<p>Ollama integration for AI topic classification and tisk summarization.</p> <p>When Ollama is running locally with the configured model, provides: - Free-form topic classification (1-3 Czech topic labels per tisk) - Czech-language summaries explaining what each proposed law changes</p> <p>Falls back gracefully to keyword classification when Ollama is unavailable.</p>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient","title":"<code>OllamaClient(base_url=OLLAMA_BASE_URL, model=OLLAMA_MODEL, timeout=OLLAMA_TIMEOUT, api_key=OLLAMA_API_KEY)</code>","text":"<p>Client for local Ollama LLM integration.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def __init__(\n    self,\n    base_url: str = OLLAMA_BASE_URL,\n    model: str = OLLAMA_MODEL,\n    timeout: float = OLLAMA_TIMEOUT,\n    api_key: str = OLLAMA_API_KEY,\n) -&gt; None:\n    self.base_url = base_url.rstrip(\"/\")\n    self.model = model\n    self.timeout = timeout\n    self._available: bool | None = None\n    self._headers: dict[str, str] = {}\n    if api_key:\n        self._headers[\"Authorization\"] = f\"Bearer {api_key}\"\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.is_available","title":"<code>is_available()</code>","text":"<p>Check if Ollama is running and the model is available.</p> <p>Caches result after first call.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def is_available(self) -&gt; bool:\n    \"\"\"Check if Ollama is running and the model is available.\n\n    Caches result after first call.\n    \"\"\"\n    if self._available is not None:\n        return self._available\n\n    try:\n        resp = httpx.get(\n            f\"{self.base_url}/api/tags\",\n            headers=self._headers,\n            timeout=OLLAMA_HEALTH_TIMEOUT,\n        )\n        resp.raise_for_status()\n        models = [m.get(\"name\", \"\") for m in resp.json().get(\"models\", [])]\n        # Match model name with or without tag suffix\n        self._available = any(\n            m == self.model\n            or m.startswith(f\"{self.model}:\")\n            or self.model.startswith(f\"{m}:\")\n            or m == self.model.split(\":\")[0]\n            for m in models\n        )\n        if self._available:\n            logger.info(\"[ollama] Available with model {}\", self.model)\n        else:\n            logger.info(\n                \"[ollama] Running but model {} not found (available: {})\",\n                self.model,\n                \", \".join(models),\n            )\n    except Exception:\n        self._available = False\n        logger.info(\"[ollama] Not available (connection failed)\")\n\n    return self._available\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.classify_topics","title":"<code>classify_topics(text, title)</code>","text":"<p>Classify a tisk into 1-3 free-form topic labels using the LLM.</p> <p>Returns list of Czech topic labels, or empty list on failure.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def classify_topics(self, text: str, title: str) -&gt; list[str]:\n    \"\"\"Classify a tisk into 1-3 free-form topic labels using the LLM.\n\n    Returns list of Czech topic labels, or empty list on failure.\n    \"\"\"\n    truncated = truncate_legislative_text(text)\n    prompt = _CLASSIFICATION_PROMPT_TEMPLATE.format(\n        title=title or \"(bez n\u00e1zvu)\",\n        text=truncated,\n    )\n    response = self._generate(prompt, _CLASSIFICATION_SYSTEM)\n    if response is None:\n        return []\n    return self._parse_topics_response(response)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.summarize","title":"<code>summarize(text, title)</code>","text":"<p>Generate a Czech-language summary of what a proposed law changes.</p> <p>Returns summary text or empty string on failure.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def summarize(self, text: str, title: str) -&gt; str:\n    \"\"\"Generate a Czech-language summary of what a proposed law changes.\n\n    Returns summary text or empty string on failure.\n    \"\"\"\n    truncated = truncate_legislative_text(text)\n    prompt = _SUMMARY_PROMPT_TEMPLATE.format(\n        title=title or \"(bez n\u00e1zvu)\",\n        text=truncated,\n    )\n    response = self._generate(prompt, _SUMMARY_SYSTEM)\n    if not response:\n        return \"\"\n    return self._strip_think(response)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.summarize_en","title":"<code>summarize_en(text, title)</code>","text":"<p>Generate an English-language critical summary of a proposed law.</p> <p>Returns summary text or empty string on failure.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def summarize_en(self, text: str, title: str) -&gt; str:\n    \"\"\"Generate an English-language critical summary of a proposed law.\n\n    Returns summary text or empty string on failure.\n    \"\"\"\n    truncated = truncate_legislative_text(text)\n    prompt = _SUMMARY_PROMPT_TEMPLATE_EN.format(\n        title=title or \"(no title)\",\n        text=truncated,\n    )\n    response = self._generate(prompt, _SUMMARY_SYSTEM_EN)\n    if not response:\n        return \"\"\n    return self._strip_think(response)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.consolidate_topics","title":"<code>consolidate_topics(all_topics)</code>","text":"<p>Consolidate/deduplicate topic labels via the LLM.</p> <p>Sends all unique topics to the model and asks it to merge similar/overlapping ones under canonical names.</p> <p>Returns dict mapping old_name -&gt; canonical_name. Topics not in the response keep their original name.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def consolidate_topics(self, all_topics: list[str]) -&gt; dict[str, str]:\n    \"\"\"Consolidate/deduplicate topic labels via the LLM.\n\n    Sends all unique topics to the model and asks it to merge\n    similar/overlapping ones under canonical names.\n\n    Returns dict mapping old_name -&gt; canonical_name.\n    Topics not in the response keep their original name.\n    \"\"\"\n    topics_list = \"\\n\".join(f\"- {t}\" for t in all_topics)\n    prompt = _CONSOLIDATION_PROMPT_TEMPLATE.format(\n        n=len(all_topics),\n        topics_list=topics_list,\n    )\n    response = self._generate(prompt, _CONSOLIDATION_SYSTEM)\n    if not response:\n        return {t: t for t in all_topics}\n\n    response = self._strip_think(response)\n    mapping: dict[str, str] = {}\n    for line in response.splitlines():\n        line = line.strip()\n        if \" -&gt; \" not in line:\n            continue\n        parts = line.split(\" -&gt; \", 1)\n        old = parts[0].strip().strip(\"- \")\n        new = parts[1].strip()\n        if old and new:\n            mapping[old] = new\n\n    # Any topics not in the mapping keep their original name\n    for t in all_topics:\n        if t not in mapping:\n            mapping[t] = t\n\n    return mapping\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.compare_versions","title":"<code>compare_versions(text_old, text_new, ct1_old, ct1_new, label_old='', label_new='')</code>","text":"<p>Compare two versions of a tisk and return a Czech-language diff summary.</p> <p>Returns 3-4 sentence summary or empty string on failure.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def compare_versions(\n    self,\n    text_old: str,\n    text_new: str,\n    ct1_old: int,\n    ct1_new: int,\n    label_old: str = \"\",\n    label_new: str = \"\",\n) -&gt; str:\n    \"\"\"Compare two versions of a tisk and return a Czech-language diff summary.\n\n    Returns 3-4 sentence summary or empty string on failure.\n    \"\"\"\n    trunc_old = truncate_legislative_text(text_old)\n    trunc_new = truncate_legislative_text(text_new)\n    prompt = _COMPARISON_PROMPT_TEMPLATE.format(\n        ct1_old=ct1_old,\n        ct1_new=ct1_new,\n        label_old=label_old or f\"CT1={ct1_old}\",\n        label_new=label_new or f\"CT1={ct1_new}\",\n        text_old=trunc_old,\n        text_new=trunc_new,\n    )\n    response = self._generate(prompt, _COMPARISON_SYSTEM)\n    if not response:\n        return \"\"\n    return self._strip_think(response)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.summarize_bilingual","title":"<code>summarize_bilingual(text, title)</code>","text":"<p>Generate both Czech and English summaries.</p> <p>Returns {\"cs\": ..., \"en\": ...}. Either may be empty on failure.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def summarize_bilingual(self, text: str, title: str) -&gt; dict[str, str]:\n    \"\"\"Generate both Czech and English summaries.\n\n    Returns {\"cs\": ..., \"en\": ...}. Either may be empty on failure.\n    \"\"\"\n    cs = self.summarize(text, title)\n    en = self.summarize_en(text, title)\n    return {\"cs\": cs, \"en\": en}\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.OllamaClient.compare_versions_bilingual","title":"<code>compare_versions_bilingual(text_old, text_new, ct1_old, ct1_new, label_old='', label_new='')</code>","text":"<p>Compare two versions and return bilingual diff summaries.</p> <p>Returns {\"cs\": ..., \"en\": ...}. Either may be empty on failure.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def compare_versions_bilingual(\n    self,\n    text_old: str,\n    text_new: str,\n    ct1_old: int,\n    ct1_new: int,\n    label_old: str = \"\",\n    label_new: str = \"\",\n) -&gt; dict[str, str]:\n    \"\"\"Compare two versions and return bilingual diff summaries.\n\n    Returns {\"cs\": ..., \"en\": ...}. Either may be empty on failure.\n    \"\"\"\n    cs = self.compare_versions(text_old, text_new, ct1_old, ct1_new, label_old, label_new)\n    trunc_old = truncate_legislative_text(text_old)\n    trunc_new = truncate_legislative_text(text_new)\n    prompt = _COMPARISON_PROMPT_TEMPLATE_EN.format(\n        ct1_old=ct1_old,\n        ct1_new=ct1_new,\n        label_old=label_old or f\"CT1={ct1_old}\",\n        label_new=label_new or f\"CT1={ct1_new}\",\n        text_old=trunc_old,\n        text_new=trunc_new,\n    )\n    response = self._generate(prompt, _COMPARISON_SYSTEM_EN)\n    en = self._strip_think(response) if response else \"\"\n    return {\"cs\": cs, \"en\": en}\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.truncate_legislative_text","title":"<code>truncate_legislative_text(text, verbatim_chars=OLLAMA_VERBATIM_CHARS, max_chars=OLLAMA_MAX_TEXT_CHARS)</code>","text":"<p>Truncate Czech legislative text intelligently for LLM processing.</p> <p>When TISK_SHORTENER is disabled (0), returns the full text unmodified.</p> <p>Strategy (when enabled): 1. First <code>verbatim_chars</code> characters verbatim (captures explanatory report) 2. From remainder: extract heading lines + first 200 chars after each heading 3. Hard cap at <code>max_chars</code> total</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def truncate_legislative_text(\n    text: str,\n    verbatim_chars: int = OLLAMA_VERBATIM_CHARS,\n    max_chars: int = OLLAMA_MAX_TEXT_CHARS,\n) -&gt; str:\n    \"\"\"Truncate Czech legislative text intelligently for LLM processing.\n\n    When TISK_SHORTENER is disabled (0), returns the full text unmodified.\n\n    Strategy (when enabled):\n    1. First `verbatim_chars` characters verbatim (captures explanatory report)\n    2. From remainder: extract heading lines + first 200 chars after each heading\n    3. Hard cap at `max_chars` total\n    \"\"\"\n    if not TISK_SHORTENER:\n        return text\n\n    if len(text) &lt;= max_chars:\n        return text\n\n    result = text[:verbatim_chars]\n    remainder = text[verbatim_chars:]\n\n    # Extract structural highlights from the remainder\n    highlights: list[str] = []\n    for match in _HEADING_RE.finditer(remainder):\n        start = match.start()\n        # Grab heading + 200 chars after it\n        snippet = remainder[start : start + 200 + len(match.group())]\n        highlights.append(snippet.strip())\n\n    if highlights:\n        result += \"\\n\\n[...]\\n\\n\" + \"\\n\\n\".join(highlights)\n\n    return result[:max_chars]\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.serialize_topics","title":"<code>serialize_topics(topics)</code>","text":"<p>Serialize topic list for parquet storage.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def serialize_topics(topics: list[str]) -&gt; str:\n    \"\"\"Serialize topic list for parquet storage.\"\"\"\n    return json.dumps(topics, ensure_ascii=False)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/ollama_service/#pspcz_analyzer.services.ollama_service.deserialize_topics","title":"<code>deserialize_topics(raw)</code>","text":"<p>Deserialize topic list from parquet storage.</p> <p>Handles both new JSON format and old single-topic-ID format.</p> Source code in <code>pspcz_analyzer/services/ollama_service.py</code> <pre><code>def deserialize_topics(raw: str) -&gt; list[str]:\n    \"\"\"Deserialize topic list from parquet storage.\n\n    Handles both new JSON format and old single-topic-ID format.\n    \"\"\"\n    if not raw:\n        return []\n    # Try JSON first (new format: '[\"topic1\", \"topic2\"]')\n    if raw.startswith(\"[\"):\n        try:\n            topics = json.loads(raw)\n            return [t for t in topics if isinstance(t, str) and t]\n        except (json.JSONDecodeError, TypeError):\n            logger.debug(\"Failed to parse topics JSON: {}\", raw)\n    # Old format: single topic ID like \"finance\" or \"justice\"\n    return [raw]\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/similarity_service/","title":"similarity_service","text":""},{"location":"reference/pspcz_analyzer/services/similarity_service/#pspcz_analyzer.services.similarity_service","title":"<code>similarity_service</code>","text":"<p>Voting similarity analysis with PCA clustering.</p>"},{"location":"reference/pspcz_analyzer/services/similarity_service/#pspcz_analyzer.services.similarity_service.compute_pca_coords","title":"<code>compute_pca_coords(data)</code>","text":"<p>Compute 2D PCA coordinates for each MP based on voting patterns.</p> <p>Returns list of dicts with: mp_name, party, x, y</p> Source code in <code>pspcz_analyzer/services/similarity_service.py</code> <pre><code>def compute_pca_coords(data: PeriodData) -&gt; list[dict]:\n    \"\"\"Compute 2D PCA coordinates for each MP based on voting patterns.\n\n    Returns list of dicts with: mp_name, party, x, y\n    \"\"\"\n    matrix, mp_info = _build_vote_matrix(data)\n\n    # Center the data\n    matrix_centered = matrix - matrix.mean(axis=0)\n\n    # SVD-based PCA (no sklearn needed)\n    U, S, Vt = np.linalg.svd(matrix_centered, full_matrices=False)\n    coords_2d = U[:, :2] * S[:2]\n\n    names = (mp_info.get_column(\"jmeno\") + \" \" + mp_info.get_column(\"prijmeni\")).to_list()\n    parties = mp_info.get_column(\"party\").to_list()\n\n    return [\n        {\n            \"mp_name\": names[i],\n            \"party\": parties[i] or \"N/A\",\n            \"x\": float(coords_2d[i, 0]),\n            \"y\": float(coords_2d[i, 1]),\n        }\n        for i in range(len(names))\n    ]\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/similarity_service/#pspcz_analyzer.services.similarity_service.compute_cross_party_similarity","title":"<code>compute_cross_party_similarity(data, top=20)</code>","text":"<p>Find the most similar cross-party MP pairs.</p> <p>Uses cosine similarity on the vote matrix.</p> Source code in <code>pspcz_analyzer/services/similarity_service.py</code> <pre><code>def compute_cross_party_similarity(data: PeriodData, top: int = 20) -&gt; list[dict]:\n    \"\"\"Find the most similar cross-party MP pairs.\n\n    Uses cosine similarity on the vote matrix.\n    \"\"\"\n    matrix, mp_info = _build_vote_matrix(data)\n\n    # Compute cosine similarity\n    norms = np.linalg.norm(matrix, axis=1, keepdims=True)\n    norms = np.where(norms == 0, 1, norms)  # avoid division by zero\n    normalized = matrix / norms\n    similarity = normalized @ normalized.T\n\n    names = (mp_info.get_column(\"jmeno\") + \" \" + mp_info.get_column(\"prijmeni\")).to_list()\n    parties = mp_info.get_column(\"party\").to_list()\n\n    # Find top cross-party pairs\n    n = len(names)\n    pairs = []\n    for i in range(n):\n        for j in range(i + 1, n):\n            if parties[i] and parties[j] and parties[i] != parties[j]:\n                pairs.append(\n                    {\n                        \"mp1_name\": names[i],\n                        \"mp1_party\": parties[i],\n                        \"mp2_name\": names[j],\n                        \"mp2_party\": parties[j],\n                        \"similarity\": float(similarity[i, j]),\n                    }\n                )\n\n    pairs.sort(key=lambda p: p[\"similarity\"], reverse=True)\n    return pairs[:top]\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/","title":"tisk_cache_manager","text":""},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager","title":"<code>tisk_cache_manager</code>","text":"<p>Cache manager for tisk metadata: topics, histories, law changes, versions.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager","title":"<code>TiskCacheManager(cache_dir)</code>","text":"<p>Loads and caches tisk metadata from the filesystem.</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def __init__(self, cache_dir: Path) -&gt; None:\n    self.cache_dir = cache_dir\n    # Topic classification cache: period -&gt; {ct -&gt; [topic_labels]}\n    self._topic_cache: dict[int, dict[int, list[str]]] = {}\n    # Summary cache: period -&gt; {ct -&gt; summary_text}\n    self._summary_cache: dict[int, dict[int, str]] = {}\n    # English summary cache: period -&gt; {ct -&gt; summary_en_text}\n    self._summary_en_cache: dict[int, dict[int, str]] = {}\n    # Track parquet mtime to detect incremental updates\n    self._topic_cache_mtime: dict[int, float] = {}\n    # Legislative history cache: period -&gt; {ct -&gt; TiskHistory}\n    self._history_cache: dict[int, dict] = {}\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager.invalidate","title":"<code>invalidate(period)</code>","text":"<p>Invalidate all caches for a period so next access re-reads from disk.</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def invalidate(self, period: int) -&gt; None:\n    \"\"\"Invalidate all caches for a period so next access re-reads from disk.\"\"\"\n    self._topic_cache.pop(period, None)\n    self._summary_cache.pop(period, None)\n    self._summary_en_cache.pop(period, None)\n    self._history_cache.pop(period, None)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager.load_topic_cache","title":"<code>load_topic_cache(period)</code>","text":"<p>Load topic classifications (and summaries) from parquet cache.</p> <p>Re-reads the parquet if it's been modified since last load (picks up incremental pipeline updates).</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def load_topic_cache(self, period: int) -&gt; dict[int, list[str]]:\n    \"\"\"Load topic classifications (and summaries) from parquet cache.\n\n    Re-reads the parquet if it's been modified since last load (picks up\n    incremental pipeline updates).\n    \"\"\"\n    meta_path = self.cache_dir / TISKY_META_DIR / str(period) / \"topic_classifications.parquet\"\n    if not meta_path.exists():\n        self._topic_cache[period] = {}\n        self._summary_cache[period] = {}\n        self._summary_en_cache[period] = {}\n        self._topic_cache_mtime[period] = 0\n        return {}\n\n    # Check if we need to re-read (new file or modified since last load)\n    current_mtime = meta_path.stat().st_mtime\n    cached_mtime = self._topic_cache_mtime.get(period, 0)\n    if period in self._topic_cache and current_mtime == cached_mtime:\n        return self._topic_cache[period]\n\n    df = pl.read_parquet(meta_path)\n    topics: dict[int, list[str]] = {}\n    summaries: dict[int, str] = {}\n    summaries_en: dict[int, str] = {}\n    for row in df.iter_rows(named=True):\n        ct = row[\"ct\"]\n        raw_topic = row.get(\"topic\", \"\")\n        parsed = deserialize_topics(raw_topic)\n        if parsed:\n            topics[ct] = parsed\n        summary = row.get(\"summary\", \"\")\n        if summary:\n            summaries[ct] = summary\n        summary_en = row.get(\"summary_en\", \"\")\n        if summary_en:\n            summaries_en[ct] = summary_en\n    self._topic_cache[period] = topics\n    self._summary_cache[period] = summaries\n    self._summary_en_cache[period] = summaries_en\n    self._topic_cache_mtime[period] = current_mtime\n    logger.debug(\n        \"Loaded topic classifications for period {}: {} tisky, {} summaries, {} EN summaries\",\n        period,\n        len(topics),\n        len(summaries),\n        len(summaries_en),\n    )\n    return topics\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager.load_history_cache","title":"<code>load_history_cache(period)</code>","text":"<p>Load legislative history JSON files for a period.</p> <p>Returns {ct: TiskHistory} dict. Caches in memory.</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def load_history_cache(self, period: int) -&gt; dict:\n    \"\"\"Load legislative history JSON files for a period.\n\n    Returns {ct: TiskHistory} dict. Caches in memory.\n    \"\"\"\n    if period in self._history_cache:\n        return self._history_cache[period]\n\n    hist_dir = self.cache_dir / TISKY_META_DIR / str(period) / TISKY_HISTORIE_DIR\n    histories: dict = {}\n    if not hist_dir.exists():\n        self._history_cache[period] = histories\n        return histories\n\n    for json_path in hist_dir.glob(\"*.json\"):\n        try:\n            ct = int(json_path.stem)\n        except ValueError:\n            continue\n        h = load_history_json(json_path)\n        if h:\n            histories[ct] = h\n\n    self._history_cache[period] = histories\n    if histories:\n        logger.debug(\n            \"Loaded {} tisk histories for period {}\",\n            len(histories),\n            period,\n        )\n    return histories\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager.load_law_changes_cache","title":"<code>load_law_changes_cache(period)</code>","text":"<p>Load law changes JSON files for a period.</p> <p>Always reads from disk (no in-memory cache) so incremental pipeline results are visible immediately in the UI. Returns {ct: [law_change_dicts]}.</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def load_law_changes_cache(self, period: int) -&gt; dict[int, list[dict]]:\n    \"\"\"Load law changes JSON files for a period.\n\n    Always reads from disk (no in-memory cache) so incremental pipeline\n    results are visible immediately in the UI.\n    Returns {ct: [law_change_dicts]}.\n    \"\"\"\n    lc_dir = self.cache_dir / TISKY_META_DIR / str(period) / TISKY_LAW_CHANGES_DIR\n    changes: dict[int, list[dict]] = {}\n    if not lc_dir.exists():\n        return changes\n\n    for json_path in lc_dir.glob(\"*.json\"):\n        try:\n            ct = int(json_path.stem)\n        except ValueError:\n            continue\n        try:\n            data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n            if data:  # only store non-empty\n                changes[ct] = data\n        except Exception:\n            logger.opt(exception=True).warning(\n                \"Failed to load law changes from {}\",\n                json_path,\n            )\n\n    return changes\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager.load_subtisk_versions_cache","title":"<code>load_subtisk_versions_cache(period)</code>","text":"<p>Load sub-tisk version info from JSON cache.</p> <p>Always reads from disk (no in-memory cache) so incremental pipeline results are visible immediately in the UI. Returns {ct: [version_dicts]}.</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def load_subtisk_versions_cache(self, period: int) -&gt; dict[int, list[dict]]:\n    \"\"\"Load sub-tisk version info from JSON cache.\n\n    Always reads from disk (no in-memory cache) so incremental pipeline\n    results are visible immediately in the UI.\n    Returns {ct: [version_dicts]}.\n    \"\"\"\n    scan_dir = self.cache_dir / TISKY_META_DIR / str(period) / \"subtisk_versions\"\n    versions: dict[int, list[dict]] = {}\n\n    if not scan_dir.exists():\n        return versions\n\n    for json_path in scan_dir.glob(\"*.json\"):\n        try:\n            ct = int(json_path.stem)\n        except ValueError:\n            continue\n        try:\n            data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n            if data:  # skip empty (means no sub-versions)\n                versions[ct] = data\n        except Exception:\n            logger.opt(exception=True).warning(\n                \"Failed to load subtisk cache from {}\",\n                json_path,\n            )\n\n    return versions\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_cache_manager/#pspcz_analyzer.services.tisk_cache_manager.TiskCacheManager.load_version_diffs_cache","title":"<code>load_version_diffs_cache(period)</code>","text":"<p>Load LLM version diff summaries for a period.</p> <p>Always reads from disk (no in-memory cache) so incremental pipeline results are visible immediately in the UI. Returns ({\"{ct}{ct1}\": summary_cs}, {\"{ct}).}\": summary_en</p> Source code in <code>pspcz_analyzer/services/tisk_cache_manager.py</code> <pre><code>def load_version_diffs_cache(self, period: int) -&gt; tuple[dict[str, str], dict[str, str]]:\n    \"\"\"Load LLM version diff summaries for a period.\n\n    Always reads from disk (no in-memory cache) so incremental pipeline\n    results are visible immediately in the UI.\n    Returns ({\"{ct}_{ct1}\": summary_cs}, {\"{ct}_{ct1}\": summary_en}).\n    \"\"\"\n    diff_dir = self.cache_dir / TISKY_META_DIR / str(period) / TISKY_VERSION_DIFFS_DIR\n    diffs: dict[str, str] = {}\n    diffs_en: dict[str, str] = {}\n    if not diff_dir.exists():\n        return diffs, diffs_en\n\n    for txt_path in diff_dir.glob(\"*.txt\"):\n        stem = txt_path.stem\n        # English diff files end with _en\n        if stem.endswith(\"_en\"):\n            key = stem[:-3]  # strip _en\n            try:\n                diffs_en[key] = txt_path.read_text(encoding=\"utf-8\")\n            except Exception:\n                logger.opt(exception=True).warning(\n                    \"Failed to load EN version diff from {}\",\n                    txt_path,\n                )\n        else:\n            try:\n                diffs[stem] = txt_path.read_text(encoding=\"utf-8\")\n            except Exception:\n                logger.opt(exception=True).warning(\n                    \"Failed to load version diff from {}\",\n                    txt_path,\n                )\n\n    return diffs, diffs_en\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_classifier/","title":"tisk_classifier","text":""},{"location":"reference/pspcz_analyzer/services/tisk_classifier/#pspcz_analyzer.services.tisk_classifier","title":"<code>tisk_classifier</code>","text":"<p>Topic classification and consolidation for parliamentary prints.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_classifier/#pspcz_analyzer.services.tisk_classifier.classify_and_save","title":"<code>classify_and_save(period, text_paths, cache_dir)</code>","text":"<p>Run topic classification on extracted texts, save parquet, return maps.</p> <p>Uses Ollama AI when available (free-form topics), falls back to keyword matching. Saves incrementally after each tisk and resumes from where it left off. Returns (topic_map, summary_map).</p> Source code in <code>pspcz_analyzer/services/tisk_classifier.py</code> <pre><code>def classify_and_save(\n    period: int,\n    text_paths: dict[int, Path],\n    cache_dir: Path,\n) -&gt; tuple[dict[int, list[str]], dict[int, str]]:\n    \"\"\"Run topic classification on extracted texts, save parquet, return maps.\n\n    Uses Ollama AI when available (free-form topics), falls back to keyword matching.\n    Saves incrementally after each tisk and resumes from where it left off.\n    Returns (topic_map, summary_map).\n    \"\"\"\n    meta_dir = cache_dir / TISKY_META_DIR / str(period)\n    meta_dir.mkdir(parents=True, exist_ok=True)\n    parquet_path = meta_dir / \"topic_classifications.parquet\"\n\n    # Load existing records to resume from (if any)\n    existing: dict[int, dict] = {}\n    if parquet_path.exists():\n        df = pl.read_parquet(parquet_path)\n        for row in df.iter_rows(named=True):\n            existing[row[\"ct\"]] = row\n\n    # Figure out which tisky still need processing\n    remaining = {ct: p for ct, p in text_paths.items() if ct not in existing}\n\n    ollama = OllamaClient()\n    use_ai = ollama.is_available()\n    total = len(text_paths)\n    already = len(existing)\n\n    if already:\n        logger.info(\n            \"[tisk pipeline] Resuming: {} already done, {} remaining out of {} total\",\n            already,\n            len(remaining),\n            total,\n        )\n\n    if use_ai:\n        logger.info(\n            \"[tisk pipeline] Ollama available, using AI classification + summarization ({} to process)\",\n            len(remaining),\n        )\n    else:\n        logger.info(\n            \"[tisk pipeline] Ollama not available, using keyword classification ({} to process)\",\n            len(remaining),\n        )\n\n    # Start from existing records\n    records = list(existing.values())\n\n    for i, (ct, text_path) in enumerate(sorted(remaining.items()), already + 1):\n        record = _classify_single_tisk(ct, text_path, ollama, use_ai, i, total)\n        records.append(record)\n\n        # Save after every tisk so progress is never lost\n        df = pl.DataFrame(records)\n        df.write_parquet(parquet_path)\n\n    # Build return maps from all records (existing + new)\n    return _build_topic_summary_maps(records, period)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_classifier/#pspcz_analyzer.services.tisk_classifier.consolidate_topics","title":"<code>consolidate_topics(period, cache_dir)</code>","text":"<p>Run LLM-powered topic deduplication after classification.</p> <p>Reads the parquet, collects all unique topic labels, asks the LLM to consolidate similar ones, applies the mapping, and re-writes the parquet.</p> <p>Returns updated (topic_map, summary_map).</p> Source code in <code>pspcz_analyzer/services/tisk_classifier.py</code> <pre><code>def consolidate_topics(\n    period: int,\n    cache_dir: Path,\n) -&gt; tuple[dict[int, list[str]], dict[int, str]]:\n    \"\"\"Run LLM-powered topic deduplication after classification.\n\n    Reads the parquet, collects all unique topic labels, asks the LLM to\n    consolidate similar ones, applies the mapping, and re-writes the parquet.\n\n    Returns updated (topic_map, summary_map).\n    \"\"\"\n    meta_dir = cache_dir / TISKY_META_DIR / str(period)\n    parquet_path = meta_dir / \"topic_classifications.parquet\"\n    consolidated_marker = meta_dir / \"topics_consolidated.done\"\n\n    if not parquet_path.exists():\n        logger.warning(\"[tisk pipeline] No parquet to consolidate for period {}\", period)\n        return {}, {}\n\n    df = pl.read_parquet(parquet_path)\n    records = df.to_dicts()\n\n    # If consolidation was already done, just return the maps from the parquet\n    if consolidated_marker.exists():\n        logger.info(\n            \"[tisk pipeline] Topics already consolidated for period {}, skipping\",\n            period,\n        )\n        return _build_topic_summary_maps(records, period, log=False)\n\n    # Collect all unique topic labels\n    all_topics: set[str] = set()\n    for r in records:\n        for t in deserialize_topics(r.get(\"topic\", \"\")):\n            all_topics.add(t)\n\n    unique_topics = sorted(all_topics)\n\n    if len(unique_topics) &lt;= 10:\n        logger.info(\n            \"[tisk pipeline] Only {} unique topics for period {}, skipping consolidation\",\n            len(unique_topics),\n            period,\n        )\n        consolidated_marker.touch()\n        return _build_topic_summary_maps(records, period, log=False)\n\n    ollama = OllamaClient()\n    if not ollama.is_available():\n        logger.info(\"[tisk pipeline] Ollama not available, skipping topic consolidation\")\n        return _build_topic_summary_maps(records, period, log=False)\n\n    logger.info(\n        \"[tisk pipeline] Consolidating topics for period {}: {} unique topics\",\n        period,\n        len(unique_topics),\n    )\n    mapping = ollama.consolidate_topics(unique_topics)\n\n    # Count how many actually changed\n    changed = sum(1 for old, new in mapping.items() if old != new)\n    canonical = len(set(mapping.values()))\n    logger.info(\n        \"[tisk pipeline] Consolidating topics for period {}: {} unique -&gt; {} canonical ({} remapped)\",\n        period,\n        len(unique_topics),\n        canonical,\n        changed,\n    )\n\n    # Apply mapping to all records\n    for r in records:\n        old_topics = deserialize_topics(r.get(\"topic\", \"\"))\n        new_topics = [mapping.get(t, t) for t in old_topics]\n        # Deduplicate while preserving order\n        seen: set[str] = set()\n        deduped: list[str] = []\n        for t in new_topics:\n            if t not in seen:\n                seen.add(t)\n                deduped.append(t)\n        r[\"topic\"] = serialize_topics(deduped)\n\n    # Re-write parquet\n    df = pl.DataFrame(records)\n    df.write_parquet(parquet_path)\n\n    # Write marker so we don't re-consolidate on next startup\n    consolidated_marker.touch()\n\n    return _build_topic_summary_maps(records, period)\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_downloader_pipeline/","title":"tisk_downloader_pipeline","text":""},{"location":"reference/pspcz_analyzer/services/tisk_downloader_pipeline/#pspcz_analyzer.services.tisk_downloader_pipeline","title":"<code>tisk_downloader_pipeline</code>","text":"<p>Download tisk PDFs and extract text from them.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_downloader_pipeline/#pspcz_analyzer.services.tisk_downloader_pipeline.download_one","title":"<code>download_one(period, ct, idd, cache_dir, force)</code>","text":"<p>Download a single PDF by its idd. Returns path or None.</p> Source code in <code>pspcz_analyzer/services/tisk_downloader_pipeline.py</code> <pre><code>def download_one(period: int, ct: int, idd: int, cache_dir: Path, force: bool) -&gt; Path | None:\n    \"\"\"Download a single PDF by its idd. Returns path or None.\"\"\"\n    pdf_dir = cache_dir / TISKY_PDF_DIR / str(period)\n    pdf_dir.mkdir(parents=True, exist_ok=True)\n    dest = pdf_dir / f\"{ct}.pdf\"\n\n    if dest.exists() and not force:\n        return dest\n\n    url = f\"{PSP_ORIG2_BASE_URL}?idd={idd}\"\n    try:\n        with httpx.Client(timeout=60, follow_redirects=True) as client:\n            with client.stream(\"GET\", url) as response:\n                response.raise_for_status()\n                with open(dest, \"wb\") as f:\n                    for chunk in response.iter_bytes(chunk_size=65536):\n                        f.write(chunk)\n        return dest\n    except Exception:\n        logger.opt(exception=True).warning(\"Failed to download tisk {}/{}\", period, ct)\n        dest.unlink(missing_ok=True)\n        return None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_downloader_pipeline/#pspcz_analyzer.services.tisk_downloader_pipeline.extract_one","title":"<code>extract_one(pdf_path, period, ct, cache_dir, force)</code>","text":"<p>Extract text from a single PDF. Returns text path or None.</p> Source code in <code>pspcz_analyzer/services/tisk_downloader_pipeline.py</code> <pre><code>def extract_one(pdf_path: Path, period: int, ct: int, cache_dir: Path, force: bool) -&gt; Path | None:\n    \"\"\"Extract text from a single PDF. Returns text path or None.\"\"\"\n    text_dir = cache_dir / TISKY_TEXT_DIR / str(period)\n    text_dir.mkdir(parents=True, exist_ok=True)\n    dest = text_dir / f\"{ct}.txt\"\n\n    if dest.exists() and not force:\n        return dest\n\n    try:\n        doc = pymupdf.open(pdf_path)\n        pages = [str(page.get_text()) for page in doc]\n        doc.close()\n        text = \"\\n\\n\".join(pages)\n    except Exception:\n        logger.opt(exception=True).warning(\"Failed to extract text from {}\", pdf_path.name)\n        return None\n\n    if not text.strip():\n        return None\n\n    dest.write_text(text, encoding=\"utf-8\")\n    return dest\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_downloader_pipeline/#pspcz_analyzer.services.tisk_downloader_pipeline.process_period_sync","title":"<code>process_period_sync(period, ct_numbers, cache_dir, force=False)</code>","text":"<p>Synchronous pipeline: scrape -&gt; download -&gt; extract for all ct numbers.</p> <p>Returns (pdf_paths, text_paths).</p> Source code in <code>pspcz_analyzer/services/tisk_downloader_pipeline.py</code> <pre><code>def process_period_sync(\n    period: int,\n    ct_numbers: list[int],\n    cache_dir: Path,\n    force: bool = False,\n) -&gt; tuple[dict[int, Path], dict[int, Path]]:\n    \"\"\"Synchronous pipeline: scrape -&gt; download -&gt; extract for all ct numbers.\n\n    Returns (pdf_paths, text_paths).\n    \"\"\"\n    pdf_paths: dict[int, Path] = {}\n    text_paths: dict[int, Path] = {}\n    total = len(ct_numbers)\n\n    for i, ct in enumerate(ct_numbers, 1):\n        # Check caches first (fast path \u2014 no HTTP needed)\n        pdf_dir = cache_dir / TISKY_PDF_DIR / str(period)\n        text_dir = cache_dir / TISKY_TEXT_DIR / str(period)\n        pdf_cached = pdf_dir / f\"{ct}.pdf\"\n        text_cached = text_dir / f\"{ct}.txt\"\n\n        if text_cached.exists() and not force:\n            text_paths[ct] = text_cached\n            if pdf_cached.exists():\n                pdf_paths[ct] = pdf_cached\n            continue\n\n        if pdf_cached.exists() and not force:\n            pdf_paths[ct] = pdf_cached\n            # Just need extraction\n            txt = extract_one(pdf_cached, period, ct, cache_dir, force)\n            if txt:\n                text_paths[ct] = txt\n            continue\n\n        # Need to scrape + download\n        if i % 50 == 0 or i == 1:\n            logger.info(\"[tisk pipeline] Period {}: processing {}/{}\", period, i, total)\n\n        doc = get_best_pdf(period, ct)\n        if doc is None:\n            time.sleep(PSP_REQUEST_DELAY)\n            continue\n\n        pdf = download_one(period, ct, doc.idd, cache_dir, force)\n        time.sleep(PSP_REQUEST_DELAY)\n\n        if pdf is None:\n            continue\n        pdf_paths[ct] = pdf\n\n        txt = extract_one(pdf, period, ct, cache_dir, force)\n        if txt:\n            text_paths[ct] = txt\n\n    return pdf_paths, text_paths\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_lookup_builder/","title":"tisk_lookup_builder","text":""},{"location":"reference/pspcz_analyzer/services/tisk_lookup_builder/#pspcz_analyzer.services.tisk_lookup_builder","title":"<code>tisk_lookup_builder</code>","text":"<p>Build tisk lookup tables mapping (schuze, bod) -&gt; TiskInfo.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_lookup_builder/#pspcz_analyzer.services.tisk_lookup_builder.build_tisk_lookup","title":"<code>build_tisk_lookup(period, votes, schuze, bod_schuze, tisky, tisk_text, topic_cache, summary_cache, summary_en_cache=None)</code>","text":"<p>Build a mapping from (schuze_num, bod_num) -&gt; TiskInfo for a given period.</p> <p>Primary path: schuze -&gt; bod_schuze -&gt; tisky (reliable, full coverage). Fallback: if schuze data is missing for this period, match vote descriptions directly to tisk names (covers new periods where schuze.zip hasn't been updated yet).</p> Source code in <code>pspcz_analyzer/services/tisk_lookup_builder.py</code> <pre><code>def build_tisk_lookup(\n    period: int,\n    votes: pl.DataFrame,\n    schuze: pl.DataFrame,\n    bod_schuze: pl.DataFrame,\n    tisky: pl.DataFrame,\n    tisk_text: TiskTextService,\n    topic_cache: dict[int, dict[int, list[str]]],\n    summary_cache: dict[int, dict[int, str]],\n    summary_en_cache: dict[int, dict[int, str]] | None = None,\n) -&gt; dict[tuple[int, int], TiskInfo]:\n    \"\"\"Build a mapping from (schuze_num, bod_num) -&gt; TiskInfo for a given period.\n\n    Primary path: schuze -&gt; bod_schuze -&gt; tisky (reliable, full coverage).\n    Fallback: if schuze data is missing for this period, match vote\n    descriptions directly to tisk names (covers new periods where\n    schuze.zip hasn't been updated yet).\n    \"\"\"\n    organ_id = PERIOD_ORGAN_IDS[period]\n    en_cache = summary_en_cache or {}\n\n    # Try primary path via schuze -&gt; bod_schuze\n    sessions = schuze.filter(pl.col(\"id_org\") == organ_id)\n    if sessions.height &gt; 0:\n        return build_tisk_lookup_via_schuze(\n            period, sessions, bod_schuze, tisky, tisk_text, topic_cache, summary_cache, en_cache\n        )\n\n    # Fallback: text matching for periods without schuze data\n    logger.info(\n        \"No session data for period {} (organ {}), using text-match fallback\",\n        period,\n        organ_id,\n    )\n    return build_tisk_lookup_via_text(\n        period, votes, tisky, tisk_text, topic_cache, summary_cache, en_cache\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_lookup_builder/#pspcz_analyzer.services.tisk_lookup_builder.build_tisk_lookup_via_schuze","title":"<code>build_tisk_lookup_via_schuze(period, sessions, bod_schuze, tisky, tisk_text, topic_cache, summary_cache, summary_en_cache=None)</code>","text":"<p>Build lookup using the schuze -&gt; bod_schuze -&gt; tisky chain.</p> Source code in <code>pspcz_analyzer/services/tisk_lookup_builder.py</code> <pre><code>def build_tisk_lookup_via_schuze(\n    period: int,\n    sessions: pl.DataFrame,\n    bod_schuze: pl.DataFrame,\n    tisky: pl.DataFrame,\n    tisk_text: TiskTextService,\n    topic_cache: dict[int, dict[int, list[str]]],\n    summary_cache: dict[int, dict[int, str]],\n    summary_en_cache: dict[int, dict[int, str]] | None = None,\n) -&gt; dict[tuple[int, int], TiskInfo]:\n    \"\"\"Build lookup using the schuze -&gt; bod_schuze -&gt; tisky chain.\"\"\"\n    session_map = dict(\n        zip(\n            sessions.get_column(\"id_schuze\").to_list(),\n            sessions.get_column(\"schuze\").to_list(),\n            strict=False,\n        )\n    )\n    session_ids = set(session_map.keys())\n\n    bods = bod_schuze.filter(\n        pl.col(\"id_schuze\").is_in(session_ids)\n        &amp; pl.col(\"id_tisk\").is_not_null()\n        &amp; (pl.col(\"id_tisk\") != 0)\n    )\n\n    if bods.height == 0:\n        return {}\n\n    # Load topic classifications, summaries, and text availability\n    topic_map = topic_cache.get(period, {})\n    summary_map = summary_cache.get(period, {})\n    summary_en_map = (summary_en_cache or {}).get(period, {})\n\n    tisk_ids = set(bods.get_column(\"id_tisk\").to_list())\n    relevant_tisky = tisky.filter(pl.col(\"id_tisk\").is_in(tisk_ids))\n    tisk_map = {}\n    for row in relevant_tisky.iter_rows(named=True):\n        ct = row.get(\"ct\")\n        if ct:\n            tisk_map[row[\"id_tisk\"]] = TiskInfo(\n                id_tisk=row[\"id_tisk\"],\n                ct=ct,\n                nazev=row.get(\"nazev_tisku\") or \"\",\n                period=period,\n                topics=topic_map.get(ct, []),\n                has_text=tisk_text.has_text(period, ct),\n                summary=summary_map.get(ct, \"\"),\n                summary_en=summary_en_map.get(ct, \"\"),\n            )\n\n    lookup: dict[tuple[int, int], TiskInfo] = {}\n    for row in bods.iter_rows(named=True):\n        id_schuze = row[\"id_schuze\"]\n        schuze_num = session_map.get(id_schuze)\n        bod_num = row.get(\"bod\")\n        id_tisk = row[\"id_tisk\"]\n        if schuze_num is not None and bod_num is not None and id_tisk in tisk_map:\n            lookup[(schuze_num, bod_num)] = tisk_map[id_tisk]\n\n    logger.info(\n        \"Period {}: built tisk lookup with {} entries (via schuze)\",\n        period,\n        len(lookup),\n    )\n    return lookup\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_lookup_builder/#pspcz_analyzer.services.tisk_lookup_builder.build_tisk_lookup_via_text","title":"<code>build_tisk_lookup_via_text(period, votes, tisky, tisk_text, topic_cache, summary_cache, summary_en_cache=None)</code>","text":"<p>Fallback: match vote descriptions to tisk names for this period.</p> <p>Used when schuze.zip hasn't been updated for a new period yet.</p> Source code in <code>pspcz_analyzer/services/tisk_lookup_builder.py</code> <pre><code>def build_tisk_lookup_via_text(\n    period: int,\n    votes: pl.DataFrame,\n    tisky: pl.DataFrame,\n    tisk_text: TiskTextService,\n    topic_cache: dict[int, dict[int, list[str]]],\n    summary_cache: dict[int, dict[int, str]],\n    summary_en_cache: dict[int, dict[int, str]] | None = None,\n) -&gt; dict[tuple[int, int], TiskInfo]:\n    \"\"\"Fallback: match vote descriptions to tisk names for this period.\n\n    Used when schuze.zip hasn't been updated for a new period yet.\n    \"\"\"\n    organ_id = PERIOD_ORGAN_IDS[period]\n    period_tisky = tisky.filter(pl.col(\"id_obdobi\") == organ_id)\n    if period_tisky.height == 0:\n        return {}\n\n    # Load topic classifications, summaries, and text availability\n    topic_map = topic_cache.get(period, {})\n    summary_map = summary_cache.get(period, {})\n    summary_en_map = (summary_en_cache or {}).get(period, {})\n\n    # Build list of tisk names for matching (longest first for greedy match)\n    tisk_entries = []\n    for row in period_tisky.iter_rows(named=True):\n        ct = row.get(\"ct\")\n        nazev = (row.get(\"nazev_tisku\") or \"\").strip()\n        if ct and nazev:\n            tisk_entries.append(\n                TiskInfo(\n                    id_tisk=row[\"id_tisk\"],\n                    ct=ct,\n                    nazev=nazev,\n                    period=period,\n                    topics=topic_map.get(ct, []),\n                    has_text=tisk_text.has_text(period, ct),\n                    summary=summary_map.get(ct, \"\"),\n                    summary_en=summary_en_map.get(ct, \"\"),\n                )\n            )\n    tisk_entries.sort(key=lambda t: len(t.nazev), reverse=True)\n\n    # Get unique (schuze, bod) combinations with descriptions\n    vote_bods = (\n        votes.filter(pl.col(\"nazev_dlouhy\").is_not_null() &amp; (pl.col(\"bod\") &gt; 0))\n        .select(\"schuze\", \"bod\", \"nazev_dlouhy\")\n        .unique(subset=[\"schuze\", \"bod\"])\n    )\n\n    lookup: dict[tuple[int, int], TiskInfo] = {}\n    for row in vote_bods.iter_rows(named=True):\n        desc = (row[\"nazev_dlouhy\"] or \"\").strip()\n        if not desc:\n            continue\n        for tisk in tisk_entries:\n            if desc.startswith(tisk.nazev) or tisk.nazev.startswith(desc):\n                lookup[(row[\"schuze\"], row[\"bod\"])] = tisk\n                break\n\n    logger.info(\n        \"Period {}: built tisk lookup with {} entries (via text match, {} tisky available)\",\n        period,\n        len(lookup),\n        len(tisk_entries),\n    )\n    return lookup\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_metadata_scraper/","title":"tisk_metadata_scraper","text":""},{"location":"reference/pspcz_analyzer/services/tisk_metadata_scraper/#pspcz_analyzer.services.tisk_metadata_scraper","title":"<code>tisk_metadata_scraper</code>","text":"<p>Scrape legislative history and law changes from psp.cz.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_metadata_scraper/#pspcz_analyzer.services.tisk_metadata_scraper.scrape_histories_sync","title":"<code>scrape_histories_sync(period, ct_numbers, cache_dir)</code>","text":"<p>Scrape legislative history pages for all tisky in a period.</p> <p>Caches results as JSON files. Skips already-cached tisky. Returns {ct: TiskHistory} dict.</p> Source code in <code>pspcz_analyzer/services/tisk_metadata_scraper.py</code> <pre><code>def scrape_histories_sync(\n    period: int,\n    ct_numbers: list[int],\n    cache_dir: Path,\n) -&gt; dict:\n    \"\"\"Scrape legislative history pages for all tisky in a period.\n\n    Caches results as JSON files. Skips already-cached tisky.\n    Returns {ct: TiskHistory} dict.\n    \"\"\"\n    hist_dir = cache_dir / TISKY_META_DIR / str(period) / TISKY_HISTORIE_DIR\n    hist_dir.mkdir(parents=True, exist_ok=True)\n\n    histories: dict[int, TiskHistory] = {}\n    total = len(ct_numbers)\n    scraped = 0\n\n    for i, ct in enumerate(ct_numbers, 1):\n        json_path = hist_dir / f\"{ct}.json\"\n\n        # Load from cache if available\n        if json_path.exists():\n            h = load_history_json(json_path)\n            if h:\n                histories[ct] = h\n            continue\n\n        # Scrape from psp.cz\n        if i % 50 == 0 or i == 1:\n            logger.info(\n                \"[tisk pipeline] Scraping history for period {}: {}/{}\",\n                period,\n                i,\n                total,\n            )\n\n        h = scrape_tisk_history(period, ct)\n        if h:\n            save_history_json(h, json_path)\n            histories[ct] = h\n            scraped += 1\n\n        time.sleep(PSP_REQUEST_DELAY)\n\n    logger.info(\n        \"[tisk pipeline] History scraping for period {}: {} cached, {} new, {} total\",\n        period,\n        len(histories) - scraped,\n        scraped,\n        len(histories),\n    )\n    return histories\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_metadata_scraper/#pspcz_analyzer.services.tisk_metadata_scraper.scrape_law_changes_sync","title":"<code>scrape_law_changes_sync(period, ct_numbers, cache_dir)</code>","text":"<p>Scrape law change pages (snzp=1) for all tisky in a period.</p> <p>Caches results as JSON. Returns {ct: [law_change_dicts]}.</p> Source code in <code>pspcz_analyzer/services/tisk_metadata_scraper.py</code> <pre><code>def scrape_law_changes_sync(\n    period: int,\n    ct_numbers: list[int],\n    cache_dir: Path,\n) -&gt; dict[int, list[dict]]:\n    \"\"\"Scrape law change pages (snzp=1) for all tisky in a period.\n\n    Caches results as JSON. Returns {ct: [law_change_dicts]}.\n    \"\"\"\n    law_changes_dir = cache_dir / TISKY_META_DIR / str(period) / TISKY_LAW_CHANGES_DIR\n    law_changes_dir.mkdir(parents=True, exist_ok=True)\n\n    result: dict[int, list[dict]] = {}\n    total = len(ct_numbers)\n    scraped = 0\n\n    for i, ct in enumerate(ct_numbers, 1):\n        # Load from cache\n        cached = load_law_changes_json(period, ct, cache_dir)\n        if cached is not None:\n            result[ct] = [asdict(c) for c in cached]\n            continue\n\n        if i % 50 == 0 or i == 1:\n            logger.info(\n                \"[tisk pipeline] Scraping law changes for period {}: {}/{}\",\n                period,\n                i,\n                total,\n            )\n\n        changes = scrape_proposed_law_changes(period, ct)\n        save_law_changes_json(changes, period, ct, cache_dir)\n        if changes:\n            result[ct] = [asdict(c) for c in changes]\n        scraped += 1\n\n        time.sleep(PSP_REQUEST_DELAY)\n\n    logger.info(\n        \"[tisk pipeline] Law changes for period {}: {} cached, {} new, {} with changes\",\n        period,\n        len(result) - scraped,\n        scraped,\n        len(result),\n    )\n    return result\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_pipeline_service/","title":"tisk_pipeline_service","text":""},{"location":"reference/pspcz_analyzer/services/tisk_pipeline_service/#pspcz_analyzer.services.tisk_pipeline_service","title":"<code>tisk_pipeline_service</code>","text":"<p>Background pipeline orchestrator: coordinates tisk processing stages.</p> <p>Runs as an asyncio background task so the web server stays responsive.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_pipeline_service/#pspcz_analyzer.services.tisk_pipeline_service.TiskPipelineService","title":"<code>TiskPipelineService(cache_dir=DEFAULT_CACHE_DIR)</code>","text":"<p>Manages background tisk processing for loaded periods.</p> Source code in <code>pspcz_analyzer/services/tisk_pipeline_service.py</code> <pre><code>def __init__(self, cache_dir: Path = DEFAULT_CACHE_DIR) -&gt; None:\n    self.cache_dir = cache_dir\n    self._tasks: dict[int, asyncio.Task] = {}\n    self._all_task: asyncio.Task | None = None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_pipeline_service/#pspcz_analyzer.services.tisk_pipeline_service.TiskPipelineService.start_period","title":"<code>start_period(period, ct_numbers, on_complete=None)</code>","text":"<p>Start background processing for a period. Idempotent \u2014 skips if already running.</p> Source code in <code>pspcz_analyzer/services/tisk_pipeline_service.py</code> <pre><code>def start_period(\n    self,\n    period: int,\n    ct_numbers: list[int],\n    on_complete=None,\n) -&gt; None:\n    \"\"\"Start background processing for a period. Idempotent \u2014 skips if already running.\"\"\"\n    if period in self._tasks and not self._tasks[period].done():\n        logger.debug(\"Tisk pipeline already running for period {}\", period)\n        return\n\n    task = asyncio.create_task(\n        self._run_period(period, ct_numbers, on_complete),\n        name=f\"tisk-pipeline-{period}\",\n    )\n    self._tasks[period] = task\n    logger.info(\n        \"[tisk pipeline] Started background processing for period {} ({} tisky)\",\n        period,\n        len(ct_numbers),\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_pipeline_service/#pspcz_analyzer.services.tisk_pipeline_service.TiskPipelineService.start_all_periods","title":"<code>start_all_periods(period_ct_numbers, on_complete=None)</code>","text":"<p>Process all periods sequentially in one background task (newest first).</p> <p>period_ct_numbers: list of (period, ct_numbers) tuples, ordered by priority.</p> Source code in <code>pspcz_analyzer/services/tisk_pipeline_service.py</code> <pre><code>def start_all_periods(\n    self,\n    period_ct_numbers: list[tuple[int, list[int]]],\n    on_complete=None,\n) -&gt; None:\n    \"\"\"Process all periods sequentially in one background task (newest first).\n\n    period_ct_numbers: list of (period, ct_numbers) tuples, ordered by priority.\n    \"\"\"\n    if self._all_task is not None and not self._all_task.done():\n        logger.debug(\"All-periods pipeline already running\")\n        return\n\n    self._all_task = asyncio.create_task(\n        self._run_all_periods(period_ct_numbers, on_complete),\n        name=\"tisk-pipeline-all\",\n    )\n    total_tisky = sum(len(cts) for _, cts in period_ct_numbers)\n    logger.info(\n        \"[tisk pipeline] Started sequential processing of {} periods ({} tisky total)\",\n        len(period_ct_numbers),\n        total_tisky,\n    )\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_pipeline_service/#pspcz_analyzer.services.tisk_pipeline_service.TiskPipelineService.cancel_all","title":"<code>cancel_all()</code>  <code>async</code>","text":"<p>Cancel all running pipeline tasks and wait for them to finish.</p> Source code in <code>pspcz_analyzer/services/tisk_pipeline_service.py</code> <pre><code>async def cancel_all(self) -&gt; None:\n    \"\"\"Cancel all running pipeline tasks and wait for them to finish.\"\"\"\n    tasks_to_cancel: list[asyncio.Task] = []\n\n    if self._all_task is not None and not self._all_task.done():\n        self._all_task.cancel()\n        tasks_to_cancel.append(self._all_task)\n\n    for task in self._tasks.values():\n        if not task.done():\n            task.cancel()\n            tasks_to_cancel.append(task)\n\n    if tasks_to_cancel:\n        logger.info(\"[tisk pipeline] Cancelling {} tasks ...\", len(tasks_to_cancel))\n        await asyncio.gather(*tasks_to_cancel, return_exceptions=True)\n        logger.info(\"[tisk pipeline] All tasks cancelled\")\n\n    self._tasks.clear()\n    self._all_task = None\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_text_service/","title":"tisk_text_service","text":""},{"location":"reference/pspcz_analyzer/services/tisk_text_service/#pspcz_analyzer.services.tisk_text_service","title":"<code>tisk_text_service</code>","text":"<p>Service for querying cached tisk text files.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_text_service/#pspcz_analyzer.services.tisk_text_service.TiskTextService","title":"<code>TiskTextService(cache_dir=DEFAULT_CACHE_DIR)</code>","text":"<p>Lightweight service to query cached extracted text from tisk PDFs.</p> Source code in <code>pspcz_analyzer/services/tisk_text_service.py</code> <pre><code>def __init__(self, cache_dir: Path = DEFAULT_CACHE_DIR) -&gt; None:\n    self.cache_dir = cache_dir\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_text_service/#pspcz_analyzer.services.tisk_text_service.TiskTextService.get_text","title":"<code>get_text(period, ct)</code>","text":"<p>Read cached text for a tisk, or None if not available.</p> Source code in <code>pspcz_analyzer/services/tisk_text_service.py</code> <pre><code>def get_text(self, period: int, ct: int) -&gt; str | None:\n    \"\"\"Read cached text for a tisk, or None if not available.\"\"\"\n    path = self._text_dir(period) / f\"{ct}.txt\"\n    if not path.exists():\n        return None\n    return path.read_text(encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_text_service/#pspcz_analyzer.services.tisk_text_service.TiskTextService.has_text","title":"<code>has_text(period, ct)</code>","text":"<p>Check whether extracted text exists for a tisk.</p> Source code in <code>pspcz_analyzer/services/tisk_text_service.py</code> <pre><code>def has_text(self, period: int, ct: int) -&gt; bool:\n    \"\"\"Check whether extracted text exists for a tisk.\"\"\"\n    return (self._text_dir(period) / f\"{ct}.txt\").exists()\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_text_service/#pspcz_analyzer.services.tisk_text_service.TiskTextService.available_tisky","title":"<code>available_tisky(period)</code>","text":"<p>List all ct numbers that have extracted text for a period.</p> Source code in <code>pspcz_analyzer/services/tisk_text_service.py</code> <pre><code>def available_tisky(self, period: int) -&gt; list[int]:\n    \"\"\"List all ct numbers that have extracted text for a period.\"\"\"\n    text_dir = self._text_dir(period)\n    if not text_dir.exists():\n        return []\n    return sorted(int(p.stem) for p in text_dir.glob(\"*.txt\") if p.stem.isdigit())\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_version_service/","title":"tisk_version_service","text":""},{"location":"reference/pspcz_analyzer/services/tisk_version_service/#pspcz_analyzer.services.tisk_version_service","title":"<code>tisk_version_service</code>","text":"<p>Sub-tisk version downloading and LLM diff analysis.</p>"},{"location":"reference/pspcz_analyzer/services/tisk_version_service/#pspcz_analyzer.services.tisk_version_service.download_subtisk_versions_sync","title":"<code>download_subtisk_versions_sync(period, ct_numbers, cache_dir)</code>","text":"<p>Download all sub-tisk versions (CT1=0..N) for tisky in a period.</p> <p>Caches scan results as JSON per-ct so restarts skip already-processed tisky. Returns {ct: [SubTiskVersion dicts]}.</p> Source code in <code>pspcz_analyzer/services/tisk_version_service.py</code> <pre><code>def download_subtisk_versions_sync(\n    period: int,\n    ct_numbers: list[int],\n    cache_dir: Path,\n) -&gt; dict[int, list[dict]]:\n    \"\"\"Download all sub-tisk versions (CT1=0..N) for tisky in a period.\n\n    Caches scan results as JSON per-ct so restarts skip already-processed tisky.\n    Returns {ct: [SubTiskVersion dicts]}.\n    \"\"\"\n    # JSON cache dir for sub-tisk scan results\n    scan_dir = cache_dir / TISKY_META_DIR / str(period) / \"subtisk_versions\"\n    scan_dir.mkdir(parents=True, exist_ok=True)\n\n    result: dict[int, list[dict]] = {}\n    total = len(ct_numbers)\n    scraped = 0\n\n    for i, ct in enumerate(ct_numbers, 1):\n        scan_cache = scan_dir / f\"{ct}.json\"\n\n        # Load from JSON cache if available\n        if scan_cache.exists():\n            try:\n                data = json.loads(scan_cache.read_text(encoding=\"utf-8\"))\n                if data:  # non-empty means this ct has sub-versions\n                    result[ct] = data\n                continue\n            except Exception:\n                logger.debug(\"Bad cache for ct={}, re-scraping\", ct)\n\n        if i % 50 == 0 or i == 1:\n            logger.info(\n                \"[tisk pipeline] Scraping sub-tisk versions for period {}: {}/{}\",\n                period,\n                i,\n                total,\n            )\n\n        # Scrape sub-tisk pages to find versions\n        versions_data = scrape_all_subtisk_documents(period, ct)\n        scraped += 1\n\n        if len(versions_data) &lt;= 1:\n            # Only CT1=0 or nothing \u2014 save empty list to cache so we don't re-scrape\n            scan_cache.write_text(\"[]\", encoding=\"utf-8\")\n            time.sleep(PSP_REQUEST_DELAY)\n            continue\n\n        text_dir = cache_dir / TISKY_TEXT_DIR / str(period)\n        version_dicts = []\n\n        for v in versions_data:\n            if v.idd and v.ct1 &gt; 0:  # CT1=0 is already downloaded by main pipeline\n                pdf = download_subtisk_pdf(period, ct, v.ct1, v.idd, cache_dir)\n                time.sleep(PSP_REQUEST_DELAY)\n\n                # Extract text if PDF downloaded\n                if pdf:\n                    _extract_subtisk_text(pdf, text_dir, ct, v)\n\n            vd = asdict(v)\n            version_dicts.append(vd)\n\n        # Save scan result to cache (even if version_dicts is just CT1=0)\n        scan_cache.write_text(\n            json.dumps(version_dicts, ensure_ascii=False, indent=2),\n            encoding=\"utf-8\",\n        )\n        if version_dicts:\n            result[ct] = version_dicts\n\n    logger.info(\n        \"[tisk pipeline] Sub-tisk versions for period {}: {} cached, {} new, {} with multiple versions\",\n        period,\n        total - scraped,\n        scraped,\n        len(result),\n    )\n    return result\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/tisk_version_service/#pspcz_analyzer.services.tisk_version_service.analyze_version_diffs_sync","title":"<code>analyze_version_diffs_sync(period, ct_numbers, cache_dir)</code>","text":"<p>Run LLM comparison on consecutive sub-tisk versions.</p> <p>Returns ({\"{ct}{ct1}\": diff_cs}, {\"{ct}).}\": diff_en</p> Source code in <code>pspcz_analyzer/services/tisk_version_service.py</code> <pre><code>def analyze_version_diffs_sync(\n    period: int,\n    ct_numbers: list[int],\n    cache_dir: Path,\n) -&gt; tuple[dict[str, str], dict[str, str]]:\n    \"\"\"Run LLM comparison on consecutive sub-tisk versions.\n\n    Returns ({\"{ct}_{ct1}\": diff_cs}, {\"{ct}_{ct1}\": diff_en}).\n    \"\"\"\n    ollama = OllamaClient()\n    if not ollama.is_available():\n        logger.info(\"[tisk pipeline] Ollama not available, skipping version diff analysis\")\n        return {}, {}\n\n    text_dir = cache_dir / TISKY_TEXT_DIR / str(period)\n    diff_dir = cache_dir / TISKY_META_DIR / str(period) / TISKY_VERSION_DIFFS_DIR\n    diff_dir.mkdir(parents=True, exist_ok=True)\n\n    result: dict[str, str] = {}\n    result_en: dict[str, str] = {}\n\n    for ct in ct_numbers:\n        if not text_dir.exists():\n            continue\n\n        versions = _collect_version_texts(text_dir, ct)\n        if len(versions) &lt; 2:\n            continue\n\n        # Compare consecutive pairs\n        for j in range(len(versions) - 1):\n            ct1_old, path_old = versions[j]\n            ct1_new, path_new = versions[j + 1]\n            diff_key = f\"{ct}_{ct1_new}\"\n            diff_file = diff_dir / f\"{diff_key}.txt\"\n            diff_file_en = diff_dir / f\"{diff_key}_en.txt\"\n\n            # Check cache \u2014 both CS and EN\n            if diff_file.exists():\n                result[diff_key] = diff_file.read_text(encoding=\"utf-8\")\n                if diff_file_en.exists():\n                    result_en[diff_key] = diff_file_en.read_text(encoding=\"utf-8\")\n                continue\n\n            summaries = _compare_version_pair_bilingual(\n                ollama, path_old, path_new, ct1_old, ct1_new, period, ct\n            )\n            if summaries[\"cs\"]:\n                diff_file.write_text(summaries[\"cs\"], encoding=\"utf-8\")\n                result[diff_key] = summaries[\"cs\"]\n            if summaries[\"en\"]:\n                diff_file_en.write_text(summaries[\"en\"], encoding=\"utf-8\")\n                result_en[diff_key] = summaries[\"en\"]\n\n    logger.info(\n        \"[tisk pipeline] Version diffs for period {}: {} comparisons\",\n        period,\n        len(result),\n    )\n    return result, result_en\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/topic_service/","title":"topic_service","text":""},{"location":"reference/pspcz_analyzer/services/topic_service/#pspcz_analyzer.services.topic_service","title":"<code>topic_service</code>","text":"<p>Topic classification for parliamentary prints (tisky) using keyword matching.</p>"},{"location":"reference/pspcz_analyzer/services/topic_service/#pspcz_analyzer.services.topic_service.classify_tisk","title":"<code>classify_tisk(text, title)</code>","text":"<p>Classify a tisk by matching normalized text against topic keywords.</p> <p>Returns list of (topic_id, match_count) sorted by match count descending.</p> Source code in <code>pspcz_analyzer/services/topic_service.py</code> <pre><code>def classify_tisk(text: str, title: str) -&gt; list[tuple[str, int]]:\n    \"\"\"Classify a tisk by matching normalized text against topic keywords.\n\n    Returns list of (topic_id, match_count) sorted by match count descending.\n    \"\"\"\n    normalized = normalize_czech(f\"{title} {text}\")\n    results: list[tuple[str, int]] = []\n\n    for topic_id, (_label_cs, _label_en, keywords) in TOPIC_TAXONOMY.items():\n        count = sum(1 for kw in keywords if kw in normalized)\n        if count &gt; 0:\n            results.append((topic_id, count))\n\n    results.sort(key=lambda x: x[1], reverse=True)\n    return results\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/topic_service/#pspcz_analyzer.services.topic_service.classify_tisk_primary_label","title":"<code>classify_tisk_primary_label(text, title)</code>","text":"<p>Return the Czech label of the best-matching topic, or None.</p> Source code in <code>pspcz_analyzer/services/topic_service.py</code> <pre><code>def classify_tisk_primary_label(text: str, title: str) -&gt; str | None:\n    \"\"\"Return the Czech label of the best-matching topic, or None.\"\"\"\n    results = classify_tisk(text, title)\n    if not results:\n        return None\n    topic_id = results[0][0]\n    label_cs, _, _ = TOPIC_TAXONOMY[topic_id]\n    return label_cs\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/votes_service/","title":"votes_service","text":""},{"location":"reference/pspcz_analyzer/services/votes_service/#pspcz_analyzer.services.votes_service","title":"<code>votes_service</code>","text":"<p>Votes / laws browser \u2014 search, list, and detail views of parliamentary votes.</p>"},{"location":"reference/pspcz_analyzer/services/votes_service/#pspcz_analyzer.services.votes_service.list_votes","title":"<code>list_votes(data, search='', page=1, per_page=30, outcome_filter='', topic_filter='')</code>","text":"<p>List votes with optional text search, topic filter, and pagination.</p> <p>Returns dict with keys: rows, total, page, per_page, total_pages.</p> Source code in <code>pspcz_analyzer/services/votes_service.py</code> <pre><code>def list_votes(\n    data: PeriodData,\n    search: str = \"\",\n    page: int = 1,\n    per_page: int = 30,\n    outcome_filter: str = \"\",\n    topic_filter: str = \"\",\n) -&gt; dict:\n    \"\"\"List votes with optional text search, topic filter, and pagination.\n\n    Returns dict with keys: rows, total, page, per_page, total_pages.\n    \"\"\"\n    void_ids = data.void_votes.get_column(\"id_hlasovani\")\n    votes = data.votes.filter(~pl.col(\"id_hlasovani\").is_in(void_ids))\n\n    votes = _apply_vote_filters(votes, data, search, outcome_filter, topic_filter)\n\n    total = votes.height\n    total_pages = max(1, (total + per_page - 1) // per_page)\n    page = max(1, min(page, total_pages))\n\n    votes = votes.sort(\"datum\", \"cas\", descending=[True, True])\n    offset = (page - 1) * per_page\n    page_rows = votes.slice(offset, per_page)\n\n    rows = page_rows.select(\n        \"id_hlasovani\",\n        \"datum\",\n        \"cas\",\n        \"schuze\",\n        \"cislo\",\n        \"bod\",\n        \"nazev_dlouhy\",\n        \"nazev_kratky\",\n        \"vysledek\",\n        \"pro\",\n        \"proti\",\n        \"zdrzel\",\n        \"nehlasoval\",\n        \"prihlaseno\",\n    ).to_dicts()\n\n    _enrich_vote_rows(rows, data)\n\n    return {\n        \"rows\": rows,\n        \"total\": total,\n        \"page\": page,\n        \"per_page\": per_page,\n        \"total_pages\": total_pages,\n    }\n</code></pre>"},{"location":"reference/pspcz_analyzer/services/votes_service/#pspcz_analyzer.services.votes_service.vote_detail","title":"<code>vote_detail(data, vote_id)</code>","text":"<p>Get full detail for a single vote: metadata + per-party + per-MP breakdown.</p> Source code in <code>pspcz_analyzer/services/votes_service.py</code> <pre><code>def vote_detail(data: PeriodData, vote_id: int) -&gt; dict | None:\n    \"\"\"Get full detail for a single vote: metadata + per-party + per-MP breakdown.\"\"\"\n    vote_row = data.votes.filter(pl.col(\"id_hlasovani\") == vote_id)\n    if vote_row.height == 0:\n        return None\n\n    info = _build_vote_info(vote_row, data)\n\n    # Individual MP votes for this vote\n    mp_rows = data.mp_votes.filter(pl.col(\"id_hlasovani\") == vote_id)\n    mp_detail = mp_rows.join(data.mp_info, on=\"id_poslanec\", how=\"left\")\n\n    return {\n        \"info\": info,\n        \"party_breakdown\": _build_party_breakdown(mp_detail),\n        \"mp_votes\": _build_mp_breakdown(mp_detail),\n    }\n</code></pre>"},{"location":"reference/pspcz_analyzer/utils/","title":"utils","text":""},{"location":"reference/pspcz_analyzer/utils/#pspcz_analyzer.utils","title":"<code>utils</code>","text":""},{"location":"reference/pspcz_analyzer/utils/text/","title":"text","text":""},{"location":"reference/pspcz_analyzer/utils/text/#pspcz_analyzer.utils.text","title":"<code>text</code>","text":"<p>Text normalization utilities for Czech diacritics.</p>"},{"location":"reference/pspcz_analyzer/utils/text/#pspcz_analyzer.utils.text.strip_diacritics","title":"<code>strip_diacritics(text)</code>","text":"<p>Remove diacritical marks from text (e.g. \u010d\u2192c, \u0159\u2192r, \u017e\u2192z).</p> Source code in <code>pspcz_analyzer/utils/text.py</code> <pre><code>def strip_diacritics(text: str) -&gt; str:\n    \"\"\"Remove diacritical marks from text (e.g. \u010d\u2192c, \u0159\u2192r, \u017e\u2192z).\"\"\"\n    nfkd = unicodedata.normalize(\"NFD\", text)\n    return \"\".join(c for c in nfkd if unicodedata.category(c) != \"Mn\")\n</code></pre>"},{"location":"reference/pspcz_analyzer/utils/text/#pspcz_analyzer.utils.text.normalize_czech","title":"<code>normalize_czech(text)</code>","text":"<p>Lowercase and strip diacritics \u2014 suitable for search matching.</p> Source code in <code>pspcz_analyzer/utils/text.py</code> <pre><code>def normalize_czech(text: str) -&gt; str:\n    \"\"\"Lowercase and strip diacritics \u2014 suitable for search matching.\"\"\"\n    return strip_diacritics(text.lower())\n</code></pre>"}]}